{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stavco9/nlp-final-project/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/stavco9/nlp-final-project.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTElBH1-e38B",
        "outputId": "5157b8f2-7c4a-4796-dbc6-db25cda62bb7"
      },
      "id": "eTElBH1-e38B",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlp-final-project'...\n",
            "remote: Enumerating objects: 262, done.\u001b[K\n",
            "remote: Counting objects: 100% (262/262), done.\u001b[K\n",
            "remote: Compressing objects: 100% (170/170), done.\u001b[K\n",
            "remote: Total 262 (delta 125), reused 178 (delta 67), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (262/262), 9.10 MiB | 6.70 MiB/s, done.\n",
            "Resolving deltas: 100% (125/125), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/nlp-final-project/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqoJj8Z4fCBj",
        "outputId": "2884119a-61d0-4918-8f8b-03f65540b980"
      },
      "id": "IqoJj8Z4fCBj",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nlp-final-project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e9081832-eb6e-4117-a375-fb3ac398fa8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9081832-eb6e-4117-a375-fb3ac398fa8e",
        "outputId": "7a96847a-8325-4933-9d7a-b5bf93846bd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-transcript-api==0.6.2 (from -r requirements.txt (line 1))\n",
            "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Collecting youtube-search-python==1.6.6 (from -r requirements.txt (line 2))\n",
            "  Downloading youtube_search_python-1.6.6-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-return-youtube-dislike==1.1.5 (from -r requirements.txt (line 3))\n",
            "  Downloading python_return_youtube_dislike-1.1.5-py3-none-any.whl (3.4 kB)\n",
            "Collecting torch==2.2.1 (from -r requirements.txt (line 4))\n",
            "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.42.3 (from -r requirements.txt (line 5))\n",
            "  Downloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai==1.35.14 (from -r requirements.txt (line 6))\n",
            "  Downloading openai-1.35.14-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.17.1 (from -r requirements.txt (line 7))\n",
            "  Downloading torchvision-0.17.1-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard==2.17.0 (from -r requirements.txt (line 8))\n",
            "  Downloading tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.31.0)\n",
            "Collecting torchview (from -r requirements.txt (line 10))\n",
            "  Downloading torchview-0.2.6-py3-none-any.whl (25 kB)\n",
            "Collecting isodate (from -r requirements.txt (line 11))\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.14.2 (from youtube-search-python==1.6.6->-r requirements.txt (line 2))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->-r requirements.txt (line 4)) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->-r requirements.txt (line 4)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->-r requirements.txt (line 4)) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->-r requirements.txt (line 4)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->-r requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->-r requirements.txt (line 4)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1->-r requirements.txt (line 4))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1->-r requirements.txt (line 4))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1->-r requirements.txt (line 4))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1->-r requirements.txt (line 4))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1->-r requirements.txt (line 4))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1->-r requirements.txt (line 4))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1->-r requirements.txt (line 4))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1->-r requirements.txt (line 4))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1->-r requirements.txt (line 4))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1->-r requirements.txt (line 4))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.1->-r requirements.txt (line 4))\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.3->-r requirements.txt (line 5)) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.3->-r requirements.txt (line 5)) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.3->-r requirements.txt (line 5)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.3->-r requirements.txt (line 5)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.3->-r requirements.txt (line 5)) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.3->-r requirements.txt (line 5)) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.3->-r requirements.txt (line 5)) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.3->-r requirements.txt (line 5)) (4.66.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.35.14->-r requirements.txt (line 6)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.35.14->-r requirements.txt (line 6)) (1.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.35.14->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.35.14->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17.1->-r requirements.txt (line 7)) (9.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.17.0->-r requirements.txt (line 8)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.17.0->-r requirements.txt (line 8)) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.17.0->-r requirements.txt (line 8)) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.17.0->-r requirements.txt (line 8)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.17.0->-r requirements.txt (line 8)) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.17.0->-r requirements.txt (line 8)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.17.0->-r requirements.txt (line 8)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.17.0->-r requirements.txt (line 8)) (3.0.3)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 9)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 9)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 9)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 9)) (2024.7.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.35.14->-r requirements.txt (line 6)) (1.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.14.2->youtube-search-python==1.6.6->-r requirements.txt (line 2))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.14.2->youtube-search-python==1.6.6->-r requirements.txt (line 2))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.35.14->-r requirements.txt (line 6)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.35.14->-r requirements.txt (line 6)) (2.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard==2.17.0->-r requirements.txt (line 8)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->-r requirements.txt (line 4)) (1.3.0)\n",
            "Installing collected packages: python-return-youtube-dislike, triton, torchview, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, isodate, h11, youtube-transcript-api, tensorboard, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, nvidia-cusolver-cu12, httpx, youtube-search-python, transformers, torch, openai, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.4\n",
            "    Uninstalling transformers-4.42.4:\n",
            "      Successfully uninstalled transformers-4.42.4\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.1+cu121\n",
            "    Uninstalling torchvision-0.18.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires tensorboard<2.16,>=2.15, but you have tensorboard 2.17.0 which is incompatible.\n",
            "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.2.1 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 isodate-0.6.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 openai-1.35.14 python-return-youtube-dislike-1.1.5 tensorboard-2.17.0 torch-2.2.1 torchview-0.2.6 torchvision-0.17.1 transformers-4.42.3 triton-2.2.0 youtube-search-python-1.6.6 youtube-transcript-api-0.6.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "34eb7fb1-eacf-4c38-84c6-0e1abc968617",
      "metadata": {
        "id": "34eb7fb1-eacf-4c38-84c6-0e1abc968617"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')\n",
        "!ln -s \"/content/drive/My Drive/IDC_MSc/Year1/NLP/Final-Project/data\" ./data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkCAI6xzg2IX",
        "outputId": "c582627a-bafc-4fca-c6b8-5b27fe7dcdc8"
      },
      "id": "xkCAI6xzg2IX",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f436902-03e9-4eeb-b255-2504901951da",
      "metadata": {
        "id": "8f436902-03e9-4eeb-b255-2504901951da"
      },
      "outputs": [],
      "source": [
        "# Please set a valid Google API\n",
        "# Please download the 'data' folder from https://drive.google.com/drive/folders/1D-OHX0LUNA5Y-0h8UXwqa4H_yETZN_t_ to your project root directory\n",
        "\n",
        "GOOGLE_API_KEY = '**********************'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a97dad-ed78-46fa-8e2a-6cdee6986ef7",
      "metadata": {
        "id": "26a97dad-ed78-46fa-8e2a-6cdee6986ef7"
      },
      "outputs": [],
      "source": [
        "##\n",
        "## Dataset preperation part\n",
        "##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9127a4d-74c9-4107-a56a-4bf00849220b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9127a4d-74c9-4107-a56a-4bf00849220b",
        "outputId": "29c1960c-2e3e-4451-beb0-215f630b9312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are currnet 5000 videos in 10 files\n",
            "There are already at least 5000. No need to do anything\n"
          ]
        }
      ],
      "source": [
        "from modules.dataset.transcript_dataset import TranscriptDataset\n",
        "\n",
        "dataset_builder = TranscriptDataset(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# dataset_folder => The folder to load / save the datasets file from. If not exists it will create it automatically\n",
        "# videos_count => Total number of videos to include in the dataset. If there are already saved videos in the \"data\" folder,\n",
        "# it will fill the remaining videos count (For example if the parameter: videos_count is 4000 and there are already 1500 videos\n",
        "# in the folder, it will fetch 2500 videos). Default is 100\n",
        "# country_code => 2 digits ISO Code of the country you want to fetch videos from (If not set, the default is United States 'us')\n",
        "# Relevant codes for countries with a majority of English based videos => 'ca' Canada, 'uk' United Kingdom, 'au' Australia, 'in' India\n",
        "#  'nz' New Zealand, 'za' South Africa\n",
        "# language => 2 digits ISO Code of the video's language you want to use (If not set, the default is English 'en')\n",
        "dataset_builder.build_dataset(dataset_folder='data/dataset', videos_count=5000, country_code='uk')\n",
        "\n",
        "# dataset_folder => The folder to load / save the datasets file from. If not exists it will create it automatically\n",
        "# comments_count => The amount of comments to fetch for each video. Default is 100\n",
        "dataset_builder.build_comments(dataset_folder='data', comments_count=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9283f73b-424f-4256-b2f3-2294f60bedb6",
      "metadata": {
        "id": "9283f73b-424f-4256-b2f3-2294f60bedb6"
      },
      "outputs": [],
      "source": [
        "##\n",
        "## Sentiment analysis part\n",
        "##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c65cbbc0-e0ce-4e44-beed-eaf0eca4e520",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "1ea277d933a14a1592a2ce7c3657e2c8",
            "10b6e9f7c7634dbe9a58947cae66b0ce",
            "d3a47c4cdc294a149e94007e5293bccc",
            "fe4889f6624a42618f2316ffe9ced11b",
            "1e0f19f26fcf420b969f5ac77706074e",
            "6902adad8dd74079997719d62f90c005",
            "d634f2beb1614c29bf9611b0525bc684",
            "10600c7a9e844e76ac70820663beb53b",
            "f49a539fe8b14fd0ad4ad12f01d0f09f",
            "12e761319b2c4daea2acb71e39fbe341",
            "199350505f4146f98cc156f3c0f48b43",
            "400521c4ec7b47f7886e90af7f59cd70",
            "65d797477f924eb585ab063b2d88efe1",
            "331c7372bf084130936e8e2224dcca65",
            "db38bf661fd447e5bb0223090f72f9fa",
            "89ce3d26ef3d4eddb2d3ab618d40e057",
            "da224cf70187439c81ee45d949acf74d",
            "d352842b063f4b75bb69f5b46f7e241d",
            "8682358a4ef046bb91764482dab9aaec",
            "f652ccfea70947e4a80a247e7cb98c25",
            "010af8321b2c489ca12f8db514d75368",
            "232a41abb1774a4aa31177b13de3fc1b",
            "74b13e85ae2e4f1f9180bcb2145d5db8",
            "e935823e671b43a78ac19abac3e76630",
            "263ea69aaab640268c61f2174201fb36",
            "8f390cf64afe48dfadb61c5e0a8d20a0",
            "7ba7aa9bc372483bb99f2340d5d727bd",
            "30eb5431263c45cd8f8b9de33adb3729",
            "71796eb063ee4349b90d251f451a3c37",
            "fe18dac36da0458d852dc6f7d66daedd",
            "59d555d6ded949baa96cad8bd8f4a5f0",
            "64b3bea6328f496790315146a2b08487",
            "607623b2f1c749b68d6c3c87cb753e13",
            "79e605f8ca0a4f2d8ab7926797017a11",
            "20670b215a084c8ab26e722bb68e8ec3",
            "5450498f4b0548e7b2e4e05ab7085916",
            "da1645398e89405f8c3b8d438af2f405",
            "d64344e14e41424abaa082e139ea714e",
            "f329795c7c4a47e0b3601bc74582bab6",
            "3018e18e42394ba0a74f79decb9f7a29",
            "e809271193f14b378b0b3a467dbb38ce",
            "a3381036a9c649e189dd4dce7edfb774",
            "a0da43e357054c6da146dcbb317ee57c",
            "2d4a60f1fe4e48d8a34fe94e6df018dc"
          ]
        },
        "id": "c65cbbc0-e0ce-4e44-beed-eaf0eca4e520",
        "outputId": "24d03da1-fb52-4abb-a35a-9acfc1bcdfcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ea277d933a14a1592a2ce7c3657e2c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "400521c4ec7b47f7886e90af7f59cd70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74b13e85ae2e4f1f9180bcb2145d5db8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79e605f8ca0a4f2d8ab7926797017a11"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from modules.sentiment_analysis.comments_sentiment_analysis import multi_sentiment_score\n",
        "from modules.sentiment_analysis.video_info import Video_info, datumToObj, save_to_json, save_to_pickle\n",
        "from modules.sentiment_analysis.GPT_comment_controversy import init_openai, get_disagreement_rating\n",
        "from modules.sentiment_analysis.scoring_video import adjust_disagreement_rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "966bcd48-210a-4009-a2d5-42bbc30c7a50",
      "metadata": {
        "id": "966bcd48-210a-4009-a2d5-42bbc30c7a50"
      },
      "outputs": [],
      "source": [
        "# put all the json into pickle\n",
        "datasets = [f'data/dataset/dataset_{i}.json' for i in range(10)]\n",
        "to_print = True #input(\"Do you want to show prints? (y/n): \") != 'n'\n",
        "basic_inf_vids = datumToObj(datasets, 'data/sentiment/videos_sample.pkl', to_print)\n",
        "number_of_vids = len(basic_inf_vids)\n",
        "\n",
        "save_to_json(basic_inf_vids, 'data/sentiment/vids_no_sentiment.json')\n",
        "sentiment_model = multi_sentiment_score\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "count = 0\n",
        "for vid in basic_inf_vids:\n",
        "    print_text = \"\\r\"\n",
        "    if to_print:\n",
        "        count += 1\n",
        "        print_text += f\"Processing video {count}/{number_of_vids}\"\n",
        "    vid.add_comments_scores(sentiment_model, model_name)\n",
        "    if to_print:\n",
        "        print_text += f\" - Disagreement rating: {vid.comments_controversy_classical}\"\n",
        "    if to_print:\n",
        "        print(print_text)\n",
        "\n",
        "save_to_json(basic_inf_vids, 'data/sentiment/vids_classical_sentiment.json')\n",
        "basic_inf_vids = adjust_disagreement_rating(basic_inf_vids)\n",
        "save_to_json(basic_inf_vids, 'data/sentiment/videos_classical_sentiment_adjusted.json')\n",
        "'''\n",
        "\n",
        "#load vids from videos_classical_sentiment_adjusted.json and adjust basic_inf_vids with its values\n",
        "with open('data/data/videos_classical_sentiment_adjusted.json') as json_file:\n",
        "    json_obj = json.load(json_file)\n",
        "for i, video_obj in enumerate(json_obj):\n",
        "    basic_inf_vids[i].comments_controversy_classical = float(video_obj['comments_controversy_classical'])\n",
        "'''\n",
        "\n",
        "\n",
        "to_continue = True #input(\"Do you want to continue with GPT? (y/n): \")\n",
        "if not to_continue:\n",
        "  print(\"Done\")\n",
        "else:\n",
        "  count = 0\n",
        "  client = init_openai()\n",
        "  sentiment_model=get_disagreement_rating\n",
        "  model_name=\"GPT\"\n",
        "  for vid in basic_inf_vids:\n",
        "      print_text = \"\\r\"\n",
        "      if to_print:\n",
        "          count += 1\n",
        "          print_text += f\"Processing video {count}/{number_of_vids}\"\n",
        "      vid.add_comments_scores(sentiment_model, model_name, client)\n",
        "      if to_print:\n",
        "          print_text += f\" - Disagreement rating: {vid.comments_controversy_GPT}\"\n",
        "      if to_print:\n",
        "          print(print_text)\n",
        "  save_to_json(basic_inf_vids, 'data/data/vids_GPT_sentiment.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2554df6b-acf8-4347-86f1-cbef72b740d1",
      "metadata": {
        "id": "2554df6b-acf8-4347-86f1-cbef72b740d1"
      },
      "outputs": [],
      "source": [
        "##\n",
        "## Training part\n",
        "##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8e982ec9-f4b5-4818-95bc-ec856b159c87",
      "metadata": {
        "id": "8e982ec9-f4b5-4818-95bc-ec856b159c87"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from modules.azsclm.azsclm import AZSC_LanguageModel\n",
        "from modules.azsclm.board import Board\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config_Model:\n",
        "    def __init__(self, num_epochs, train_data, valid_data, test_data, model, optimizer, threshold_percentage, criterion, device):\n",
        "        self.num_epochs = num_epochs\n",
        "        self.train_data = train_data\n",
        "        self.valid_data = valid_data\n",
        "        self.test_data = test_data\n",
        "        self.device = device\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.threshold_percentage = threshold_percentage\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, config, data_name = 'GPT'):\n",
        "        self.num_epochs = config.num_epochs\n",
        "        self.train_data = config.train_data\n",
        "        self.valid_data = config.valid_data\n",
        "        self.test_data = config.test_data\n",
        "        self.pred_score = []\n",
        "        self.true_score = []\n",
        "        self.device = config.device\n",
        "        self.model = config.model\n",
        "        self.optimizer = config.optimizer\n",
        "        self.criterion = config.criterion\n",
        "        self.data_name = 'comments_controversy_' + data_name\n",
        "        self.max_available = 1\n",
        "        self.min_available = -1\n",
        "        self.board = Board()\n",
        "\n",
        "    def calculate_accuracy(self, min_item, max_item, pred, true):\n",
        "        pred = max(pred, self.min_available)\n",
        "        pred = min(pred, self.max_available)\n",
        "        true = max(true, self.min_available)\n",
        "        true = min(true, self.max_available)\n",
        "\n",
        "        acc = 1 - (abs(pred - true) / abs(max_item - min_item))\n",
        "        return float(acc)\n",
        "\n",
        "    def train(self):\n",
        "        size = len(self.train_data.dataset)\n",
        "        num_train_batches = len(self.train_data)\n",
        "        num_valid_batches = len(self.valid_data)\n",
        "        self.model.train()\n",
        "        for epoch in range(self.num_epochs):\n",
        "            print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "            epoch_loss = 0\n",
        "\n",
        "            for batch_idx, (src, trg) in enumerate(self.train_data):\n",
        "                src, trg = src.to(self.device), trg.to(self.device)\n",
        "                output = self.model(src)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss = self.criterion(output, trg)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.5)\n",
        "                train_loss = loss.item()\n",
        "                self.optimizer.step()\n",
        "                epoch_loss += train_loss\n",
        "\n",
        "            self.board.info_handler(loss=train_loss, batch=batch_idx, size=size, epoch_loss=epoch_loss, name='(training) ' + self.data_name)\n",
        "\n",
        "            print(\"Evaluating with validation set\")\n",
        "            self.model.eval()\n",
        "            running_valid_loss = 0\n",
        "            total_acc = 0\n",
        "\n",
        "            list_targets = [item[1].to('cuda') for item in self.valid_data]\n",
        "            list_targets = torch.stack(list_targets, dim=1).to('cuda')\n",
        "            max_item = min(torch.max(list_targets).item(), self.max_available)\n",
        "            min_item = max(torch.min(list_targets).item(), self.min_available)\n",
        "\n",
        "            # Disable gradient computation and reduce memory consumption.\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, (src, trg) in enumerate(self.valid_data):\n",
        "                    src, trg = src.to(self.device), trg.to(self.device)\n",
        "                    output = self.model(src)\n",
        "                    loss = self.criterion(output, trg)\n",
        "                    valid_loss = loss.item()\n",
        "                    running_valid_loss += valid_loss\n",
        "                    total_acc += self.calculate_accuracy(max_item, min_item, output, trg)\n",
        "                    avg_acc = total_acc / (batch_idx + 1)\n",
        "                    print(f\"Accuracy: {avg_acc}\")\n",
        "\n",
        "            avg_train_loss = epoch_loss / num_train_batches\n",
        "            avg_valid_loss = running_valid_loss / num_valid_batches\n",
        "            print(f\"Average train loss: {avg_train_loss}, avarage valid loss: {avg_valid_loss}\")\n",
        "            avg_acc = total_acc / num_valid_batches\n",
        "            print(f\"Average accuracy rate: {avg_acc}\")\n",
        "\n",
        "        batch = next(iter(self.train_data))\n",
        "        src = batch[0].to(self.device)\n",
        "        self.board.add_graph(self.model, src)\n",
        "\n",
        "    def test(self):\n",
        "        size = len(self.test_data.dataset)\n",
        "        num_batches = len(self.test_data)\n",
        "        self.model.eval()\n",
        "        epoch_loss = 0.0\n",
        "        sections_losses = {}\n",
        "        total_acc = 0\n",
        "\n",
        "        list_targets = [item[1].to('cuda') for item in self.test_data]\n",
        "        list_targets = torch.stack(list_targets, dim=1).to('cuda')\n",
        "        max_item = min(torch.max(list_targets).item(), self.max_available)\n",
        "        min_item = max(torch.min(list_targets).item(), self.min_available)\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (src, trg) in enumerate(self.test_data):\n",
        "                src, trg = src.to(self.device), trg.to(self.device)\n",
        "                pred = self.model(src)\n",
        "                pred = pred.to(self.device)\n",
        "                loss = self.criterion(pred, trg)\n",
        "                #for section_name, section_loss in loss.last_losses.items():\n",
        "                #    if section_name not in sections_losses:\n",
        "                #        sections_losses[section_name] = torch.Tensor([0]).to(self.device)\n",
        "                #    sections_losses[section_name] += section_loss\n",
        "                #for section_name in sections_losses.keys():\n",
        "                #    sections_losses[section_name] /= size\n",
        "                epoch_loss = self.board.info_handler(loss=loss.item(), batch=batch_idx, size=size, epoch_loss=epoch_loss, name = '(test) ' + self.data_name)\n",
        "                avg_loss = epoch_loss / num_batches\n",
        "                print(f\"Average loss: {avg_loss}\")\n",
        "                total_acc += self.calculate_accuracy(max_item, min_item, pred, trg)\n",
        "                avg_acc = total_acc / (batch_idx + 1)\n",
        "                print(f\"Accuracy: {avg_acc}\")\n",
        "                self.pred_score.extend(pred.cpu().numpy())\n",
        "                self.true_score.extend(trg.cpu().numpy())\n",
        "\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        print(f\"Average loss rate: {avg_acc}\")\n",
        "\n",
        "        avg_acc = total_acc / num_batches\n",
        "        print(f\"Average accuracy rate: {avg_acc}\")\n",
        "\n",
        "    def calculate_metrics(self):\n",
        "      # Convert lists to tensors for calculation\n",
        "      true_score_tensor = torch.tensor(self.true_score)\n",
        "      pred_score_tensor = torch.tensor(self.pred_score)\n",
        "\n",
        "      # Calculating precision, recall, and F1 score using PyTorch\n",
        "      TP = ((pred_score_tensor >= 0) & (true_score_tensor >= 0)).sum().item()\n",
        "      FP = ((pred_score_tensor >= 0) & (true_score_tensor < 0)).sum().item()\n",
        "      FN = ((pred_score_tensor < 0) & (true_score_tensor >= 0)).sum().item()\n",
        "\n",
        "      precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
        "      recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
        "      f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "      print(f'Precision: {precision}')\n",
        "      print(f'Recall: {recall}')\n",
        "      print(f'F1 Score: {f1}')\n",
        "\n",
        "      return (precision, recall, f1)\n",
        "\n"
      ],
      "metadata": {
        "id": "wA4Q6ogKxfPH"
      },
      "id": "wA4Q6ogKxfPH",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    src, tgt = zip(*batch)\n",
        "    src = torch.stack(src).long()  # Ensure src is Long\n",
        "    src = src.view(src.size(0), -1)\n",
        "    tgt = torch.stack(tgt).float()  # Ensure tgt is Float\n",
        "    tgt = tgt.view(tgt.size(0), -1)\n",
        "    return src, tgt\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, scores):\n",
        "        self.texts = texts\n",
        "        self.scores = scores\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.texts[idx]\n",
        "        trg = self.scores[idx]\n",
        "        return src, trg\n",
        "\n",
        "def homogonize_token_length(text, tokenizer, text_length, device=\"cuda\"):\n",
        "    tokenized_text = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)['input_ids'].long().to(device)\n",
        "    if tokenized_text.shape[1] < text_length:\n",
        "        tokenized_text = torch.cat((tokenized_text, torch.zeros(tokenized_text.shape[0], text_length - tokenized_text.shape[1]).to(device)), dim=1)\n",
        "    else:\n",
        "        tokenized_text = tokenized_text[:, :text_length]\n",
        "    return tokenized_text\n",
        "\n",
        "class Datasets:\n",
        "    def __init__(self, json_file, tokenizer, text_length, ratios=[0.8, 0.1, 0.1], batch_size=1, device=\"cuda\"):\n",
        "        self.json_file = json_file\n",
        "        assert sum(ratios) == 1\n",
        "        self.ratios = ratios\n",
        "        self.batch_size = batch_size\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_length = text_length\n",
        "        self.device = device\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        with open(self.json_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        random.shuffle(data)\n",
        "        tokens = [homogonize_token_length(d[0], self.tokenizer, self.text_length, self.device) for d in data]\n",
        "        GPT_scores = [torch.tensor(d[1], dtype=torch.float32).to(self.device) for d in data]\n",
        "        BERT_scores = [torch.tensor(d[2], dtype=torch.float32).to(self.device) for d in data]\n",
        "\n",
        "        train_size = int(self.ratios[0] * len(data))\n",
        "        val_size = int(self.ratios[1] * len(data))\n",
        "        #test_size = len(data) - train_size - val_size\n",
        "\n",
        "        train_text = tokens[:train_size]\n",
        "        val_text = tokens[train_size:train_size + val_size]\n",
        "        test_text = tokens[train_size + val_size:]\n",
        "\n",
        "        train_scores1 = GPT_scores[:train_size]\n",
        "        val_scores1 = GPT_scores[train_size:train_size + val_size]\n",
        "        test_scores1 = GPT_scores[train_size + val_size:]\n",
        "\n",
        "        train_scores2 = BERT_scores[:train_size]\n",
        "        val_scores2 = BERT_scores[train_size:train_size + val_size]\n",
        "        test_scores2 = BERT_scores[train_size + val_size:]\n",
        "\n",
        "        train_dataset1 = CustomDataset(train_text, train_scores1)\n",
        "        val_dataset1 = CustomDataset(val_text, val_scores1)\n",
        "        test_dataset1 = CustomDataset(test_text, test_scores1)\n",
        "\n",
        "        train_dataset2 = CustomDataset(train_text, train_scores2)\n",
        "        val_dataset2 = CustomDataset(val_text, val_scores2)\n",
        "        test_dataset2 = CustomDataset(test_text, test_scores2)\n",
        "\n",
        "        # Create data loaders\n",
        "        self.train_loader1 = DataLoader(train_dataset1, batch_size=self.batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "        self.val_loader1 = DataLoader(val_dataset1, batch_size=self.batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "        self.test_loader1 = DataLoader(test_dataset1, batch_size=self.batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "        self.train_loader2 = DataLoader(train_dataset2, batch_size=self.batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "        self.val_loader2 = DataLoader(val_dataset2, batch_size=self.batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "        self.test_loader2 = DataLoader(test_dataset2, batch_size=self.batch_size, shuffle=False, collate_fn=custom_collate_fn)"
      ],
      "metadata": {
        "id": "PsE2BJTrxPyQ"
      },
      "id": "PsE2BJTrxPyQ",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(torch.cuda.memory_summary(device=\"cuda\", abbreviated=True))\n",
        "\n",
        "    text_length = 4096\n",
        "    tokenizer_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "    datasets = Datasets(\"data/sentiment/vids_sentiment_fixed.json\", tokenizer, text_length, ratios=[0.8, 0.1, 0.1], batch_size=1, device=\"cuda\")\n",
        "\n",
        "    # Instantiate the model\n",
        "    model = AZSC_LanguageModel(tokenizer, text_length).to(\"cuda\")\n",
        "\n",
        "    # Instantiate the optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-2, weight_decay=0.5)\n",
        "\n",
        "    # Instantiate the loss function\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # the gpt model\n",
        "    traind_data_gpt = datasets.train_loader1\n",
        "    valid_data_gpt = datasets.val_loader1\n",
        "    test_data_gpt = datasets.test_loader1\n",
        "\n",
        "    config_gpt = Config_Model(num_epochs=2, train_data=traind_data_gpt, valid_data=valid_data_gpt, test_data=test_data_gpt, model=model, optimizer=optimizer, threshold_percentage = 5, criterion=criterion, device=\"cuda\")\n",
        "    # Instantiate the Train object\n",
        "    model_gpt = Model(config_gpt, data_name='GPT')\n",
        "\n",
        "    # the bert model\n",
        "    trained_data_classical = datasets.train_loader2\n",
        "    valid_data_classical = datasets.val_loader2\n",
        "    test_data_classical = datasets.test_loader2\n",
        "    config_classical = Config_Model(num_epochs=2, train_data=trained_data_classical, valid_data=valid_data_classical, test_data=test_data_classical, model=model, optimizer=optimizer, threshold_percentage = 5, criterion=criterion, device=\"cuda\")\n",
        "    # Instantiate the Train object\n",
        "    model_classical = Model(config_classical, data_name='classical')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX7KyBrox5Ps",
        "outputId": "f7dddeb4-f722-4bff-a113-035a1fb14aa4"
      },
      "id": "oX7KyBrox5Ps",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 145507 KiB |   2560 MiB | 199382 GiB | 199382 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 145507 KiB |   2560 MiB | 199382 GiB | 199382 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 141447 KiB |   2556 MiB | 199361 GiB | 199361 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   | 184320 KiB |   2614 MiB |   2700 MiB |   2520 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  38813 KiB |  67597 KiB |   8843 GiB |   8843 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |   12522    |   24955    |    8587 K  |    8574 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |   12522    |   24955    |    8587 K  |    8574 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      47    |      83    |     101    |      54    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      16    |      36    |    2808 K  |    2808 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       9    |     765 K  |     765 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       9    |       9    |       9    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for batch_idx, (src, trg) in enumerate(test_data_classical):\n",
        "#    src, trg = src.to('cuda'), trg.to('cuda')\n",
        "\n",
        "count = 0\n",
        "for (src, trg) in test_data_classical:\n",
        "  if count == 10:\n",
        "    break\n",
        "\n",
        "  print(f\"Src tensor -> {src}\")\n",
        "  print(f\"Trg tensor -> {trg.item()}\")\n",
        "  print(f\"Prd tensor -> {model_classical.model(src).item()}\")\n",
        "\n",
        "  count+=1\n",
        "#targets = [item[1].to('cuda') for item in test_data_classical]\n",
        "#targets[5]\n",
        "#targets = torch.stack(targets, dim=1).to('cuda')\n",
        "#max_item = min(torch.max(targets).item(), 1)\n",
        "#print(type(max_item))\n",
        "#min_item = max(torch.min(targets).item(), -1)\n",
        "#print(type(min_item))\n",
        "#print(abs(max_item - min_item) == 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNyCekQ5Dthq",
        "outputId": "9aafa537-94a6-4734-8f1c-6f6d0ed43082"
      },
      "id": "tNyCekQ5Dthq",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Src tensor -> tensor([[ 101, 2206, 7226,  ...,    0,    0,    0]], device='cuda:0')\n",
            "Trg tensor -> 0.35774075984954834\n",
            "Prd tensor -> 0.12340768426656723\n",
            "Src tensor -> tensor([[ 101, 2651, 2057,  ...,    0,    0,    0]], device='cuda:0')\n",
            "Trg tensor -> 0.3160344958305359\n",
            "Prd tensor -> 0.12340768426656723\n",
            "Src tensor -> tensor([[ 101, 2292, 2033,  ...,    0,    0,    0]], device='cuda:0')\n",
            "Trg tensor -> 0.5235332250595093\n",
            "Prd tensor -> 0.12340768426656723\n",
            "Src tensor -> tensor([[ 101, 2821, 2026,  ...,    0,    0,    0]], device='cuda:0')\n",
            "Trg tensor -> 0.006920829880982637\n",
            "Prd tensor -> 0.12340768426656723\n",
            "Src tensor -> tensor([[ 101, 2023, 2003,  ...,    0,    0,    0]], device='cuda:0')\n",
            "Trg tensor -> 0.02276647835969925\n",
            "Prd tensor -> 0.12340768426656723\n",
            "Src tensor -> tensor([[ 101, 1028, 1028,  ...,    0,    0,    0]], device='cuda:0')\n",
            "Trg tensor -> 0.21816067397594452\n",
            "Prd tensor -> 0.12340768426656723\n",
            "Src tensor -> tensor([[ 101, 1011, 6723,  ...,    0,    0,    0]], device='cuda:0')\n",
            "Trg tensor -> 0.2273346483707428\n",
            "Prd tensor -> 0.12340768426656723\n",
            "Src tensor -> tensor([[ 101, 2182, 2012,  ...,    0,    0,    0]], device='cuda:0')\n",
            "Trg tensor -> 0.03355775400996208\n",
            "Prd tensor -> 0.12340768426656723\n",
            "Src tensor -> tensor([[ 101, 1031, 2189,  ...,    0,    0,    0]], device='cuda:0')\n",
            "Trg tensor -> 0.2316627949476242\n",
            "Prd tensor -> 0.12340768426656723\n",
            "Src tensor -> tensor([[ 101, 2204, 2954,  ...,    0,    0,    0]], device='cuda:0')\n",
            "Trg tensor -> 0.1810580939054489\n",
            "Prd tensor -> 0.12340768426656723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Start training\n",
        "    model_classical.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOqBFfqAC6d8",
        "outputId": "06c795df-efa5-40d1-e4b2-39cec0065e86"
      },
      "id": "zOqBFfqAC6d8",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Evaluating with validation set\n",
            "Accuracy: 0.9362105131149292\n",
            "Accuracy: 0.8956868946552277\n",
            "Accuracy: 0.8856644233067831\n",
            "Accuracy: 0.7060914486646652\n",
            "Accuracy: 0.7358859896659851\n",
            "Accuracy: 0.754283607006073\n",
            "Accuracy: 0.6965963244438171\n",
            "Accuracy: 0.6994268298149109\n",
            "Accuracy: 0.7214216258790758\n",
            "Accuracy: 0.7341583251953125\n",
            "Accuracy: 0.7567969452251088\n",
            "Accuracy: 0.775814468661944\n",
            "Accuracy: 0.7836752671461839\n",
            "Accuracy: 0.7881862819194794\n",
            "Accuracy: 0.7946133812268575\n",
            "Accuracy: 0.8017869181931019\n",
            "Accuracy: 0.796718790250666\n",
            "Accuracy: 0.7994999786218008\n",
            "Accuracy: 0.7933515467141804\n",
            "Accuracy: 0.7959179371595383\n",
            "Accuracy: 0.7991512474559602\n",
            "Accuracy: 0.7862722846594724\n",
            "Accuracy: 0.7768531415773474\n",
            "Accuracy: 0.7646587019165357\n",
            "Accuracy: 0.7691798686981202\n",
            "Accuracy: 0.7721280799462245\n",
            "Accuracy: 0.7637487936902929\n",
            "Accuracy: 0.769157316003527\n",
            "Accuracy: 0.7489616151513725\n",
            "Accuracy: 0.7299164851506551\n",
            "Accuracy: 0.7206045658357682\n",
            "Accuracy: 0.7244871351867914\n",
            "Accuracy: 0.7257832071997903\n",
            "Accuracy: 0.70981985856505\n",
            "Accuracy: 0.7140243240765163\n",
            "Accuracy: 0.7179256793525484\n",
            "Accuracy: 0.721619188785553\n",
            "Accuracy: 0.7127088731841037\n",
            "Accuracy: 0.7103669200188074\n",
            "Accuracy: 0.7145076677203178\n",
            "Accuracy: 0.7178535534114372\n",
            "Accuracy: 0.7169990638891856\n",
            "Accuracy: 0.7199820127598074\n",
            "Accuracy: 0.7201776490970091\n",
            "Accuracy: 0.7231510983573066\n",
            "Accuracy: 0.725842851659526\n",
            "Accuracy: 0.7208848405391612\n",
            "Accuracy: 0.7141068788866202\n",
            "Accuracy: 0.7171515433155761\n",
            "Accuracy: 0.7197181880474091\n",
            "Accuracy: 0.7232321968265608\n",
            "Accuracy: 0.7276534254734333\n",
            "Accuracy: 0.7303485825376691\n",
            "Accuracy: 0.7324727144506242\n",
            "Accuracy: 0.7278656937859275\n",
            "Accuracy: 0.7316856033035687\n",
            "Accuracy: 0.7265079774354634\n",
            "Accuracy: 0.7286905461344225\n",
            "Accuracy: 0.7310099884615107\n",
            "Accuracy: 0.7329696138699849\n",
            "Accuracy: 0.7291521537499349\n",
            "Accuracy: 0.7248391162964606\n",
            "Accuracy: 0.7270389397939047\n",
            "Accuracy: 0.7303173523396254\n",
            "Accuracy: 0.732057486130641\n",
            "Accuracy: 0.7337756220138434\n",
            "Accuracy: 0.7354378691360132\n",
            "Accuracy: 0.7370613927350325\n",
            "Accuracy: 0.7387017244878022\n",
            "Accuracy: 0.7355275349957603\n",
            "Accuracy: 0.7372887571093062\n",
            "Accuracy: 0.7339399771557914\n",
            "Accuracy: 0.7347554817591628\n",
            "Accuracy: 0.7363480410060367\n",
            "Accuracy: 0.736295619805654\n",
            "Accuracy: 0.737726175471356\n",
            "Accuracy: 0.7399048696864735\n",
            "Accuracy: 0.741659864401206\n",
            "Accuracy: 0.7409159616578983\n",
            "Accuracy: 0.7431378968060016\n",
            "Accuracy: 0.7463083458535466\n",
            "Accuracy: 0.7475155017724852\n",
            "Accuracy: 0.7439845391066677\n",
            "Accuracy: 0.7413283437490463\n",
            "Accuracy: 0.7440059100880342\n",
            "Accuracy: 0.745290057603703\n",
            "Accuracy: 0.7387869501936024\n",
            "Accuracy: 0.7400465045462955\n",
            "Accuracy: 0.7412888246975587\n",
            "Accuracy: 0.7424335559209188\n",
            "Accuracy: 0.7404823303222656\n",
            "Accuracy: 0.7416206455749014\n",
            "Accuracy: 0.7428935394492201\n",
            "Accuracy: 0.7449250126138647\n",
            "Accuracy: 0.7433521214284395\n",
            "Accuracy: 0.7458729924013218\n",
            "Accuracy: 0.7451540698710176\n",
            "Accuracy: 0.7445929129512943\n",
            "Accuracy: 0.7396726415614889\n",
            "Accuracy: 0.741841207742691\n",
            "Accuracy: 0.7431125452022741\n",
            "Accuracy: 0.7441615865511053\n",
            "Accuracy: 0.7451431959578134\n",
            "Accuracy: 0.7430397810844275\n",
            "Accuracy: 0.739027684643155\n",
            "Accuracy: 0.73961655430074\n",
            "Accuracy: 0.7406051988913634\n",
            "Accuracy: 0.741578773215965\n",
            "Accuracy: 0.7435214147655242\n",
            "Accuracy: 0.7452932000160217\n",
            "Accuracy: 0.746615988714201\n",
            "Accuracy: 0.7476805533681598\n",
            "Accuracy: 0.7458488772400712\n",
            "Accuracy: 0.7470925466010445\n",
            "Accuracy: 0.7479442777840988\n",
            "Accuracy: 0.7493092366333666\n",
            "Accuracy: 0.7451681387730134\n",
            "Accuracy: 0.7472496007458639\n",
            "Accuracy: 0.7480731005428218\n",
            "Accuracy: 0.7491279890139898\n",
            "Accuracy: 0.7501267702126306\n",
            "Accuracy: 0.7509053188269256\n",
            "Accuracy: 0.7524684061849021\n",
            "Accuracy: 0.7532626202990932\n",
            "Accuracy: 0.753215003490448\n",
            "Accuracy: 0.7549685348594\n",
            "Accuracy: 0.7557049041657936\n",
            "Accuracy: 0.7551241554319859\n",
            "Accuracy: 0.7563561991203663\n",
            "Accuracy: 0.7576558599105248\n",
            "Accuracy: 0.7583928472213163\n",
            "Accuracy: 0.7591321563178842\n",
            "Accuracy: 0.7597764914197133\n",
            "Accuracy: 0.7614227742401521\n",
            "Accuracy: 0.7619769838121202\n",
            "Accuracy: 0.7626701277845046\n",
            "Accuracy: 0.7625769167920969\n",
            "Accuracy: 0.7595657455748406\n",
            "Accuracy: 0.7581812546407576\n",
            "Accuracy: 0.7593889674970082\n",
            "Accuracy: 0.7600252890417761\n",
            "Accuracy: 0.757346554541252\n",
            "Accuracy: 0.7582950846298592\n",
            "Accuracy: 0.758896558235089\n",
            "Accuracy: 0.756205063441704\n",
            "Accuracy: 0.7568750989763704\n",
            "Accuracy: 0.7580131057168351\n",
            "Accuracy: 0.7556828260421753\n",
            "Accuracy: 0.7526993643517462\n",
            "Accuracy: 0.7509667185942331\n",
            "Accuracy: 0.7518875022597661\n",
            "Accuracy: 0.752755685464332\n",
            "Accuracy: 0.7533719613661174\n",
            "Accuracy: 0.7545097296114092\n",
            "Accuracy: 0.7559234572995094\n",
            "Accuracy: 0.7565049983752079\n",
            "Accuracy: 0.7570695539188993\n",
            "Accuracy: 0.7547883923295178\n",
            "Accuracy: 0.7563034449733278\n",
            "Accuracy: 0.7545801073312759\n",
            "Accuracy: 0.7527657129009317\n",
            "Accuracy: 0.7525638989460322\n",
            "Accuracy: 0.7532645131181355\n",
            "Accuracy: 0.7546803787714098\n",
            "Accuracy: 0.7526027202606201\n",
            "Accuracy: 0.7531577592872711\n",
            "Accuracy: 0.7536704718709706\n",
            "Accuracy: 0.7542156782888231\n",
            "Accuracy: 0.7548015428012644\n",
            "Accuracy: 0.7553422570228576\n",
            "Accuracy: 0.755862275759379\n",
            "Accuracy: 0.7554782989413239\n",
            "Accuracy: 0.7560051255832517\n",
            "Accuracy: 0.7565869702689949\n",
            "Accuracy: 0.7572955874034336\n",
            "Accuracy: 0.7555212717164647\n",
            "Accuracy: 0.7561699380982394\n",
            "Accuracy: 0.7567087519034911\n",
            "Accuracy: 0.7574592445815742\n",
            "Accuracy: 0.7587817092736562\n",
            "Accuracy: 0.7592194017784372\n",
            "Accuracy: 0.7596899850682898\n",
            "Accuracy: 0.7601825396219889\n",
            "Accuracy: 0.7608260965865591\n",
            "Accuracy: 0.7612914977846919\n",
            "Accuracy: 0.7582793357551739\n",
            "Accuracy: 0.7572716321537202\n",
            "Accuracy: 0.7578595749875332\n",
            "Accuracy: 0.7583747439914279\n",
            "Accuracy: 0.7588737864243357\n",
            "Accuracy: 0.7597906960866838\n",
            "Accuracy: 0.7603249236320456\n",
            "Accuracy: 0.7608198782940603\n",
            "Accuracy: 0.7612502077191147\n",
            "Accuracy: 0.7616758346557617\n",
            "Accuracy: 0.7607933477479585\n",
            "Accuracy: 0.7612339451833425\n",
            "Accuracy: 0.7617413416655376\n",
            "Accuracy: 0.762279807922229\n",
            "Accuracy: 0.7603297466039658\n",
            "Accuracy: 0.759634464534361\n",
            "Accuracy: 0.7600580669275605\n",
            "Accuracy: 0.7606083647958164\n",
            "Accuracy: 0.7612511515617371\n",
            "Accuracy: 0.7617140101223457\n",
            "Accuracy: 0.7621289283326529\n",
            "Accuracy: 0.76195906728938\n",
            "Accuracy: 0.7615082590625837\n",
            "Accuracy: 0.7620647598111459\n",
            "Accuracy: 0.7616158167521159\n",
            "Accuracy: 0.7614217612415694\n",
            "Accuracy: 0.7605749492375355\n",
            "Accuracy: 0.7610609184408412\n",
            "Accuracy: 0.7611896688135985\n",
            "Accuracy: 0.7621418800464896\n",
            "Accuracy: 0.7614997978563662\n",
            "Accuracy: 0.7618912354043003\n",
            "Accuracy: 0.7623919855564012\n",
            "Accuracy: 0.7629479270547492\n",
            "Accuracy: 0.7604935754429211\n",
            "Accuracy: 0.7590504517922034\n",
            "Accuracy: 0.7594426773689888\n",
            "Accuracy: 0.7598885530313568\n",
            "Accuracy: 0.7602679136076144\n",
            "Accuracy: 0.7606523593266805\n",
            "Accuracy: 0.7614210095025796\n",
            "Accuracy: 0.7609234355094674\n",
            "Accuracy: 0.7612971509234947\n",
            "Accuracy: 0.7616681237928732\n",
            "Accuracy: 0.762201754424883\n",
            "Accuracy: 0.7632245914244549\n",
            "Accuracy: 0.7617376066487411\n",
            "Accuracy: 0.7623709895580112\n",
            "Accuracy: 0.7605865103566748\n",
            "Accuracy: 0.7610337044330353\n",
            "Accuracy: 0.7612744360151937\n",
            "Accuracy: 0.7616266359256793\n",
            "Accuracy: 0.762052368967473\n",
            "Accuracy: 0.762595484695674\n",
            "Accuracy: 0.7629790119826794\n",
            "Accuracy: 0.7629049860590227\n",
            "Accuracy: 0.7632530568059811\n",
            "Accuracy: 0.7627937354668668\n",
            "Accuracy: 0.763498097413876\n",
            "Accuracy: 0.7629451369752689\n",
            "Accuracy: 0.7637055094649152\n",
            "Accuracy: 0.762199311362587\n",
            "Accuracy: 0.7626008119794631\n",
            "Accuracy: 0.7614481518546261\n",
            "Accuracy: 0.7607971785068512\n",
            "Accuracy: 0.7595920752719104\n",
            "Accuracy: 0.7584224135156662\n",
            "Accuracy: 0.7587706097972252\n",
            "Accuracy: 0.7591152475105496\n",
            "Accuracy: 0.7600386231553321\n",
            "Accuracy: 0.760392167372629\n",
            "Accuracy: 0.760723838314472\n",
            "Accuracy: 0.7610575859398805\n",
            "Accuracy: 0.7613807882121171\n",
            "Accuracy: 0.761718895802131\n",
            "Accuracy: 0.7625962566598622\n",
            "Accuracy: 0.762917637370015\n",
            "Accuracy: 0.7612641657713248\n",
            "Accuracy: 0.7615977595701362\n",
            "Accuracy: 0.7619209606692476\n",
            "Accuracy: 0.7622614097326321\n",
            "Accuracy: 0.7627811429652382\n",
            "Accuracy: 0.7630885645969591\n",
            "Accuracy: 0.7625143025001185\n",
            "Accuracy: 0.7632601572407617\n",
            "Accuracy: 0.7640639015669313\n",
            "Accuracy: 0.7645315887296901\n",
            "Accuracy: 0.7650520065765241\n",
            "Accuracy: 0.7645149548558423\n",
            "Accuracy: 0.7650808919559825\n",
            "Accuracy: 0.7656361272801524\n",
            "Accuracy: 0.7659336786838215\n",
            "Accuracy: 0.7646861385098465\n",
            "Accuracy: 0.7642808742420648\n",
            "Accuracy: 0.7639609941414425\n",
            "Accuracy: 0.7642544902516429\n",
            "Accuracy: 0.764543798798365\n",
            "Accuracy: 0.7648333226834085\n",
            "Accuracy: 0.7651173950920642\n",
            "Accuracy: 0.7654013370212756\n",
            "Accuracy: 0.7660300606614227\n",
            "Accuracy: 0.7663783744652513\n",
            "Accuracy: 0.7666699859417148\n",
            "Accuracy: 0.7671662719604466\n",
            "Accuracy: 0.7675888521917935\n",
            "Accuracy: 0.7678620934076735\n",
            "Accuracy: 0.7681352559426059\n",
            "Accuracy: 0.7677814360364712\n",
            "Accuracy: 0.7681766754510452\n",
            "Accuracy: 0.768441682144747\n",
            "Accuracy: 0.7689491425817078\n",
            "Accuracy: 0.7674495156365212\n",
            "Accuracy: 0.7670049761365724\n",
            "Accuracy: 0.7673561794303333\n",
            "Accuracy: 0.7676516662041346\n",
            "Accuracy: 0.7679168100768942\n",
            "Accuracy: 0.7667466518499993\n",
            "Accuracy: 0.7670044220320069\n",
            "Accuracy: 0.7659170425644046\n",
            "Accuracy: 0.7652373116524493\n",
            "Accuracy: 0.7655005671230017\n",
            "Accuracy: 0.7657984481572329\n",
            "Accuracy: 0.7660626033683876\n",
            "Accuracy: 0.766509809926104\n",
            "Accuracy: 0.7667791258904242\n",
            "Accuracy: 0.7670293176289157\n",
            "Accuracy: 0.767498231278016\n",
            "Accuracy: 0.7680329689964319\n",
            "Accuracy: 0.7685192214075927\n",
            "Accuracy: 0.7687946132251194\n",
            "Accuracy: 0.7694110287518441\n",
            "Accuracy: 0.7676007879269235\n",
            "Accuracy: 0.7681047033588841\n",
            "Accuracy: 0.7681086554796345\n",
            "Accuracy: 0.7683847960084677\n",
            "Accuracy: 0.7686341161668486\n",
            "Accuracy: 0.7688709889879878\n",
            "Accuracy: 0.769342396281452\n",
            "Accuracy: 0.7695813745628168\n",
            "Accuracy: 0.7699576918895428\n",
            "Accuracy: 0.7706434280594434\n",
            "Accuracy: 0.7694811507467101\n",
            "Accuracy: 0.769862998912974\n",
            "Accuracy: 0.7704775529067205\n",
            "Accuracy: 0.7709284583727519\n",
            "Accuracy: 0.7698452658163457\n",
            "Accuracy: 0.7700835509472582\n",
            "Accuracy: 0.7703506984152235\n",
            "Accuracy: 0.7696169991336183\n",
            "Accuracy: 0.770111139140912\n",
            "Accuracy: 0.7703402202044215\n",
            "Accuracy: 0.771016965810906\n",
            "Accuracy: 0.7714046425015263\n",
            "Accuracy: 0.77172719447662\n",
            "Accuracy: 0.7719580411911011\n",
            "Accuracy: 0.7721172723252753\n",
            "Accuracy: 0.7727130116426457\n",
            "Accuracy: 0.7731599331597198\n",
            "Accuracy: 0.7733766550934592\n",
            "Accuracy: 0.7739013794539631\n",
            "Accuracy: 0.7741726149368837\n",
            "Accuracy: 0.7747541203966058\n",
            "Accuracy: 0.7749731419072754\n",
            "Accuracy: 0.7754938023479757\n",
            "Accuracy: 0.7757825602803912\n",
            "Accuracy: 0.7756352699719943\n",
            "Accuracy: 0.7753883634101261\n",
            "Accuracy: 0.7756225404253086\n",
            "Accuracy: 0.775836860224352\n",
            "Accuracy: 0.776223300544309\n",
            "Accuracy: 0.776415834433577\n",
            "Accuracy: 0.7766924313470429\n",
            "Accuracy: 0.7768820116306816\n",
            "Accuracy: 0.7770760540842678\n",
            "Accuracy: 0.7773123370276557\n",
            "Accuracy: 0.777527752840618\n",
            "Accuracy: 0.7780601253825656\n",
            "Accuracy: 0.7772906473517089\n",
            "Accuracy: 0.7778942909214522\n",
            "Accuracy: 0.778318821730679\n",
            "Accuracy: 0.7785002610396817\n",
            "Accuracy: 0.7787386263423784\n",
            "Accuracy: 0.7790811256222103\n",
            "Accuracy: 0.7788659114501664\n",
            "Accuracy: 0.7790515725677077\n",
            "Accuracy: 0.7792383593047726\n",
            "Accuracy: 0.7794134075282723\n",
            "Accuracy: 0.7779251268338262\n",
            "Accuracy: 0.7781307421584818\n",
            "Accuracy: 0.7783449263572693\n",
            "Accuracy: 0.7788428212416932\n",
            "Accuracy: 0.7793796569978527\n",
            "Accuracy: 0.7795638189744697\n",
            "Accuracy: 0.7798704707842704\n",
            "Accuracy: 0.7800453308381532\n",
            "Accuracy: 0.7795265587608958\n",
            "Accuracy: 0.7791774684846089\n",
            "Accuracy: 0.7775497273116448\n",
            "Accuracy: 0.7777290609665215\n",
            "Accuracy: 0.777622837834544\n",
            "Accuracy: 0.7778296041365115\n",
            "Accuracy: 0.7780231674820262\n",
            "Accuracy: 0.7779106337999561\n",
            "Accuracy: 0.7764030036092724\n",
            "Accuracy: 0.7765942220504467\n",
            "Accuracy: 0.7767677820856919\n",
            "Accuracy: 0.7769633414489883\n",
            "Accuracy: 0.7766582559386586\n",
            "Accuracy: 0.7771770162933369\n",
            "Accuracy: 0.7774084711376624\n",
            "Accuracy: 0.777587617286528\n",
            "Accuracy: 0.7777594180791745\n",
            "Accuracy: 0.7779497534186397\n",
            "Accuracy: 0.7779327831172704\n",
            "Accuracy: 0.7783361001312733\n",
            "Accuracy: 0.7785775923074927\n",
            "Accuracy: 0.7787415355592224\n",
            "Accuracy: 0.7790918143156444\n",
            "Accuracy: 0.7794333544403019\n",
            "Accuracy: 0.7796437120731966\n",
            "Accuracy: 0.7793362896724287\n",
            "Accuracy: 0.7795086725630983\n",
            "Accuracy: 0.778223014345356\n",
            "Average train loss: 0.05059986162538384, avarage valid loss: 0.0581435463839684\n",
            "Average accuracy rate: 0.778223014345356\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Evaluating with validation set\n",
            "Accuracy: 0.9300123453140259\n",
            "Accuracy: 0.89568692445755\n",
            "Accuracy: 0.8877305189768473\n",
            "Accuracy: 0.7060914784669876\n",
            "Accuracy: 0.7371256470680236\n",
            "Accuracy: 0.7563496828079224\n",
            "Accuracy: 0.6974817940167019\n",
            "Accuracy: 0.6994268447160721\n",
            "Accuracy: 0.7207329538133409\n",
            "Accuracy: 0.7329186975955964\n",
            "Accuracy: 0.7562334808436307\n",
            "Accuracy: 0.7747814406951269\n",
            "Accuracy: 0.7831984850076529\n",
            "Accuracy: 0.7881862819194794\n",
            "Accuracy: 0.7942001700401307\n",
            "Accuracy: 0.8010121434926987\n",
            "Accuracy: 0.7956249889205483\n",
            "Accuracy: 0.798811286687851\n",
            "Accuracy: 0.7923728823661804\n",
            "Accuracy: 0.7952981144189835\n",
            "Accuracy: 0.7988560937699818\n",
            "Accuracy: 0.7857088094407861\n",
            "Accuracy: 0.7760446797246519\n",
            "Accuracy: 0.7636256689826647\n",
            "Accuracy: 0.7684360837936401\n",
            "Accuracy: 0.7716512955152072\n",
            "Accuracy: 0.7630601039639225\n",
            "Accuracy: 0.7682718592030662\n",
            "Accuracy: 0.747892961419862\n",
            "Accuracy: 0.72867684563001\n",
            "Accuracy: 0.7192049737899534\n",
            "Accuracy: 0.7233249731361866\n",
            "Accuracy: 0.7244684389143279\n",
            "Accuracy: 0.708361459128997\n",
            "Accuracy: 0.7127846837043762\n",
            "Accuracy: 0.7168926447629929\n",
            "Accuracy: 0.7207815920984423\n",
            "Accuracy: 0.7117302072675604\n",
            "Accuracy: 0.7092544191922897\n",
            "Accuracy: 0.7135779350996018\n",
            "Accuracy: 0.7170976734742885\n",
            "Accuracy: 0.7161136056695666\n",
            "Accuracy: 0.7192612914151923\n",
            "Accuracy: 0.7193324403329329\n",
            "Accuracy: 0.7221869336234199\n",
            "Accuracy: 0.7250343898068303\n",
            "Accuracy: 0.7199617028236389\n",
            "Accuracy: 0.7130738434692224\n",
            "Accuracy: 0.716266083474062\n",
            "Accuracy: 0.7189744019508362\n",
            "Accuracy: 0.7226245286417943\n",
            "Accuracy: 0.7271766387499295\n",
            "Accuracy: 0.7299977383523617\n",
            "Accuracy: 0.7322431493688513\n",
            "Accuracy: 0.7275276086547158\n",
            "Accuracy: 0.7314642369747162\n",
            "Accuracy: 0.7261817549404345\n",
            "Accuracy: 0.7284768141549209\n",
            "Accuracy: 0.7309049327494734\n",
            "Accuracy: 0.7329696118831635\n",
            "Accuracy: 0.7290505415103474\n",
            "Accuracy: 0.7246391725155615\n",
            "Accuracy: 0.7269405533397009\n",
            "Accuracy: 0.7303173495456576\n",
            "Accuracy: 0.7319621269519513\n",
            "Accuracy: 0.7337756202076421\n",
            "Accuracy: 0.7355303773239478\n",
            "Accuracy: 0.7372436900349224\n",
            "Accuracy: 0.7389712083166924\n",
            "Accuracy: 0.7357046229498727\n",
            "Accuracy: 0.7375506498444249\n",
            "Accuracy: 0.7341121460000674\n",
            "Accuracy: 0.7348403857178885\n",
            "Accuracy: 0.7365155566383053\n",
            "Accuracy: 0.7363782588640849\n",
            "Accuracy: 0.7378892820132407\n",
            "Accuracy: 0.7399853615017681\n",
            "Accuracy: 0.7418187879599057\n",
            "Accuracy: 0.7409944157057171\n",
            "Accuracy: 0.7431378930807113\n",
            "Accuracy: 0.7462331969061016\n",
            "Accuracy: 0.7475168566878249\n",
            "Accuracy: 0.7439112009772335\n",
            "Accuracy: 0.741182090980666\n",
            "Accuracy: 0.7437884583192713\n",
            "Accuracy: 0.7451472060624943\n",
            "Accuracy: 0.7385744966309646\n",
            "Accuracy: 0.7399068989536979\n",
            "Accuracy: 0.7412204300419668\n",
            "Accuracy: 0.7422970520125495\n",
            "Accuracy: 0.7402792147227696\n",
            "Accuracy: 0.741487109142801\n",
            "Accuracy: 0.7428280858583348\n",
            "Accuracy: 0.7447943167483553\n",
            "Accuracy: 0.7431575574372944\n",
            "Accuracy: 0.7457450199872255\n",
            "Accuracy: 0.7449635181230368\n",
            "Accuracy: 0.7443410583904811\n",
            "Accuracy: 0.7393607226285067\n",
            "Accuracy: 0.7415943896770477\n",
            "Accuracy: 0.7428068028818263\n",
            "Accuracy: 0.7439196080553765\n",
            "Accuracy: 0.7449637437329709\n",
            "Accuracy: 0.7428024565944304\n",
            "Accuracy: 0.7387335896492004\n",
            "Accuracy: 0.7392667605067199\n",
            "Accuracy: 0.7403166010000996\n",
            "Accuracy: 0.741350237969999\n",
            "Accuracy: 0.743238112248412\n",
            "Accuracy: 0.745068819956346\n",
            "Accuracy: 0.7464494699830407\n",
            "Accuracy: 0.7475708621953215\n",
            "Accuracy: 0.7456853057430909\n",
            "Accuracy: 0.7469847798347473\n",
            "Accuracy: 0.7478913452314294\n",
            "Accuracy: 0.7492033279147642\n",
            "Accuracy: 0.7450101589545225\n",
            "Accuracy: 0.7471454865851644\n",
            "Accuracy: 0.7480219472356203\n",
            "Accuracy: 0.7491289133826892\n",
            "Accuracy: 0.7501789114691995\n",
            "Accuracy: 0.7510078373502512\n",
            "Accuracy: 0.7525196996161608\n",
            "Accuracy: 0.7533634852978491\n",
            "Accuracy: 0.7532654762268066\n",
            "Accuracy: 0.7550677988264296\n",
            "Accuracy: 0.7557545820559104\n",
            "Accuracy: 0.7551250220276415\n",
            "Accuracy: 0.7564051072726878\n",
            "Accuracy: 0.7576567131739397\n",
            "Accuracy: 0.7584410082292921\n",
            "Accuracy: 0.7592269082864126\n",
            "Accuracy: 0.7599171337328459\n",
            "Accuracy: 0.761516112000195\n",
            "Accuracy: 0.7620237178272671\n",
            "Accuracy: 0.7627620929304291\n",
            "Accuracy: 0.7626229685588475\n",
            "Accuracy: 0.7595665489417919\n",
            "Accuracy: 0.7581374610928323\n",
            "Accuracy: 0.7593897598130362\n",
            "Accuracy: 0.7599821171016558\n",
            "Accuracy: 0.7572600371401075\n",
            "Accuracy: 0.7582525160762813\n",
            "Accuracy: 0.7588973285423385\n",
            "Accuracy: 0.7561630820405895\n",
            "Accuracy: 0.7568758587314658\n",
            "Accuracy: 0.758056025115811\n",
            "Accuracy: 0.755683575933044\n",
            "Accuracy: 0.7526585103681424\n",
            "Accuracy: 0.7508848158518473\n",
            "Accuracy: 0.7517650944507674\n",
            "Accuracy: 0.7526748603896091\n",
            "Accuracy: 0.7533321754605162\n",
            "Accuracy: 0.7544299538259382\n",
            "Accuracy: 0.7558042080171646\n",
            "Accuracy: 0.7564262453562174\n",
            "Accuracy: 0.7570307812872966\n",
            "Accuracy: 0.7547106358069408\n",
            "Accuracy: 0.7562415123735584\n",
            "Accuracy: 0.7544798232614994\n",
            "Accuracy: 0.7526275537769247\n",
            "Accuracy: 0.7523883323610565\n",
            "Accuracy: 0.7531280491981038\n",
            "Accuracy: 0.7545069532423485\n",
            "Accuracy: 0.7523927811420326\n",
            "Accuracy: 0.7529864232224154\n",
            "Accuracy: 0.7534630469933241\n",
            "Accuracy: 0.7540463819390252\n",
            "Accuracy: 0.7546699241068237\n",
            "Accuracy: 0.7552478723666247\n",
            "Accuracy: 0.755804689655527\n",
            "Accuracy: 0.755385011434555\n",
            "Accuracy: 0.7559482052147044\n",
            "Accuracy: 0.7565659986830305\n",
            "Accuracy: 0.7573101540974209\n",
            "Accuracy: 0.7555005387826399\n",
            "Accuracy: 0.7561143044024538\n",
            "Accuracy: 0.756688251923979\n",
            "Accuracy: 0.7574734857628466\n",
            "Accuracy: 0.7588114049699571\n",
            "Accuracy: 0.7592146893891182\n",
            "Accuracy: 0.7597193544382578\n",
            "Accuracy: 0.7601778784084842\n",
            "Accuracy: 0.760787774687228\n",
            "Accuracy: 0.7612868866404972\n",
            "Accuracy: 0.7582414259192765\n",
            "Accuracy: 0.7572007797618601\n",
            "Accuracy: 0.7578220684477623\n",
            "Accuracy: 0.7583702304375866\n",
            "Accuracy: 0.7589019188755437\n",
            "Accuracy: 0.7598511323878903\n",
            "Accuracy: 0.7604173275952538\n",
            "Accuracy: 0.7609439183393291\n",
            "Accuracy: 0.7614055580085086\n",
            "Accuracy: 0.761862173752907\n",
            "Accuracy: 0.7609471128303178\n",
            "Accuracy: 0.761418392513004\n",
            "Accuracy: 0.7619561616218451\n",
            "Accuracy: 0.7625246952526533\n",
            "Accuracy: 0.7605424183607101\n",
            "Accuracy: 0.7598152415669379\n",
            "Accuracy: 0.760268633023347\n",
            "Accuracy: 0.760848426466505\n",
            "Accuracy: 0.7615204196350247\n",
            "Accuracy: 0.7620121999484737\n",
            "Accuracy: 0.7624557588285613\n",
            "Accuracy: 0.7622543760543861\n",
            "Accuracy: 0.7617723491902535\n",
            "Accuracy: 0.7623572426549555\n",
            "Accuracy: 0.7618773917357127\n",
            "Accuracy: 0.7616527213304529\n",
            "Accuracy: 0.7607755832514673\n",
            "Accuracy: 0.7612897098903925\n",
            "Accuracy: 0.7613884277432879\n",
            "Accuracy: 0.7623685432034869\n",
            "Accuracy: 0.7616967164255954\n",
            "Accuracy: 0.7621158094999427\n",
            "Accuracy: 0.7625870975332523\n",
            "Accuracy: 0.7631138459732544\n",
            "Accuracy: 0.7606305666945198\n",
            "Accuracy: 0.7591587771657365\n",
            "Accuracy: 0.7595784344651677\n",
            "Accuracy: 0.7600514960930487\n",
            "Accuracy: 0.7604577999029841\n",
            "Accuracy: 0.7608689493603177\n",
            "Accuracy: 0.7616092153882559\n",
            "Accuracy: 0.7610835075903569\n",
            "Accuracy: 0.7614837061417731\n",
            "Accuracy: 0.761880930854764\n",
            "Accuracy: 0.7624405850534853\n",
            "Accuracy: 0.763448741528895\n",
            "Accuracy: 0.7619340743484169\n",
            "Accuracy: 0.76259321562722\n",
            "Accuracy: 0.7607812985905216\n",
            "Accuracy: 0.7612540389629121\n",
            "Accuracy: 0.761467573501296\n",
            "Accuracy: 0.7618451110924347\n",
            "Accuracy: 0.7622959688931954\n",
            "Accuracy: 0.7628639991313344\n",
            "Accuracy: 0.7632722335557143\n",
            "Accuracy: 0.7631712724064396\n",
            "Accuracy: 0.7635438553065308\n",
            "Accuracy: 0.7630578301571034\n",
            "Accuracy: 0.7637357074217718\n",
            "Accuracy: 0.7631564785023125\n",
            "Accuracy: 0.7639411879264242\n",
            "Accuracy: 0.7624089416222051\n",
            "Accuracy: 0.7628345895678766\n",
            "Accuracy: 0.7616560983370586\n",
            "Accuracy: 0.7609795005321502\n",
            "Accuracy: 0.7597489767815487\n",
            "Accuracy: 0.7585540964962945\n",
            "Accuracy: 0.7589262709787241\n",
            "Accuracy: 0.7592946980881878\n",
            "Accuracy: 0.7602356041178984\n",
            "Accuracy: 0.7606125904712826\n",
            "Accuracy: 0.7609675213520629\n",
            "Accuracy: 0.7613243483757788\n",
            "Accuracy: 0.7616704520571647\n",
            "Accuracy: 0.7620312848916421\n",
            "Accuracy: 0.7629311968540323\n",
            "Accuracy: 0.7632749565230071\n",
            "Accuracy: 0.7615965591184086\n",
            "Accuracy: 0.7619523717598482\n",
            "Accuracy: 0.7622976240122094\n",
            "Accuracy: 0.7626599586547765\n",
            "Accuracy: 0.763154984859938\n",
            "Accuracy: 0.7634841390510103\n",
            "Accuracy: 0.7628853649011775\n",
            "Accuracy: 0.7636528015136719\n",
            "Accuracy: 0.7644779686558291\n",
            "Accuracy: 0.7649213461314931\n",
            "Accuracy: 0.7654176323841779\n",
            "Accuracy: 0.7648566252123701\n",
            "Accuracy: 0.7654438586668535\n",
            "Accuracy: 0.766020236239917\n",
            "Accuracy: 0.7663387770256841\n",
            "Accuracy: 0.7650674840957998\n",
            "Accuracy: 0.7646386371291239\n",
            "Accuracy: 0.7642953429903303\n",
            "Accuracy: 0.764565591710318\n",
            "Accuracy: 0.764875776378821\n",
            "Accuracy: 0.7651860288512159\n",
            "Accuracy: 0.7654906838712557\n",
            "Accuracy: 0.7657950639724731\n",
            "Accuracy: 0.7664007390295708\n",
            "Accuracy: 0.7667693578823103\n",
            "Accuracy: 0.7670811334004005\n",
            "Accuracy: 0.767554549816158\n",
            "Accuracy: 0.7679971643562975\n",
            "Accuracy: 0.7682903019832992\n",
            "Accuracy: 0.7685832248566902\n",
            "Accuracy: 0.7682067218900948\n",
            "Accuracy: 0.7686215969575506\n",
            "Accuracy: 0.7689061061810639\n",
            "Accuracy: 0.7694329373739861\n",
            "Accuracy: 0.7679108120375611\n",
            "Accuracy: 0.7674439253423038\n",
            "Accuracy: 0.7678143902367174\n",
            "Accuracy: 0.7681290102005005\n",
            "Accuracy: 0.7684131601324113\n",
            "Accuracy: 0.7672208344304798\n",
            "Accuracy: 0.7674974956528189\n",
            "Accuracy: 0.7663881055226451\n",
            "Accuracy: 0.7656865080849069\n",
            "Accuracy: 0.7659685512383779\n",
            "Accuracy: 0.7662850973660472\n",
            "Accuracy: 0.7665677966622563\n",
            "Accuracy: 0.766993309494747\n",
            "Accuracy: 0.7672810600649926\n",
            "Accuracy: 0.7675495676672344\n",
            "Accuracy: 0.7679969477347839\n",
            "Accuracy: 0.7685102894664192\n",
            "Accuracy: 0.7690147611365956\n",
            "Accuracy: 0.769308256535303\n",
            "Accuracy: 0.7699034321534483\n",
            "Accuracy: 0.7680720854256807\n",
            "Accuracy: 0.7685940100711847\n",
            "Accuracy: 0.7685769983219877\n",
            "Accuracy: 0.7688710445538163\n",
            "Accuracy: 0.7691381588531803\n",
            "Accuracy: 0.7693927152926877\n",
            "Accuracy: 0.7698816968929657\n",
            "Accuracy: 0.7701381408138039\n",
            "Accuracy: 0.770531816299145\n",
            "Accuracy: 0.7712348041358901\n",
            "Accuracy: 0.7700517635097562\n",
            "Accuracy: 0.7704507688560137\n",
            "Accuracy: 0.771082375730787\n",
            "Accuracy: 0.7715502307270512\n",
            "Accuracy: 0.7704464341218378\n",
            "Accuracy: 0.7707015776849655\n",
            "Accuracy: 0.7709854823333007\n",
            "Accuracy: 0.7702313251124171\n",
            "Accuracy: 0.7707051293173832\n",
            "Accuracy: 0.7709508894809655\n",
            "Accuracy: 0.7716074308231252\n",
            "Accuracy: 0.7719750228012807\n",
            "Accuracy: 0.7723141760952705\n",
            "Accuracy: 0.7725615262985229\n",
            "Accuracy: 0.772700811236485\n",
            "Accuracy: 0.7733129675973925\n",
            "Accuracy: 0.7737762104318023\n",
            "Accuracy: 0.7740091587914977\n",
            "Accuracy: 0.7745500155117201\n",
            "Accuracy: 0.7748372901037249\n",
            "Accuracy: 0.7754347423998698\n",
            "Accuracy: 0.775669619098477\n",
            "Accuracy: 0.7761705241094005\n",
            "Accuracy: 0.7764750576019287\n",
            "Accuracy: 0.7763081357689665\n",
            "Accuracy: 0.7760417092252861\n",
            "Accuracy: 0.7762564768534544\n",
            "Accuracy: 0.7764865149885921\n",
            "Accuracy: 0.7768885850906372\n",
            "Accuracy: 0.7770966607868002\n",
            "Accuracy: 0.7773887124382147\n",
            "Accuracy: 0.7775936612869774\n",
            "Accuracy: 0.7778029866869404\n",
            "Accuracy: 0.7780544675058789\n",
            "Accuracy: 0.7782849971607452\n",
            "Accuracy: 0.7788324000427077\n",
            "Accuracy: 0.7780437196910217\n",
            "Accuracy: 0.7786446570695101\n",
            "Accuracy: 0.7790501507994246\n",
            "Accuracy: 0.7792465268262748\n",
            "Accuracy: 0.7794997474478116\n",
            "Accuracy: 0.779857021311055\n",
            "Accuracy: 0.7796229072379549\n",
            "Accuracy: 0.7798232743868957\n",
            "Accuracy: 0.780024687716903\n",
            "Accuracy: 0.7802142838957489\n",
            "Accuracy: 0.7787072388480241\n",
            "Accuracy: 0.7788941903229065\n",
            "Accuracy: 0.7791228672663371\n",
            "Accuracy: 0.7796351776478139\n",
            "Accuracy: 0.7801534708994453\n",
            "Accuracy: 0.7803519831763374\n",
            "Accuracy: 0.7806729094019665\n",
            "Accuracy: 0.7808619687431737\n",
            "Accuracy: 0.7803247851023837\n",
            "Accuracy: 0.7799573796581848\n",
            "Accuracy: 0.7783114188022464\n",
            "Accuracy: 0.7785049101027349\n",
            "Accuracy: 0.7783805724862334\n",
            "Accuracy: 0.7786014331745977\n",
            "Accuracy: 0.7788090180673033\n",
            "Accuracy: 0.778678484337846\n",
            "Accuracy: 0.7771529464917808\n",
            "Accuracy: 0.7773581347404382\n",
            "Accuracy: 0.7775455931263506\n",
            "Accuracy: 0.7777549799303619\n",
            "Accuracy: 0.7774321086534107\n",
            "Accuracy: 0.7779331735245467\n",
            "Accuracy: 0.778178405610821\n",
            "Accuracy: 0.7783712594196049\n",
            "Accuracy: 0.7785566988159487\n",
            "Accuracy: 0.7787606042234143\n",
            "Accuracy: 0.7787260674593741\n",
            "Accuracy: 0.7791119056940079\n",
            "Accuracy: 0.7793669201192118\n",
            "Accuracy: 0.7795443181967854\n",
            "Accuracy: 0.7799079850057219\n",
            "Accuracy: 0.7802628470529424\n",
            "Accuracy: 0.7804864606739562\n",
            "Accuracy: 0.7801616961145635\n",
            "Accuracy: 0.7803472798930925\n",
            "Accuracy: 0.7790443745313906\n",
            "Average train loss: 0.05006904914205591, avarage valid loss: 0.0588226529132169\n",
            "Average accuracy rate: 0.7790443745313906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    model_classical.test()\n",
        "    precision, recall, f1 = model_classical.calculate_metrics()\n",
        "\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZtOFOxd04cS",
        "outputId": "c546f635-84f2-42f8-a1e5-badf36c03ae3"
      },
      "id": "VZtOFOxd04cS",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average loss: 0.00013393167315459834\n",
            "Accuracy: 0.7286232709884644\n",
            "Average loss: 0.00022443188581524825\n",
            "Accuracy: 0.7527729272842407\n",
            "Average loss: 0.0006149207764282459\n",
            "Accuracy: 0.6807226737340292\n",
            "Average loss: 0.0006480163553865945\n",
            "Accuracy: 0.7268166989088058\n",
            "Average loss: 0.0006727203863059602\n",
            "Accuracy: 0.7581432223320007\n",
            "Average loss: 0.000694618262804863\n",
            "Accuracy: 0.7801640729109446\n",
            "Average loss: 0.0007209617122099167\n",
            "Accuracy: 0.794375513281141\n",
            "Average loss: 0.0007406519816779509\n",
            "Accuracy: 0.8070718869566917\n",
            "Average loss: 0.0007692353209344352\n",
            "Accuracy: 0.814578546418084\n",
            "Average loss: 0.0007773415884003044\n",
            "Accuracy: 0.8264443039894104\n",
            "Average loss: 0.000809547802001783\n",
            "Accuracy: 0.830124248157848\n",
            "Average loss: 0.0008117187497058383\n",
            "Accuracy: 0.8414013435443243\n",
            "Average loss: 0.0008216300333293564\n",
            "Accuracy: 0.8479224947782663\n",
            "Average loss: 0.0008477673174461304\n",
            "Accuracy: 0.8502220341137477\n",
            "Average loss: 0.0008478334855814266\n",
            "Accuracy: 0.8598051031430562\n",
            "Average loss: 0.0008728132538569143\n",
            "Accuracy: 0.8612423278391361\n",
            "Average loss: 0.0008970707133302335\n",
            "Accuracy: 0.8626108695479\n",
            "Average loss: 0.0009021310624897213\n",
            "Accuracy: 0.8673130538728502\n",
            "Average loss: 0.0009320234528564558\n",
            "Accuracy: 0.8675488484533209\n",
            "Average loss: 0.0009506923647078532\n",
            "Accuracy: 0.8691054612398148\n",
            "Average loss: 0.0009876408819453478\n",
            "Accuracy: 0.868551030045464\n",
            "Average loss: 0.0011097366717965598\n",
            "Accuracy: 0.8627483357082714\n",
            "Average loss: 0.001133321814046301\n",
            "Accuracy: 0.8637644648551941\n",
            "Average loss: 0.0014229846142304113\n",
            "Accuracy: 0.8528119499484698\n",
            "Average loss: 0.0014249653665568231\n",
            "Accuracy: 0.8573793745040894\n",
            "Average loss: 0.0027310268495678363\n",
            "Accuracy: 0.8302706571725699\n",
            "Average loss: 0.0027566901268384265\n",
            "Accuracy: 0.832157227728102\n",
            "Average loss: 0.002786947622912632\n",
            "Accuracy: 0.8335449184690203\n",
            "Average loss: 0.00279439810810578\n",
            "Accuracy: 0.8370776320325917\n",
            "Average loss: 0.002829748814261487\n",
            "Accuracy: 0.8378609915574392\n",
            "Average loss: 0.002942580815694627\n",
            "Accuracy: 0.8350562914725272\n",
            "Average loss: 0.003481828616358831\n",
            "Accuracy: 0.8231940921396017\n",
            "Average loss: 0.0035186396908279786\n",
            "Accuracy: 0.8242405671061892\n",
            "Average loss: 0.0036829642266572437\n",
            "Accuracy: 0.8205689261941349\n",
            "Average loss: 0.0037751469076217135\n",
            "Accuracy: 0.8192629132952008\n",
            "Average loss: 0.0037775184579492437\n",
            "Accuracy: 0.8232802864578035\n",
            "Average loss: 0.004101188361111293\n",
            "Accuracy: 0.8166545130111076\n",
            "Average loss: 0.004134149875049267\n",
            "Accuracy: 0.8179365600410261\n",
            "Average loss: 0.004153681513024622\n",
            "Accuracy: 0.8199475835531186\n",
            "Average loss: 0.004539714265899043\n",
            "Accuracy: 0.8129307374358177\n",
            "Average loss: 0.004541642432146856\n",
            "Accuracy: 0.8166992213667893\n",
            "Average loss: 0.0045673724057428994\n",
            "Accuracy: 0.8182314733664194\n",
            "Average loss: 0.004599932678657507\n",
            "Accuracy: 0.8193468867346297\n",
            "Average loss: 0.004626132234181746\n",
            "Accuracy: 0.820724759589542\n",
            "Average loss: 0.0046548927995811425\n",
            "Accuracy: 0.8219140701823764\n",
            "Average loss: 0.004678233065324759\n",
            "Accuracy: 0.8233227211496105\n",
            "Average loss: 0.00468346043866981\n",
            "Accuracy: 0.8259411035700047\n",
            "Average loss: 0.004829735861228362\n",
            "Accuracy: 0.8236588562528292\n",
            "Average loss: 0.004865607172160242\n",
            "Accuracy: 0.8243914429022341\n",
            "Average loss: 0.004894056657647569\n",
            "Accuracy: 0.8254021286964417\n",
            "Average loss: 0.004931147099359361\n",
            "Accuracy: 0.8260253995072608\n",
            "Average loss: 0.004964439107877807\n",
            "Accuracy: 0.8267691227105948\n",
            "Average loss: 0.005138190081934005\n",
            "Accuracy: 0.8242056144858306\n",
            "Average loss: 0.00530293388555725\n",
            "Accuracy: 0.8218873904811012\n",
            "Average loss: 0.005303001747659703\n",
            "Accuracy: 0.8250147353519093\n",
            "Average loss: 0.005613315449289342\n",
            "Accuracy: 0.8207630908914975\n",
            "Average loss: 0.005635223827018153\n",
            "Accuracy: 0.8219820198259855\n",
            "Average loss: 0.006038556719005744\n",
            "Accuracy: 0.8169316937183512\n",
            "Average loss: 0.006074954449801093\n",
            "Accuracy: 0.8176367292969914\n",
            "Average loss: 0.006155517187094452\n",
            "Accuracy: 0.8171682208776474\n",
            "Average loss: 0.006190389904432089\n",
            "Accuracy: 0.8178953684744288\n",
            "Average loss: 0.006190779326815817\n",
            "Accuracy: 0.8205965190164505\n",
            "Average loss: 0.006796076524471751\n",
            "Accuracy: 0.8142867409993731\n",
            "Average loss: 0.0068262634512815034\n",
            "Accuracy: 0.8151754336431623\n",
            "Average loss: 0.006847717386752753\n",
            "Accuracy: 0.8163479098906884\n",
            "Average loss: 0.0068822924352233954\n",
            "Accuracy: 0.8170413736141089\n",
            "Average loss: 0.00691877340354173\n",
            "Accuracy: 0.817658176173025\n",
            "Average loss: 0.007775991195145563\n",
            "Accuracy: 0.810243249816053\n",
            "Average loss: 0.007812508194289943\n",
            "Accuracy: 0.8109396842942722\n",
            "Average loss: 0.007837636599376821\n",
            "Accuracy: 0.8119612957750048\n",
            "Average loss: 0.00805237952303198\n",
            "Accuracy: 0.8097698780852305\n",
            "Average loss: 0.00808616990270377\n",
            "Accuracy: 0.8105187697543038\n",
            "Average loss: 0.00812309696772079\n",
            "Accuracy: 0.8111623999190657\n",
            "Average loss: 0.008155972400194647\n",
            "Accuracy: 0.8118973430749532\n",
            "Average loss: 0.00839304082505916\n",
            "Accuracy: 0.8095913736025492\n",
            "Average loss: 0.008421610757813607\n",
            "Accuracy: 0.8104475556235564\n",
            "Average loss: 0.008451916666761063\n",
            "Accuracy: 0.8112327750627096\n",
            "Average loss: 0.008487756553506365\n",
            "Accuracy: 0.8118530871012272\n",
            "Average loss: 0.008523164013551162\n",
            "Accuracy: 0.812468448017217\n",
            "Average loss: 0.008557125602113424\n",
            "Accuracy: 0.8131044074892998\n",
            "Average loss: 0.008564853999641632\n",
            "Accuracy: 0.8146069557578476\n",
            "Average loss: 0.00876868974577874\n",
            "Accuracy: 0.8127850548523229\n",
            "Average loss: 0.008793426827291912\n",
            "Accuracy: 0.8136354921812035\n",
            "Average loss: 0.008823626333770979\n",
            "Accuracy: 0.8143200249899001\n",
            "Average loss: 0.009244819285089672\n",
            "Accuracy: 0.8108427216024959\n",
            "Average loss: 0.009274244637628904\n",
            "Accuracy: 0.8115631397380385\n",
            "Average loss: 0.009302721649377108\n",
            "Accuracy: 0.8122907493306303\n",
            "Average loss: 0.009330435031130568\n",
            "Accuracy: 0.8130210177464918\n",
            "Average loss: 0.009413988222495834\n",
            "Accuracy: 0.8127135400022014\n",
            "Average loss: 0.010749952192773202\n",
            "Accuracy: 0.8052712380886078\n",
            "Average loss: 0.010782571981743365\n",
            "Accuracy: 0.8059393776642098\n",
            "Average loss: 0.010835374450870426\n",
            "Accuracy: 0.8061966079732646\n",
            "Average loss: 0.011044707889569265\n",
            "Accuracy: 0.8046324112082041\n",
            "Average loss: 0.011081839931870402\n",
            "Accuracy: 0.8051906709975385\n",
            "Average loss: 0.011106813835085189\n",
            "Accuracy: 0.8060077635865462\n",
            "Average loss: 0.011136529451341186\n",
            "Accuracy: 0.8066969830542803\n",
            "Average loss: 0.011173231063606558\n",
            "Accuracy: 0.807225256236558\n",
            "Average loss: 0.011183645117178946\n",
            "Accuracy: 0.8084201721512542\n",
            "Average loss: 0.011780248323197068\n",
            "Accuracy: 0.8045698542787572\n",
            "Average loss: 0.011813940807237154\n",
            "Accuracy: 0.805163032412529\n",
            "Average loss: 0.011846155064285551\n",
            "Accuracy: 0.8057743594197944\n",
            "Average loss: 0.011882385397798427\n",
            "Accuracy: 0.8062947526866314\n",
            "Average loss: 0.011894171526896903\n",
            "Accuracy: 0.8073937956569264\n",
            "Average loss: 0.011925809113550135\n",
            "Accuracy: 0.8079775451467588\n",
            "Average loss: 0.011958703815315214\n",
            "Accuracy: 0.8085254618099758\n",
            "Average loss: 0.011964852508035398\n",
            "Accuracy: 0.8097832753973188\n",
            "Average loss: 0.01206520308787287\n",
            "Accuracy: 0.809365635163316\n",
            "Average loss: 0.012097040520449987\n",
            "Accuracy: 0.8099056554061396\n",
            "Average loss: 0.012432455046389184\n",
            "Accuracy: 0.8077096485216683\n",
            "Average loss: 0.012457935355118159\n",
            "Accuracy: 0.8083816723390059\n",
            "Average loss: 0.012475513623496404\n",
            "Accuracy: 0.8092222444645993\n",
            "Average loss: 0.012536502022810122\n",
            "Accuracy: 0.8092905469238758\n",
            "Average loss: 0.012658243692017205\n",
            "Accuracy: 0.8086885757150903\n",
            "Average loss: 0.01296890996965841\n",
            "Accuracy: 0.8067412026095808\n",
            "Average loss: 0.013004874312900937\n",
            "Accuracy: 0.80719887536505\n",
            "Average loss: 0.013036431428740686\n",
            "Accuracy: 0.8077253642781027\n",
            "Average loss: 0.013071850809714439\n",
            "Accuracy: 0.8081759433461051\n",
            "Average loss: 0.013852962846768362\n",
            "Accuracy: 0.8042475752911326\n",
            "Average loss: 0.01428400281250261\n",
            "Accuracy: 0.8018014290753532\n",
            "Average loss: 0.014377047162199189\n",
            "Accuracy: 0.8015681564807892\n",
            "Average loss: 0.014379621728212664\n",
            "Accuracy: 0.8028971341030657\n",
            "Average loss: 0.014406705283767135\n",
            "Accuracy: 0.8035124462158953\n",
            "Average loss: 0.014436863784794696\n",
            "Accuracy: 0.804062944602191\n",
            "Average loss: 0.014462402144449809\n",
            "Accuracy: 0.8046874168419069\n",
            "Average loss: 0.014664029865596648\n",
            "Accuracy: 0.803586151599884\n",
            "Average loss: 0.01471869629281062\n",
            "Accuracy: 0.8037689843821147\n",
            "Average loss: 0.014801191983293037\n",
            "Accuracy: 0.8036370727959581\n",
            "Average loss: 0.014821857461072618\n",
            "Accuracy: 0.8043383527547121\n",
            "Average loss: 0.014856283300309215\n",
            "Accuracy: 0.8047885543616243\n",
            "Average loss: 0.014892983735928017\n",
            "Accuracy: 0.8051974250720098\n",
            "Average loss: 0.014929867587577575\n",
            "Accuracy: 0.8055973471576021\n",
            "Average loss: 0.014959815839126638\n",
            "Accuracy: 0.806097923354669\n",
            "Average loss: 0.015053406670737852\n",
            "Accuracy: 0.8058501603011798\n",
            "Average loss: 0.015060047803108762\n",
            "Accuracy: 0.8068480700699251\n",
            "Average loss: 0.015460684239117587\n",
            "Accuracy: 0.8048020844106321\n",
            "Average loss: 0.015784049322672484\n",
            "Accuracy: 0.8031368146047873\n",
            "Average loss: 0.01581597241421128\n",
            "Accuracy: 0.8036066911516399\n",
            "Average loss: 0.015852453230337876\n",
            "Accuracy: 0.8040035081946332\n",
            "Average loss: 0.01586535226833359\n",
            "Accuracy: 0.8048076633926776\n",
            "Average loss: 0.01590160729325229\n",
            "Accuracy: 0.8051933684519359\n",
            "Average loss: 0.015932227126052943\n",
            "Accuracy: 0.805654710066234\n",
            "Average loss: 0.01593887852228008\n",
            "Accuracy: 0.8065974473113745\n",
            "Average loss: 0.015975890551174127\n",
            "Accuracy: 0.8069522901848479\n",
            "Average loss: 0.016139226086031667\n",
            "Accuracy: 0.806211723635594\n",
            "Average loss: 0.016174599359723422\n",
            "Accuracy: 0.8065863596981969\n",
            "Average loss: 0.01621026485290935\n",
            "Accuracy: 0.8069519261791281\n",
            "Average loss: 0.01635513593188612\n",
            "Accuracy: 0.8063451643703746\n",
            "Average loss: 0.016389273251932138\n",
            "Accuracy: 0.8067279139886031\n",
            "Average loss: 0.016405265306466167\n",
            "Accuracy: 0.8073956854391418\n",
            "Average loss: 0.016775811536921894\n",
            "Accuracy: 0.8056704465548198\n",
            "Average loss: 0.016777124914281048\n",
            "Accuracy: 0.8067794264547082\n",
            "Average loss: 0.01698290080977774\n",
            "Accuracy: 0.805837598286177\n",
            "Average loss: 0.017017713945185788\n",
            "Accuracy: 0.8062023368536257\n",
            "Average loss: 0.01701828836972354\n",
            "Accuracy: 0.8073453578081998\n",
            "Average loss: 0.0170265049404229\n",
            "Accuracy: 0.8081546360446561\n",
            "Average loss: 0.017063432864073903\n",
            "Accuracy: 0.808470966342168\n",
            "Average loss: 0.01707164291324372\n",
            "Accuracy: 0.8092629355230149\n",
            "Average loss: 0.017103320696687883\n",
            "Accuracy: 0.8096348152884955\n",
            "Average loss: 0.017137033185991173\n",
            "Accuracy: 0.8099757740332646\n",
            "Average loss: 0.01733203881293132\n",
            "Accuracy: 0.809116818383336\n",
            "Average loss: 0.01766346433436322\n",
            "Accuracy: 0.8076508892989307\n",
            "Average loss: 0.017693338613913156\n",
            "Accuracy: 0.8080470690756668\n",
            "Average loss: 0.017730343505696154\n",
            "Accuracy: 0.8083495636659166\n",
            "Average loss: 0.017764502818584046\n",
            "Accuracy: 0.8086824805998221\n",
            "Average loss: 0.017793075831517544\n",
            "Accuracy: 0.8090823108499701\n",
            "Average loss: 0.017805689069330295\n",
            "Accuracy: 0.8097307265522968\n",
            "Average loss: 0.01785092415183743\n",
            "Accuracy: 0.8099256718230105\n",
            "Average loss: 0.017878436813132396\n",
            "Accuracy: 0.8103249371051788\n",
            "Average loss: 0.017886412659661403\n",
            "Accuracy: 0.8110554133646587\n",
            "Average loss: 0.017923012260303593\n",
            "Accuracy: 0.8113323639420902\n",
            "Average loss: 0.017925286821051354\n",
            "Accuracy: 0.8122288678821764\n",
            "Average loss: 0.017931510592000162\n",
            "Accuracy: 0.8129804428926733\n",
            "Average loss: 0.01796397409595623\n",
            "Accuracy: 0.8132891872025638\n",
            "Average loss: 0.018002424702486988\n",
            "Accuracy: 0.8135265707969666\n",
            "Average loss: 0.018003877930424073\n",
            "Accuracy: 0.8144306009156363\n",
            "Average loss: 0.018054914402969965\n",
            "Accuracy: 0.814533144235611\n",
            "Average loss: 0.01806098022161012\n",
            "Accuracy: 0.8152546906201853\n",
            "Average loss: 0.018075325961328374\n",
            "Accuracy: 0.815793618057551\n",
            "Average loss: 0.018107505589077623\n",
            "Accuracy: 0.8160795679305519\n",
            "Average loss: 0.018143262011148093\n",
            "Accuracy: 0.8163223518265619\n",
            "Average loss: 0.018473730151145434\n",
            "Accuracy: 0.8149820032699332\n",
            "Average loss: 0.018494321229880414\n",
            "Accuracy: 0.8154139315689003\n",
            "Average loss: 0.018531354156744426\n",
            "Accuracy: 0.815642815470044\n",
            "Average loss: 0.01856177526347981\n",
            "Accuracy: 0.8159418452693068\n",
            "Average loss: 0.018596510735671205\n",
            "Accuracy: 0.8161897108361528\n",
            "Average loss: 0.018631430716493747\n",
            "Accuracy: 0.8164329403831113\n",
            "Average loss: 0.018658043363553232\n",
            "Accuracy: 0.8167676881035382\n",
            "Average loss: 0.018692930695840736\n",
            "Accuracy: 0.8170056003839412\n",
            "Average loss: 0.01869300731166554\n",
            "Accuracy: 0.8179394823533518\n",
            "Average loss: 0.01870922609668036\n",
            "Accuracy: 0.818400661255184\n",
            "Average loss: 0.018744673955681232\n",
            "Accuracy: 0.8186204848489212\n",
            "Average loss: 0.01877007578798994\n",
            "Accuracy: 0.8189496214812001\n",
            "Average loss: 0.01880651505577212\n",
            "Accuracy: 0.819154276749013\n",
            "Average loss: 0.018843597154196555\n",
            "Accuracy: 0.8193504143621504\n",
            "Average loss: 0.018871284627010293\n",
            "Accuracy: 0.8196440641696636\n",
            "Average loss: 0.018888652148447496\n",
            "Accuracy: 0.8200656573991386\n",
            "Average loss: 0.01891407685267573\n",
            "Accuracy: 0.8203788345235253\n",
            "Average loss: 0.018933719181746356\n",
            "Accuracy: 0.82076112941058\n",
            "Average loss: 0.01896886007909815\n",
            "Accuracy: 0.8209632985555946\n",
            "Average loss: 0.019005335416322187\n",
            "Accuracy: 0.821150372326374\n",
            "Average loss: 0.019041780526449878\n",
            "Accuracy: 0.821335876759012\n",
            "Average loss: 0.019042231097401094\n",
            "Accuracy: 0.8221424304612792\n",
            "Average loss: 0.019079102775983146\n",
            "Accuracy: 0.8223171510132663\n",
            "Average loss: 0.019114009311687833\n",
            "Accuracy: 0.8225090135546291\n",
            "Average loss: 0.019257038143751275\n",
            "Accuracy: 0.8220068146542805\n",
            "Average loss: 0.01933299886184658\n",
            "Accuracy: 0.8218787529514832\n",
            "Average loss: 0.019341274931565105\n",
            "Accuracy: 0.8224133509368712\n",
            "Average loss: 0.01937400893929873\n",
            "Accuracy: 0.8226221218132056\n",
            "Average loss: 0.019383799679115594\n",
            "Accuracy: 0.8231197510039407\n",
            "Average loss: 0.019420369813620133\n",
            "Accuracy: 0.823286771774292\n",
            "Average loss: 0.019424921089650923\n",
            "Accuracy: 0.8238871843328973\n",
            "Average loss: 0.019461898532404674\n",
            "Accuracy: 0.824045294860624\n",
            "Average loss: 0.019489402400651626\n",
            "Accuracy: 0.8242940113578044\n",
            "Average loss: 0.019494090628518833\n",
            "Accuracy: 0.8248778091412838\n",
            "Average loss: 0.01952851980234455\n",
            "Accuracy: 0.8250523667002834\n",
            "Average loss: 0.019541078352214265\n",
            "Accuracy: 0.8254775876800219\n",
            "Average loss: 0.019577670291024644\n",
            "Accuracy: 0.8256281606063315\n",
            "Average loss: 0.019610113442175177\n",
            "Accuracy: 0.8258153481767811\n",
            "Average loss: 0.019613852633598827\n",
            "Accuracy: 0.8264036616778265\n",
            "Average loss: 0.019643130887144675\n",
            "Accuracy: 0.8266159948977557\n",
            "Average loss: 0.020074995432257377\n",
            "Accuracy: 0.8251955188237704\n",
            "Average loss: 0.020955362722940286\n",
            "Accuracy: 0.8228488478574667\n",
            "Average loss: 0.021283546660303795\n",
            "Accuracy: 0.8217382928181122\n",
            "Average loss: 0.021313476746519615\n",
            "Accuracy: 0.8219613911850112\n",
            "Average loss: 0.021313490890752736\n",
            "Accuracy: 0.8227402790387471\n",
            "Average loss: 0.02134990692998699\n",
            "Accuracy: 0.8228984778433774\n",
            "Average loss: 0.021385359961235424\n",
            "Accuracy: 0.8230635807377651\n",
            "Average loss: 0.02165827338376181\n",
            "Accuracy: 0.8221405585084045\n",
            "Average loss: 0.021695347343335548\n",
            "Accuracy: 0.8222937466796308\n",
            "Average loss: 0.021963388195184428\n",
            "Accuracy: 0.821397201133811\n",
            "Average loss: 0.022013031409158695\n",
            "Accuracy: 0.8214551383282716\n",
            "Average loss: 0.022018789045208594\n",
            "Accuracy: 0.8219821984911787\n",
            "Average loss: 0.0220559018909071\n",
            "Accuracy: 0.8221331151769908\n",
            "Average loss: 0.022089402479337966\n",
            "Accuracy: 0.8223132124313941\n",
            "Average loss: 0.022123234382367473\n",
            "Accuracy: 0.8224889278411865\n",
            "Average loss: 0.022155007483269866\n",
            "Accuracy: 0.8226810153258048\n",
            "Average loss: 0.022191238384662363\n",
            "Accuracy: 0.8228336414204368\n",
            "Average loss: 0.02241514410502421\n",
            "Accuracy: 0.8221037355290741\n",
            "Average loss: 0.023189167696042515\n",
            "Accuracy: 0.8201184018386458\n",
            "Average loss: 0.02320358181755293\n",
            "Accuracy: 0.8204969599843025\n",
            "Average loss: 0.023230616169745723\n",
            "Accuracy: 0.8207358774802497\n",
            "Average loss: 0.023267665726396488\n",
            "Accuracy: 0.8208868353327444\n",
            "Average loss: 0.02327148282664472\n",
            "Accuracy: 0.8214353919029236\n",
            "Average loss: 0.02327801204527554\n",
            "Accuracy: 0.8219216461064386\n",
            "Average loss: 0.023302472412679645\n",
            "Accuracy: 0.8221751312820279\n",
            "Average loss: 0.023335128093913228\n",
            "Accuracy: 0.8223532743570281\n",
            "Average loss: 0.023371850320208755\n",
            "Accuracy: 0.8224971863904945\n",
            "Average loss: 0.023959230979498234\n",
            "Accuracy: 0.8209213225110885\n",
            "Average loss: 0.02399606390370359\n",
            "Accuracy: 0.821068970075094\n",
            "Average loss: 0.024027897208931635\n",
            "Accuracy: 0.821255479812622\n",
            "Average loss: 0.02406312367211285\n",
            "Accuracy: 0.8214131218503671\n",
            "Average loss: 0.02460954765394224\n",
            "Accuracy: 0.8199466197263627\n",
            "Average loss: 0.02464646732743814\n",
            "Accuracy: 0.8200951235096445\n",
            "Average loss: 0.024669217841111698\n",
            "Accuracy: 0.8203630658585256\n",
            "Average loss: 0.024693273069566736\n",
            "Accuracy: 0.8206165047252879\n",
            "Average loss: 0.02472235158167855\n",
            "Accuracy: 0.8208232782781124\n",
            "Average loss: 0.02472706125126046\n",
            "Accuracy: 0.8213224513057605\n",
            "Average loss: 0.024727130221713085\n",
            "Accuracy: 0.8219911303631094\n",
            "Average loss: 0.024761482502366917\n",
            "Accuracy: 0.8221477723489857\n",
            "Average loss: 0.025053185380551353\n",
            "Accuracy: 0.8212914393498347\n",
            "Average loss: 0.025089649473758937\n",
            "Accuracy: 0.8214336172374272\n",
            "Average loss: 0.02564513503343178\n",
            "Accuracy: 0.8200057340032272\n",
            "Average loss: 0.02564544685971513\n",
            "Accuracy: 0.8206403341583426\n",
            "Average loss: 0.02630516524236444\n",
            "Accuracy: 0.8190383014805389\n",
            "Average loss: 0.026336781122387125\n",
            "Accuracy: 0.8192236248052345\n",
            "Average loss: 0.026368870884322482\n",
            "Accuracy: 0.8194038536315574\n",
            "Average loss: 0.026401078772033078\n",
            "Accuracy: 0.8195818175983786\n",
            "Average loss: 0.026574327486096304\n",
            "Accuracy: 0.819103341716439\n",
            "Average loss: 0.02660673185561192\n",
            "Accuracy: 0.8192795929412416\n",
            "Average loss: 0.026640591293988377\n",
            "Accuracy: 0.8194435607503961\n",
            "Average loss: 0.027377393746936412\n",
            "Accuracy: 0.8177610688543847\n",
            "Average loss: 0.028055939128901117\n",
            "Accuracy: 0.816185366581468\n",
            "Average loss: 0.028091040931148444\n",
            "Accuracy: 0.8163497797298781\n",
            "Average loss: 0.028265862662669308\n",
            "Accuracy: 0.8158884748925258\n",
            "Average loss: 0.028293434410702836\n",
            "Accuracy: 0.8161102266745134\n",
            "Average loss: 0.02832418194600062\n",
            "Accuracy: 0.8163053788568663\n",
            "Average loss: 0.02836052679130284\n",
            "Accuracy: 0.8164581811385034\n",
            "Average loss: 0.028376870294039513\n",
            "Accuracy: 0.8167774001900241\n",
            "Average loss: 0.02845651372054795\n",
            "Accuracy: 0.8166840422538019\n",
            "Average loss: 0.028492733668678362\n",
            "Accuracy: 0.8168347233108112\n",
            "Average loss: 0.028511809078982635\n",
            "Accuracy: 0.8171220873598526\n",
            "Average loss: 0.028753927252965675\n",
            "Accuracy: 0.8164767060719483\n",
            "Average loss: 0.02890187305934976\n",
            "Accuracy: 0.816117348603562\n",
            "Average loss: 0.02893755951637969\n",
            "Accuracy: 0.8162715758236361\n",
            "Average loss: 0.0289676924491953\n",
            "Accuracy: 0.8164645820333246\n",
            "Average loss: 0.02922369374430828\n",
            "Accuracy: 0.8157944589644879\n",
            "Average loss: 0.029260290130751834\n",
            "Accuracy: 0.815942015797419\n",
            "Average loss: 0.029288743893508447\n",
            "Accuracy: 0.8161467876699235\n",
            "Average loss: 0.02932519902784717\n",
            "Accuracy: 0.8162930523235493\n",
            "Average loss: 0.029347375974019797\n",
            "Accuracy: 0.8165457361731036\n",
            "Average loss: 0.029983096000222115\n",
            "Accuracy: 0.8151444106577188\n",
            "Average loss: 0.030011648210589423\n",
            "Accuracy: 0.8153483683932318\n",
            "Average loss: 0.03040616866487986\n",
            "Accuracy: 0.8143889415793044\n",
            "Average loss: 0.03041498999897912\n",
            "Accuracy: 0.8147833795369077\n",
            "Average loss: 0.03044893622299042\n",
            "Accuracy: 0.8149481007608317\n",
            "Average loss: 0.030476330195173564\n",
            "Accuracy: 0.81515864041206\n",
            "Average loss: 0.030477421396375486\n",
            "Accuracy: 0.8156985261223533\n",
            "Average loss: 0.03083798635658773\n",
            "Accuracy: 0.814822795407084\n",
            "Average loss: 0.030870465129690983\n",
            "Accuracy: 0.8149951666493879\n",
            "Average loss: 0.030884160152054835\n",
            "Accuracy: 0.815322587688764\n",
            "Average loss: 0.030911441568226044\n",
            "Accuracy: 0.8155292242864438\n",
            "Average loss: 0.03124057100243945\n",
            "Accuracy: 0.8147313908630649\n",
            "Average loss: 0.031276924565829575\n",
            "Accuracy: 0.8148762202105506\n",
            "Average loss: 0.031303050449195124\n",
            "Accuracy: 0.8150909109727332\n",
            "Average loss: 0.03133459978962713\n",
            "Accuracy: 0.8152653270080441\n",
            "Average loss: 0.03137112730590205\n",
            "Accuracy: 0.8154058873263839\n",
            "Average loss: 0.03142562049803107\n",
            "Accuracy: 0.8154433210431947\n",
            "Average loss: 0.03146071620579433\n",
            "Accuracy: 0.815591499209404\n",
            "Average loss: 0.03147758587512016\n",
            "Accuracy: 0.8158765984970389\n",
            "Average loss: 0.0321558554765699\n",
            "Accuracy: 0.8145005260744402\n",
            "Average loss: 0.03219126646201136\n",
            "Accuracy: 0.8146483045298953\n",
            "Average loss: 0.03241463174269772\n",
            "Accuracy: 0.8141191106958267\n",
            "Average loss: 0.03245000565468616\n",
            "Accuracy: 0.8142673969268799\n",
            "Average loss: 0.03365755534297888\n",
            "Accuracy: 0.8122638040667127\n",
            "Average loss: 0.03369353509620274\n",
            "Accuracy: 0.8124132631317017\n",
            "Average loss: 0.03373003417988091\n",
            "Accuracy: 0.8125585755215415\n",
            "Average loss: 0.033766711559587194\n",
            "Accuracy: 0.812701880743827\n",
            "Average loss: 0.034914217418636546\n",
            "Accuracy: 0.8107929329077402\n",
            "Average loss: 0.03493587443467399\n",
            "Accuracy: 0.8110439691423996\n",
            "Average loss: 0.03497284176008445\n",
            "Accuracy: 0.8111889135092497\n",
            "Average loss: 0.03542284828722919\n",
            "Accuracy: 0.8102274542657014\n",
            "Average loss: 0.03545345577432694\n",
            "Accuracy: 0.8104139177695565\n",
            "Average loss: 0.03547224238036506\n",
            "Accuracy: 0.8106862037174473\n",
            "Average loss: 0.035875761137080325\n",
            "Accuracy: 0.8098166618082259\n",
            "Average loss: 0.036275167900191994\n",
            "Accuracy: 0.8089598747400137\n",
            "Average loss: 0.036275204893059354\n",
            "Accuracy: 0.8095320525344896\n",
            "Average loss: 0.03631156081428566\n",
            "Accuracy: 0.8096821382869639\n",
            "Average loss: 0.03673171926157454\n",
            "Accuracy: 0.8087969507749487\n",
            "Average loss: 0.036732661652832774\n",
            "Accuracy: 0.8093089238488566\n",
            "Average loss: 0.0367694161415052\n",
            "Accuracy: 0.8094559792316322\n",
            "Average loss: 0.03680112522193091\n",
            "Accuracy: 0.8096327128006973\n",
            "Average loss: 0.036837641503275446\n",
            "Accuracy: 0.8097792967974421\n",
            "Average loss: 0.03703593189130715\n",
            "Accuracy: 0.8093589279386733\n",
            "Average loss: 0.037354966006877495\n",
            "Accuracy: 0.8086756938945747\n",
            "Average loss: 0.0373557769783696\n",
            "Accuracy: 0.8091837751331614\n",
            "Average loss: 0.037391668051512214\n",
            "Accuracy: 0.8093335759781656\n",
            "Average loss: 0.037428237481846025\n",
            "Accuracy: 0.8094785669790885\n",
            "Average loss: 0.0374282598794929\n",
            "Accuracy: 0.8100318570108809\n",
            "Average loss: 0.03811308148517527\n",
            "Accuracy: 0.8087820626993095\n",
            "Average loss: 0.03820814457877066\n",
            "Accuracy: 0.8086720215923646\n",
            "Average loss: 0.038239182627652246\n",
            "Accuracy: 0.8088499905426831\n",
            "Average loss: 0.03826806789487286\n",
            "Accuracy: 0.8090404040283627\n",
            "Average loss: 0.038288656627129256\n",
            "Accuracy: 0.8092869306444774\n",
            "Average loss: 0.038370698531822955\n",
            "Accuracy: 0.809223895502645\n",
            "Average loss: 0.03840123656497182\n",
            "Accuracy: 0.809401263879693\n",
            "Average loss: 0.03881411561329603\n",
            "Accuracy: 0.8085750250113493\n",
            "Average loss: 0.039003325215524055\n",
            "Accuracy: 0.8081971315897851\n",
            "Average loss: 0.039363053566396\n",
            "Accuracy: 0.8074702653049053\n",
            "Average loss: 0.039377182512391036\n",
            "Accuracy: 0.8077693689176892\n",
            "Average loss: 0.039413726492631855\n",
            "Accuracy: 0.8079135847091675\n",
            "Average loss: 0.03970061776769465\n",
            "Accuracy: 0.807329266838878\n",
            "Average loss: 0.039774246858393404\n",
            "Accuracy: 0.8073049995370887\n",
            "Average loss: 0.03981105628828317\n",
            "Accuracy: 0.8074478493533797\n",
            "Average loss: 0.04023044132896181\n",
            "Accuracy: 0.8066352373462612\n",
            "Average loss: 0.040384450372405296\n",
            "Accuracy: 0.8063601876648379\n",
            "Average loss: 0.040420829806119336\n",
            "Accuracy: 0.8065068286456419\n",
            "Average loss: 0.04045036531821851\n",
            "Accuracy: 0.8066918536060664\n",
            "Average loss: 0.040479216371827866\n",
            "Accuracy: 0.8068799939235496\n",
            "Average loss: 0.04076718143910401\n",
            "Accuracy: 0.8063095075838413\n",
            "Average loss: 0.04107354753237903\n",
            "Accuracy: 0.8057074215677049\n",
            "Average loss: 0.041110581013493554\n",
            "Accuracy: 0.8058503334542061\n",
            "Average loss: 0.041135153489420456\n",
            "Accuracy: 0.8060655537889807\n",
            "Average loss: 0.04115999974238304\n",
            "Accuracy: 0.8062778093926506\n",
            "Average loss: 0.04119296641039643\n",
            "Accuracy: 0.8064401279111485\n",
            "Average loss: 0.04122712707483781\n",
            "Accuracy: 0.8065949364884258\n",
            "Average loss: 0.04125725120551933\n",
            "Accuracy: 0.806771718413452\n",
            "Average loss: 0.04133858803996457\n",
            "Accuracy: 0.8067219793309308\n",
            "Average loss: 0.04133859768889146\n",
            "Accuracy: 0.8072409320460714\n",
            "Average loss: 0.04137573765879208\n",
            "Accuracy: 0.8073760340530375\n",
            "Average loss: 0.04137599632744374\n",
            "Accuracy: 0.8078644063021686\n",
            "Average loss: 0.041424574140193904\n",
            "Accuracy: 0.8079417616209251\n",
            "Average loss: 0.04143972271858729\n",
            "Accuracy: 0.80821270407528\n",
            "Average loss: 0.04147328899024637\n",
            "Accuracy: 0.8083626509671556\n",
            "Average loss: 0.04150817545935685\n",
            "Accuracy: 0.8085047212195269\n",
            "Average loss: 0.041544857030014724\n",
            "Accuracy: 0.8086366505622864\n",
            "Average loss: 0.04158192274170488\n",
            "Accuracy: 0.8087659056516404\n",
            "Average loss: 0.041609434869193\n",
            "Accuracy: 0.8089469070459867\n",
            "Average loss: 0.041640122811203606\n",
            "Accuracy: 0.8091086833565323\n",
            "Average loss: 0.04166925344330682\n",
            "Accuracy: 0.8092784158158114\n",
            "Average loss: 0.04167451566288103\n",
            "Accuracy: 0.8096387576115759\n",
            "Average loss: 0.04168601565220453\n",
            "Accuracy: 0.8099296779770238\n",
            "Average loss: 0.041700226104825475\n",
            "Accuracy: 0.8101958399979856\n",
            "Average loss: 0.04184553664085243\n",
            "Accuracy: 0.8099533710093785\n",
            "Average loss: 0.04188211005770116\n",
            "Accuracy: 0.8100789814877013\n",
            "Average loss: 0.04210008929550185\n",
            "Accuracy: 0.8096730385507857\n",
            "Average loss: 0.04213564937660144\n",
            "Accuracy: 0.8098038498292933\n",
            "Average loss: 0.04215478060265768\n",
            "Accuracy: 0.8100302849321094\n",
            "Average loss: 0.042179888968408574\n",
            "Accuracy: 0.8102170609936272\n",
            "Average loss: 0.04221670413615401\n",
            "Accuracy: 0.8103391762564299\n",
            "Average loss: 0.04224804465362113\n",
            "Accuracy: 0.8104888819731199\n",
            "Average loss: 0.042266974920709424\n",
            "Accuracy: 0.8107126300292247\n",
            "Average loss: 0.042279379031173805\n",
            "Accuracy: 0.8109848239288038\n",
            "Average loss: 0.0423143942090641\n",
            "Accuracy: 0.8111127040161734\n",
            "Average loss: 0.04235039338377092\n",
            "Accuracy: 0.8112350207597471\n",
            "Average loss: 0.04235746026798529\n",
            "Accuracy: 0.8115550919424129\n",
            "Average loss: 0.04239102297563189\n",
            "Accuracy: 0.8116879074862509\n",
            "Average loss: 0.04240890956629877\n",
            "Accuracy: 0.8119124384310744\n",
            "Average loss: 0.04300448433778445\n",
            "Accuracy: 0.8109471624221035\n",
            "Average loss: 0.043038345307164355\n",
            "Accuracy: 0.8110789935989189\n",
            "Average loss: 0.043067631079436355\n",
            "Accuracy: 0.8112340478599072\n",
            "Average loss: 0.04310057541615162\n",
            "Accuracy: 0.8113691433706783\n",
            "Average loss: 0.04313405108829634\n",
            "Accuracy: 0.8115008777350335\n",
            "Average loss: 0.04335255768182611\n",
            "Accuracy: 0.8111084993956402\n",
            "Average loss: 0.043360896343236206\n",
            "Accuracy: 0.8114084434686321\n",
            "Average loss: 0.04338508079610575\n",
            "Accuracy: 0.8115893645051085\n",
            "Average loss: 0.04339660999715683\n",
            "Accuracy: 0.8118573179092313\n",
            "Average loss: 0.04343309191724141\n",
            "Accuracy: 0.8119715881289077\n",
            "Average loss: 0.04346962453988986\n",
            "Accuracy: 0.8120850570061627\n",
            "Average loss: 0.04349978185972781\n",
            "Accuracy: 0.8122296565027867\n",
            "Average loss: 0.04359053127613728\n",
            "Accuracy: 0.8121427929982906\n",
            "Average loss rate: 0.8121427929982906\n",
            "Average accuracy rate: 0.8121427929982906\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_tensor(tensor_arr):\n",
        "  true_score_items = [float(\"{:.2f}\".format(x.item())) for x in tensor_arr]\n",
        "  start = 0\n",
        "  end = len(true_score_items)\n",
        "  step = 20\n",
        "  for i in range(start, end, step):\n",
        "      x = i\n",
        "      print(true_score_items[x:x+step])\n",
        "\n",
        "true_score_tensor = torch.tensor(model_classical.true_score)\n",
        "pred_score_tensor = torch.tensor(model_classical.pred_score)\n",
        "\n",
        "print(\"True values\\n-----------\")\n",
        "print_tensor(true_score_tensor)\n",
        "print(\"Pred values\\n-----------\")\n",
        "print_tensor(pred_score_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_udyVjfGir9",
        "outputId": "3b45f557-4aff-4fda-cebd-92d32fceff21"
      },
      "id": "w_udyVjfGir9",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True values\n",
            "-----------\n",
            "[0.36, 0.32, 0.52, 0.01, 0.02, 0.22, 0.23, 0.03, 0.23, 0.18, 0.01, 0.09, 0.19, 0.02, 0.13, 0.02, 0.02, 0.08, 0.01, 0.04]\n",
            "[0.0, 0.35, 0.22, 0.47, 0.09, 0.86, 0.02, 0.01, 0.07, 0.0, 0.34, 0.59, 0.0, 0.38, 0.32, 0.15, 0.49, 0.01, 0.03, 0.52]\n",
            "[0.1, 0.02, 0.01, 0.23, 0.01, 0.03, 0.08, 0.37, 0.0, 0.02, 0.0, 0.01, 0.39, 0.38, 0.13, 0.48, 0.03, 0.53, 0.0, 0.31]\n",
            "[0.0, 0.14, 0.62, 0.01, 0.03, 0.0, 0.0, 0.72, 0.0, 0.02, 0.42, 0.01, 0.0, 0.01, 0.44, 0.02, 0.23, 0.0, 0.0, 0.24]\n",
            "[0.07, 0.41, 0.22, 0.01, 0.54, 0.01, 0.02, 0.02, 0.31, 0.86, 0.01, 0.27, 0.42, 0.0, 0.02, 0.23, 0.0, 0.19, 0.62, 0.01]\n",
            "[0.01, 0.0, 0.05, 0.01, 0.01, 0.17, 0.33, 0.01, 0.49, 0.02, 0.04, 0.28, 0.35, 0.48, 0.0, 0.01, 0.0, 0.69, 0.54, 0.32]\n",
            "[0.09, 0.02, 0.01, 0.02, 0.41, 0.27, 0.31, 0.03, 0.0, 0.0, 0.0, 0.01, 0.32, 0.07, 0.53, 0.49, 0.01, 0.0, 0.05, 0.0]\n",
            "[0.01, 0.18, 0.0, 0.38, 0.0, 0.0, 0.37, 0.01, 0.04, 0.51, 0.15, 0.41, 0.0, 0.11, 0.07, 0.0, 0.07, 0.01, 0.24, 0.41]\n",
            "[0.49, 0.01, 0.0, 0.01, 0.02, 0.2, 0.26, 0.02, 0.18, 0.0, 0.09, 0.17, 0.01, 0.25, 0.1, 0.27, 0.07, 0.2, 0.01, 0.0]\n",
            "[0.49, 0.03, 0.0, 0.01, 0.0, 0.0, 0.02, 0.0, 0.12, 0.04, 0.0, 0.02, 0.0, 0.0, 0.02, 0.04, 0.02, 0.03, 0.0, 0.0]\n",
            "[0.0, 0.11, 0.0, 0.0, 0.37, 0.3, 0.07, 0.01, 0.06, 0.0, 0.17, 0.0, 0.02, 0.08, 0.0, 0.05, 0.0, 0.01, 0.16, 0.23]\n",
            "[0.54, 0.72, 0.49, 0.01, 0.13, 0.0, 0.0, 0.46, 0.0, 0.45, 0.27, 0.07, 0.0, 0.01, 0.01, 0.01, 0.0, 0.43, 0.69, 0.2]\n",
            "[0.02, 0.0, 0.08, 0.18, 0.02, 0.01, 0.0, 0.61, 0.0, 0.01, 0.0, 0.6, 0.0, 0.03, 0.02, 0.01, 0.17, 0.13, 0.0, 0.47]\n",
            "[0.0, 0.6, 0.13, 0.64, 0.01, 0.01, 0.01, 0.39, 0.24, 0.01, 0.67, 0.65, 0.0, 0.39, 0.02, 0.01, 0.0, 0.04, 0.3, 0.0]\n",
            "[0.21, 0.44, 0.37, 0.0, 0.01, 0.45, 0.0, 0.02, 0.0, 0.03, 0.63, 0.23, 0.53, 0.18, 0.01, 0.02, 0.1, 0.51, 0.01, 0.2]\n",
            "[0.02, 0.49, 0.0, 0.23, 0.01, 0.0, 0.27, 0.0, 0.04, 0.65, 0.0, 0.43, 0.0, 0.83, 0.0, 0.0, 0.0, 0.81, 0.03, 0.0]\n",
            "[0.55, 0.01, 0.04, 0.53, 0.53, 0.12, 0.0, 0.54, 0.1, 0.0, 0.01, 0.0, 0.41, 0.49, 0.11, 0.0, 0.0, 0.13, 0.65, 0.32]\n",
            "[0.01, 0.01, 0.03, 0.31, 0.01, 0.53, 0.4, 0.51, 0.2, 0.0, 0.47, 0.3, 0.0, 0.54, 0.37, 0.0, 0.01, 0.01, 0.47, 0.48]\n",
            "[0.0, 0.02, 0.02, 0.01, 0.01, 0.01, 0.31, 0.13, 0.0, 0.11, 0.26, 0.2, 0.01, 0.0, 0.0, 0.0, 0.02, 0.01, 0.01, 0.08]\n",
            "[0.19, 0.05, 0.37, 0.0, 0.42, 0.0, 0.03, 0.02, 0.0, 0.01, 0.04, 0.05, 0.0, 0.0, 0.18, 0.01, 0.04, 0.62, 0.01, 0.01]\n",
            "[0.01, 0.01, 0.42, 0.06, 0.02, 0.19, 0.0, 0.0, 0.01, 0.32, 0.36, 0.32, 0.52, 0.01, 0.02, 0.22, 0.23, 0.03, 0.23, 0.18]\n",
            "[0.01, 0.09, 0.19, 0.02, 0.13, 0.02, 0.02, 0.08, 0.01, 0.04, 0.0, 0.35, 0.22, 0.47, 0.09, 0.86, 0.02, 0.01, 0.07, 0.0]\n",
            "[0.34, 0.59, 0.0, 0.38, 0.32, 0.15, 0.49, 0.01, 0.03, 0.52, 0.1, 0.02, 0.01, 0.23, 0.01, 0.03, 0.08, 0.37, 0.0, 0.02]\n",
            "[0.0, 0.01, 0.39, 0.38, 0.13, 0.48, 0.03, 0.53, 0.0, 0.31, 0.0, 0.14, 0.62, 0.01, 0.03, 0.0, 0.0, 0.72, 0.0, 0.02]\n",
            "[0.42, 0.01, 0.0, 0.01, 0.44, 0.02, 0.23, 0.0, 0.0, 0.24, 0.07, 0.41, 0.22, 0.01, 0.54, 0.01, 0.02, 0.02, 0.31, 0.86]\n",
            "[0.01, 0.27, 0.42, 0.0, 0.02, 0.23, 0.0, 0.19, 0.62, 0.01, 0.01, 0.0, 0.05, 0.01, 0.01, 0.17, 0.33, 0.01, 0.49, 0.02]\n",
            "[0.04, 0.28, 0.35, 0.48, 0.0, 0.01, 0.0, 0.69, 0.54, 0.32, 0.09, 0.02, 0.01, 0.02, 0.41, 0.27, 0.31, 0.03, 0.0, 0.0]\n",
            "[0.0, 0.01, 0.32, 0.07, 0.53, 0.49, 0.01, 0.0, 0.05, 0.0, 0.01, 0.18, 0.0, 0.38, 0.0, 0.0, 0.37, 0.01, 0.04, 0.51]\n",
            "[0.15, 0.41, 0.0, 0.11, 0.07, 0.0, 0.07, 0.01, 0.24, 0.41, 0.49, 0.01, 0.0, 0.01, 0.02, 0.2, 0.26, 0.02, 0.18, 0.0]\n",
            "[0.09, 0.17, 0.01, 0.25, 0.1, 0.27, 0.07, 0.2, 0.01, 0.0, 0.49, 0.03, 0.0, 0.01, 0.0, 0.0, 0.02, 0.0, 0.12, 0.04]\n",
            "[0.0, 0.02, 0.0, 0.0, 0.02, 0.04, 0.02, 0.03, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.37, 0.3, 0.07, 0.01, 0.06, 0.0]\n",
            "[0.17, 0.0, 0.02, 0.08, 0.0, 0.05, 0.0, 0.01, 0.16, 0.23, 0.54, 0.72, 0.49, 0.01, 0.13, 0.0, 0.0, 0.46, 0.0, 0.45]\n",
            "[0.27, 0.07, 0.0, 0.01, 0.01, 0.01, 0.0, 0.43, 0.69, 0.2, 0.02, 0.0, 0.08, 0.18, 0.02, 0.01, 0.0, 0.61, 0.0, 0.01]\n",
            "[0.0, 0.6, 0.0, 0.03, 0.02, 0.01, 0.17, 0.13, 0.0, 0.47, 0.0, 0.6, 0.13, 0.64, 0.01, 0.01, 0.01, 0.39, 0.24, 0.01]\n",
            "[0.67, 0.65, 0.0, 0.39, 0.02, 0.01, 0.0, 0.04, 0.3, 0.0, 0.21, 0.44, 0.37, 0.0, 0.01, 0.45, 0.0, 0.02, 0.0, 0.03]\n",
            "[0.63, 0.23, 0.53, 0.18, 0.01, 0.02, 0.1, 0.51, 0.01, 0.2, 0.02, 0.49, 0.0, 0.23, 0.01, 0.0, 0.27, 0.0, 0.04, 0.65]\n",
            "[0.0, 0.43, 0.0, 0.83, 0.0, 0.0, 0.0, 0.81, 0.03, 0.0, 0.55, 0.01, 0.04, 0.53, 0.53, 0.12, 0.0, 0.54, 0.1, 0.0]\n",
            "[0.01, 0.0, 0.41, 0.49, 0.11, 0.0, 0.0, 0.13, 0.65, 0.32, 0.01, 0.01, 0.03, 0.31, 0.01, 0.53, 0.4, 0.51, 0.2, 0.0]\n",
            "[0.47, 0.3, 0.0, 0.54, 0.37, 0.0, 0.01, 0.01, 0.47, 0.48, 0.0, 0.02, 0.02, 0.01, 0.01, 0.01, 0.31, 0.13, 0.0, 0.11]\n",
            "[0.26, 0.2, 0.01, 0.0, 0.0, 0.0, 0.02, 0.01, 0.01, 0.08, 0.19, 0.05, 0.37, 0.0, 0.42, 0.0, 0.03, 0.02, 0.0, 0.01]\n",
            "[0.04, 0.05, 0.0, 0.0, 0.18, 0.01, 0.04, 0.62, 0.01, 0.01, 0.01, 0.01, 0.42, 0.06, 0.02, 0.19, 0.0, 0.0, 0.01, 0.32]\n",
            "Pred values\n",
            "-----------\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n",
            "[0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_gpt.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJwabLcelsHl",
        "outputId": "a50c986f-5ef0-41e2-8c18-5bd90d197123"
      },
      "id": "zJwabLcelsHl",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Evaluating with validation set\n",
            "Accuracy: 0.7633202075958252\n",
            "Accuracy: 0.550000011920929\n",
            "Accuracy: 0.5455599427223206\n",
            "Accuracy: 0.6499999910593033\n",
            "Accuracy: 0.6726640343666077\n",
            "Accuracy: 0.6333333353201548\n",
            "Accuracy: 0.6052399788584027\n",
            "Accuracy: 0.6250000074505806\n",
            "Accuracy: 0.6596310999658372\n",
            "Accuracy: 0.667335969209671\n",
            "Accuracy: 0.6851527149027045\n",
            "Accuracy: 0.672779972354571\n",
            "Accuracy: 0.6874369107759916\n",
            "Accuracy: 0.7071428554398673\n",
            "Accuracy: 0.7108880122502644\n",
            "Accuracy: 0.714165024459362\n",
            "Accuracy: 0.7288212074952967\n",
            "Accuracy: 0.7307378186119927\n",
            "Accuracy: 0.7415768723738821\n",
            "Accuracy: 0.7426640391349792\n",
            "Accuracy: 0.7436476662045434\n",
            "Accuracy: 0.7445418726314198\n",
            "Accuracy: 0.7355043914007104\n",
            "Accuracy: 0.7397200341025988\n",
            "Accuracy: 0.7486640381813049\n",
            "Accuracy: 0.7492277370049403\n",
            "Accuracy: 0.7302444797975046\n",
            "Accuracy: 0.7126171695334571\n",
            "Accuracy: 0.7099986405208193\n",
            "Accuracy: 0.707554680109024\n",
            "Accuracy: 0.7093535680924693\n",
            "Accuracy: 0.7110400255769491\n",
            "Accuracy: 0.712624273516915\n",
            "Accuracy: 0.7141153304015889\n",
            "Accuracy: 0.71552118403571\n",
            "Accuracy: 0.7105533679326376\n",
            "Accuracy: 0.7119794987343453\n",
            "Accuracy: 0.7073663488814705\n",
            "Accuracy: 0.7139292665016956\n",
            "Accuracy: 0.7151640400290489\n",
            "Accuracy: 0.7083717424695085\n",
            "Accuracy: 0.7096800392582303\n",
            "Accuracy: 0.7149591043938038\n",
            "Accuracy: 0.7109073021195151\n",
            "Accuracy: 0.7137022468778822\n",
            "Accuracy: 0.7147808981978375\n",
            "Accuracy: 0.7158136494616245\n",
            "Accuracy: 0.7168033694227537\n",
            "Accuracy: 0.711086562701634\n",
            "Accuracy: 0.7155984282493592\n",
            "Accuracy: 0.7081686515434116\n",
            "Accuracy: 0.7092292583905734\n",
            "Accuracy: 0.7040868165358057\n",
            "Accuracy: 0.7051837311850654\n",
            "Accuracy: 0.7039382056756454\n",
            "Accuracy: 0.6991657348615783\n",
            "Accuracy: 0.7002912519270914\n",
            "Accuracy: 0.6974703648994709\n",
            "Accuracy: 0.6930501865128339\n",
            "Accuracy: 0.693777346611023\n",
            "Accuracy: 0.6949173935124131\n",
            "Accuracy: 0.6875264644622803\n",
            "Accuracy: 0.691481279948401\n",
            "Accuracy: 0.6875000074505806\n",
            "Accuracy: 0.6851796966332656\n",
            "Accuracy: 0.6893939457156442\n",
            "Accuracy: 0.6856220933928419\n",
            "Accuracy: 0.6867647127193564\n",
            "Accuracy: 0.683140294275422\n",
            "Accuracy: 0.6857142925262452\n",
            "Accuracy: 0.6868073335835632\n",
            "Accuracy: 0.687870012389289\n",
            "Accuracy: 0.6857989136486837\n",
            "Accuracy: 0.6868464987020235\n",
            "Accuracy: 0.6848442761103312\n",
            "Accuracy: 0.6858768541561929\n",
            "Accuracy: 0.6826405292981631\n",
            "Accuracy: 0.6836748841481332\n",
            "Accuracy: 0.6846830527993697\n",
            "Accuracy: 0.6815830126404763\n",
            "Accuracy: 0.6850612487322019\n",
            "Accuracy: 0.6860156262793192\n",
            "Accuracy: 0.6869470067771084\n",
            "Accuracy: 0.6899200166974749\n",
            "Accuracy: 0.6907835483551026\n",
            "Accuracy: 0.6916269978811574\n",
            "Accuracy: 0.6898459956563753\n",
            "Accuracy: 0.6869691075249151\n",
            "Accuracy: 0.6878269850538018\n",
            "Accuracy: 0.688665798637602\n",
            "Accuracy: 0.6869956228759263\n",
            "Accuracy: 0.6842747991499694\n",
            "Accuracy: 0.6837630252684316\n",
            "Accuracy: 0.6821983101520133\n",
            "Accuracy: 0.6796139052039698\n",
            "Accuracy: 0.680485845853885\n",
            "Accuracy: 0.679003309343279\n",
            "Accuracy: 0.6775510286798283\n",
            "Accuracy: 0.6771381882706073\n",
            "Accuracy: 0.6800000077486038\n",
            "Accuracy: 0.6808249602223387\n",
            "Accuracy: 0.6816337371573729\n",
            "Accuracy: 0.6824268096859015\n",
            "Accuracy: 0.6800638580551515\n",
            "Accuracy: 0.6818091562816075\n",
            "Accuracy: 0.6813834079031674\n",
            "Accuracy: 0.6821491723862764\n",
            "Accuracy: 0.684505937827958\n",
            "Accuracy: 0.6870638662521992\n",
            "Accuracy: 0.6847876478325237\n",
            "Accuracy: 0.6854951483709318\n",
            "Accuracy: 0.6841664399419513\n",
            "Accuracy: 0.6837462044395177\n",
            "Accuracy: 0.6844442220110643\n",
            "Accuracy: 0.6851301001465839\n",
            "Accuracy: 0.6847124253881389\n",
            "Accuracy: 0.6862389873235654\n",
            "Accuracy: 0.6824291636378078\n",
            "Accuracy: 0.6795237069370366\n",
            "Accuracy: 0.6816666747132937\n",
            "Accuracy: 0.6812948825930761\n",
            "Accuracy: 0.6819672213226068\n",
            "Accuracy: 0.680786022810432\n",
            "Accuracy: 0.6814516210748304\n",
            "Accuracy: 0.6786934466361999\n",
            "Accuracy: 0.6809523886158353\n",
            "Accuracy: 0.6816009541196147\n",
            "Accuracy: 0.6822393857873976\n",
            "Accuracy: 0.6787854354510936\n",
            "Accuracy: 0.6776923152116628\n",
            "Accuracy: 0.6743258074039721\n",
            "Accuracy: 0.6725254591667291\n",
            "Accuracy: 0.6732081264481509\n",
            "Accuracy: 0.6714429899827757\n",
            "Accuracy: 0.6721235619650947\n",
            "Accuracy: 0.6742647132452797\n",
            "Accuracy: 0.6749147533500282\n",
            "Accuracy: 0.6755553725836934\n",
            "Accuracy: 0.6767145412431347\n",
            "Accuracy: 0.6750000076634544\n",
            "Accuracy: 0.676335611664657\n",
            "Accuracy: 0.6769481792416371\n",
            "Accuracy: 0.6738693793336828\n",
            "Accuracy: 0.6744905656410588\n",
            "Accuracy: 0.6735401465975005\n",
            "Accuracy: 0.6746575415950932\n",
            "Accuracy: 0.6716781011244066\n",
            "Accuracy: 0.6700902750363221\n",
            "Accuracy: 0.6720582606808451\n",
            "Accuracy: 0.6726666736602783\n",
            "Accuracy: 0.6717660983666679\n",
            "Accuracy: 0.6723684280326492\n",
            "Accuracy: 0.6729628841082255\n",
            "Accuracy: 0.6720779290446988\n",
            "Accuracy: 0.6739569110255088\n",
            "Accuracy: 0.6743589807779361\n",
            "Accuracy: 0.6749256127958845\n",
            "Accuracy: 0.6759493735772145\n",
            "Accuracy: 0.6771278062706474\n",
            "Accuracy: 0.6789165083318949\n",
            "Accuracy: 0.679440754911174\n",
            "Accuracy: 0.6767076625500197\n",
            "Accuracy: 0.6752350992220311\n",
            "Accuracy: 0.6757722035413836\n",
            "Accuracy: 0.6775149182839827\n",
            "Accuracy: 0.6766665139830256\n",
            "Accuracy: 0.6746306654935825\n",
            "Accuracy: 0.6738095294151988\n",
            "Accuracy: 0.6743391783985161\n",
            "Accuracy: 0.6748625962173238\n",
            "Accuracy: 0.6753798921902975\n",
            "Accuracy: 0.675891173093818\n",
            "Accuracy: 0.6763965432354481\n",
            "Accuracy: 0.6755935734715955\n",
            "Accuracy: 0.6742283521379744\n",
            "Accuracy: 0.6747345558621667\n",
            "Accuracy: 0.6752350397702664\n",
            "Accuracy: 0.6762916978155629\n",
            "Accuracy: 0.6767778906076314\n",
            "Accuracy: 0.6772586812575658\n",
            "Accuracy: 0.676482002379486\n",
            "Accuracy: 0.6779116608284332\n",
            "Accuracy: 0.6783783741987468\n",
            "Accuracy: 0.6792387081229169\n",
            "Accuracy: 0.6768464977676804\n",
            "Accuracy: 0.6773114101861113\n",
            "Accuracy: 0.6760246103460138\n",
            "Accuracy: 0.6764889486292576\n",
            "Accuracy: 0.6757492177070133\n",
            "Accuracy: 0.6765962208572187\n",
            "Accuracy: 0.6770502731438083\n",
            "Accuracy: 0.6774995957190791\n",
            "Accuracy: 0.6779442621018602\n",
            "Accuracy: 0.6772160947937327\n",
            "Accuracy: 0.6764953958682525\n",
            "Accuracy: 0.6769383796015565\n",
            "Accuracy: 0.6773768660380756\n",
            "Accuracy: 0.6761612244928726\n",
            "Accuracy: 0.6771017217156875\n",
            "Accuracy: 0.6775328141450881\n",
            "Accuracy: 0.676334540642316\n",
            "Accuracy: 0.6771283291354038\n",
            "Accuracy: 0.6775529196696916\n",
            "Accuracy: 0.6778427572811351\n",
            "Accuracy: 0.6766663528070217\n",
            "Accuracy: 0.6755013697355696\n",
            "Accuracy: 0.6759256153290975\n",
            "Accuracy: 0.6773073197557375\n",
            "Accuracy: 0.6777188646736327\n",
            "Accuracy: 0.6781264901161194\n",
            "Accuracy: 0.6774561266763516\n",
            "Accuracy: 0.6772637856456468\n",
            "Accuracy: 0.6786067732063258\n",
            "Accuracy: 0.6790026303763702\n",
            "Accuracy: 0.6783406637435736\n",
            "Accuracy: 0.6786107523021875\n",
            "Accuracy: 0.6790011184556144\n",
            "Accuracy: 0.6793879032681841\n",
            "Accuracy: 0.6797711557993605\n",
            "Accuracy: 0.6777571041475643\n",
            "Accuracy: 0.6771187453248382\n",
            "Accuracy: 0.6775070401999328\n",
            "Accuracy: 0.675530236398158\n",
            "Accuracy: 0.6749103683978319\n",
            "Accuracy: 0.6756293437216017\n",
            "Accuracy: 0.6754570007324219\n",
            "Accuracy: 0.6748456474442839\n",
            "Accuracy: 0.6756722900951118\n",
            "Accuracy: 0.676812061055779\n",
            "Accuracy: 0.6748984424964242\n",
            "Accuracy: 0.6760316952998504\n",
            "Accuracy: 0.6764079388873331\n",
            "Accuracy: 0.6758082473226883\n",
            "Accuracy: 0.6739316299939767\n",
            "Accuracy: 0.6747375386826535\n",
            "Accuracy: 0.672881361791643\n",
            "Accuracy: 0.6718847308983783\n",
            "Accuracy: 0.6708964750546367\n",
            "Accuracy: 0.6712831852326333\n",
            "Accuracy: 0.6707223378121853\n",
            "Accuracy: 0.6711065613382585\n",
            "Accuracy: 0.6701378558785462\n",
            "Accuracy: 0.6705213223465185\n",
            "Accuracy: 0.6709016456467206\n",
            "Accuracy: 0.6712788642669211\n",
            "Accuracy: 0.6716530160690711\n",
            "Accuracy: 0.6727259991622647\n",
            "Accuracy: 0.673897749714313\n",
            "Accuracy: 0.6742568760511866\n",
            "Accuracy: 0.6746131293773651\n",
            "Accuracy: 0.6740635942177944\n",
            "Accuracy: 0.673518420448379\n",
            "Accuracy: 0.6733728133642627\n",
            "Accuracy: 0.6720472503834822\n",
            "Accuracy: 0.6724051835490208\n",
            "Accuracy: 0.6727603203617036\n",
            "Accuracy: 0.6731126934637819\n",
            "Accuracy: 0.6734623349914255\n",
            "Accuracy: 0.672934217112405\n",
            "Accuracy: 0.6732818555373412\n",
            "Accuracy: 0.6736268300663009\n",
            "Accuracy: 0.6727224522419558\n",
            "Accuracy: 0.6734471584000968\n",
            "Accuracy: 0.6733078882098198\n",
            "Accuracy: 0.6736475573395783\n",
            "Accuracy: 0.6739846725661055\n",
            "Accuracy: 0.6734704221232554\n",
            "Accuracy: 0.6740794123998329\n",
            "Accuracy: 0.6744111625678477\n",
            "Accuracy: 0.6753825280401442\n",
            "Accuracy: 0.6757070213226375\n",
            "Accuracy: 0.6751958918483818\n",
            "Accuracy: 0.6761537085085998\n",
            "Accuracy: 0.6764718344176772\n",
            "Accuracy: 0.6752362270788713\n",
            "Accuracy: 0.6736472544894703\n",
            "Accuracy: 0.6745968306107641\n",
            "Accuracy: 0.67491597944884\n",
            "Accuracy: 0.6759496861033969\n",
            "Accuracy: 0.6762617236801556\n",
            "Accuracy: 0.6750532471412442\n",
            "Accuracy: 0.6753662505471114\n",
            "Accuracy: 0.6756770419147747\n",
            "Accuracy: 0.6766898697110969\n",
            "Accuracy: 0.6758477292562786\n",
            "Accuracy: 0.6761535770826407\n",
            "Accuracy: 0.676457293565265\n",
            "Accuracy: 0.6773608440740241\n",
            "Accuracy: 0.6776582813180442\n",
            "Accuracy: 0.6786433221965\n",
            "Accuracy: 0.6781554750560486\n",
            "Accuracy: 0.6790408324705411\n",
            "Accuracy: 0.6795788494800138\n",
            "Accuracy: 0.6798636840314282\n",
            "Accuracy: 0.6800562810089629\n",
            "Accuracy: 0.6792340633031484\n",
            "Accuracy: 0.6777439815829499\n",
            "Accuracy: 0.6780311501266172\n",
            "Accuracy: 0.6772239551097653\n",
            "Accuracy: 0.6775109426180521\n",
            "Accuracy: 0.6780397428626634\n",
            "Accuracy: 0.6783221285074752\n",
            "Accuracy: 0.679174794222262\n",
            "Accuracy: 0.6794515883452014\n",
            "Accuracy: 0.6797265674247116\n",
            "Accuracy: 0.6789322970739378\n",
            "Accuracy: 0.679532909043837\n",
            "Accuracy: 0.6798049457274474\n",
            "Accuracy: 0.6807224706152882\n",
            "Accuracy: 0.6809889149281286\n",
            "Accuracy: 0.6814895287587328\n",
            "Accuracy: 0.6817518065755184\n",
            "Accuracy: 0.6816078066445006\n",
            "Accuracy: 0.6825049796681495\n",
            "Accuracy: 0.6817245823996407\n",
            "Accuracy: 0.6819827964034262\n",
            "Accuracy: 0.6812089700428093\n",
            "Accuracy: 0.6814671814816553\n",
            "Accuracy: 0.6822672211638064\n",
            "Accuracy: 0.6831455115228892\n",
            "Accuracy: 0.6840183296678966\n",
            "Accuracy: 0.6835607566078257\n",
            "Accuracy: 0.6831060168174767\n",
            "Accuracy: 0.6838886520744841\n",
            "Accuracy: 0.6841330568607037\n",
            "Accuracy: 0.6836807462701037\n",
            "Accuracy: 0.6829253918169471\n",
            "Accuracy: 0.6837802538421096\n",
            "Accuracy: 0.6846299191376358\n",
            "Accuracy: 0.6848683745572062\n",
            "Accuracy: 0.6844206749492542\n",
            "Accuracy: 0.6846583241439728\n",
            "Accuracy: 0.6842139442045767\n",
            "Accuracy: 0.6837722252229017\n",
            "Accuracy: 0.6840096818867014\n",
            "Accuracy: 0.6848409630003429\n",
            "Accuracy: 0.6835111078947164\n",
            "Accuracy: 0.6830766957539779\n",
            "Accuracy: 0.6838247876251693\n",
            "Accuracy: 0.6840585976839065\n",
            "Accuracy: 0.6842910364226512\n",
            "Accuracy: 0.6844442199196732\n",
            "Accuracy: 0.6837218747889683\n",
            "Accuracy: 0.6832944269097129\n",
            "Accuracy: 0.6835263856943102\n",
            "Accuracy: 0.6825239389618009\n",
            "Accuracy: 0.6827567812345213\n",
            "Accuracy: 0.6820496635875483\n",
            "Accuracy: 0.6827792629471481\n",
            "Accuracy: 0.6830093799318586\n",
            "Accuracy: 0.6837320877955511\n",
            "Accuracy: 0.6839581904086199\n",
            "Accuracy: 0.6844662980047231\n",
            "Accuracy: 0.6840488220338767\n",
            "Accuracy: 0.6842721217115161\n",
            "Accuracy: 0.6844941668965844\n",
            "Accuracy: 0.6847149681310359\n",
            "Accuracy: 0.6849345358390382\n",
            "Accuracy: 0.6845215700130941\n",
            "Accuracy: 0.6847404551174906\n",
            "Accuracy: 0.6834992898468165\n",
            "Accuracy: 0.6837197896196039\n",
            "Accuracy: 0.6844900385078977\n",
            "Accuracy: 0.683809241259491\n",
            "Accuracy: 0.6840270795234262\n",
            "Accuracy: 0.684790175306341\n",
            "Accuracy: 0.6854765231667812\n",
            "Accuracy: 0.6856880549179472\n",
            "Accuracy: 0.6852842385206765\n",
            "Accuracy: 0.6857654167188181\n",
            "Accuracy: 0.6859744592818289\n",
            "Accuracy: 0.6861823779600923\n",
            "Accuracy: 0.6863891817928959\n",
            "Accuracy: 0.6865948797228502\n",
            "Accuracy: 0.6867994805971781\n",
            "Accuracy: 0.6872689505206778\n",
            "Accuracy: 0.6868695098145887\n",
            "Accuracy: 0.6870717603378195\n",
            "Accuracy: 0.6866749477889733\n",
            "Accuracy: 0.6868766458410965\n",
            "Accuracy: 0.6875322972695659\n",
            "Accuracy: 0.6866138352773576\n",
            "Accuracy: 0.6868141130118707\n",
            "Accuracy: 0.687013347633183\n",
            "Accuracy: 0.6877310276031494\n",
            "Accuracy: 0.6868215685063693\n",
            "Accuracy: 0.6872776372488155\n",
            "Accuracy: 0.6876626944111794\n",
            "Accuracy: 0.6878571867329909\n",
            "Accuracy: 0.6880506816582802\n",
            "Accuracy: 0.6882431868397062\n",
            "Accuracy: 0.6889449137510085\n",
            "Accuracy: 0.689134163862573\n",
            "Accuracy: 0.6898300673453336\n",
            "Accuracy: 0.6900161183333096\n",
            "Accuracy: 0.690201229669831\n",
            "Accuracy: 0.6903854084555389\n",
            "Accuracy: 0.6905686617197104\n",
            "Accuracy: 0.6912522493747243\n",
            "Accuracy: 0.691432419270277\n",
            "Accuracy: 0.6918610670322789\n",
            "Accuracy: 0.6914750439907188\n",
            "Accuracy: 0.6910909366962632\n",
            "Accuracy: 0.6904612062591138\n",
            "Accuracy: 0.6906411050278464\n",
            "Accuracy: 0.690261890001485\n",
            "Accuracy: 0.6909327947829806\n",
            "Accuracy: 0.6913553128639857\n",
            "Average train loss: 0.5062035218163089, avarage valid loss: 0.5312496975598419\n",
            "Average accuracy rate: 0.6913553128639857\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Evaluating with validation set\n",
            "Accuracy: 0.7562227249145508\n",
            "Accuracy: 0.5499999821186066\n",
            "Accuracy: 0.5479257305463155\n",
            "Accuracy: 0.6499999910593033\n",
            "Accuracy: 0.6712445378303528\n",
            "Accuracy: 0.6333333154519399\n",
            "Accuracy: 0.6062538708959307\n",
            "Accuracy: 0.6249999776482582\n",
            "Accuracy: 0.660419676038954\n",
            "Accuracy: 0.6687554359436035\n",
            "Accuracy: 0.6857979189265858\n",
            "Accuracy: 0.6739628612995148\n",
            "Accuracy: 0.6879828526423528\n",
            "Accuracy: 0.707142846924918\n",
            "Accuracy: 0.7104148387908935\n",
            "Accuracy: 0.7132778316736221\n",
            "Accuracy: 0.7275687105515424\n",
            "Accuracy: 0.7291606002383761\n",
            "Accuracy: 0.7404562140765943\n",
            "Accuracy: 0.7412445396184921\n",
            "Accuracy: 0.7419577865373521\n",
            "Accuracy: 0.7426061928272247\n",
            "Accuracy: 0.7339614552000294\n",
            "Accuracy: 0.7385371128718058\n",
            "Accuracy: 0.7472445392608642\n",
            "Accuracy: 0.7475898540936984\n",
            "Accuracy: 0.7289301289452447\n",
            "Accuracy: 0.7116032413073948\n",
            "Accuracy: 0.7092644140638155\n",
            "Accuracy: 0.7070815086364746\n",
            "Accuracy: 0.7086667091615738\n",
            "Accuracy: 0.7101528346538544\n",
            "Accuracy: 0.7115488919344816\n",
            "Accuracy: 0.7128628281986013\n",
            "Accuracy: 0.7141016823904854\n",
            "Accuracy: 0.7093704475296868\n",
            "Accuracy: 0.7106367252968453\n",
            "Accuracy: 0.7062456858785529\n",
            "Accuracy: 0.7126553547688019\n",
            "Accuracy: 0.7137445390224457\n",
            "Accuracy: 0.707159969864822\n",
            "Accuracy: 0.7083281306993394\n",
            "Accuracy: 0.7138036919194598\n",
            "Accuracy: 0.7099394540895115\n",
            "Accuracy: 0.7129136270946926\n",
            "Accuracy: 0.7138551292212113\n",
            "Accuracy: 0.7147565674274525\n",
            "Accuracy: 0.7156204457084338\n",
            "Accuracy: 0.7100726244400959\n",
            "Accuracy: 0.7147467172145844\n",
            "Accuracy: 0.7074728058833702\n",
            "Accuracy: 0.7084103043262775\n",
            "Accuracy: 0.7034172269533265\n",
            "Accuracy: 0.7043951065452011\n",
            "Accuracy: 0.7032929637215354\n",
            "Accuracy: 0.6986587537186486\n",
            "Accuracy: 0.6996686479501557\n",
            "Accuracy: 0.6969808648372519\n",
            "Accuracy: 0.6926892773579743\n",
            "Accuracy: 0.6935407439867656\n",
            "Accuracy: 0.6945683174445981\n",
            "Accuracy: 0.6872974941807408\n",
            "Accuracy: 0.6913686016249279\n",
            "Accuracy: 0.6874999860301614\n",
            "Accuracy: 0.6852888666666471\n",
            "Accuracy: 0.6893939258474292\n",
            "Accuracy: 0.6857280046192568\n",
            "Accuracy: 0.6867646916824228\n",
            "Accuracy: 0.6832431338835454\n",
            "Accuracy: 0.685714271238872\n",
            "Accuracy: 0.6867073480512055\n",
            "Accuracy: 0.6876728393965297\n",
            "Accuracy: 0.6857016666294777\n",
            "Accuracy: 0.6866546539036004\n",
            "Accuracy: 0.6847496215502421\n",
            "Accuracy: 0.6856900571208251\n",
            "Accuracy: 0.6825483317499037\n",
            "Accuracy: 0.6834928752520145\n",
            "Accuracy: 0.6844135062604011\n",
            "Accuracy: 0.6814055524766445\n",
            "Accuracy: 0.6847983576633312\n",
            "Accuracy: 0.6856693865322485\n",
            "Accuracy: 0.686519426753722\n",
            "Accuracy: 0.6895820200443268\n",
            "Accuracy: 0.6903660283369176\n",
            "Accuracy: 0.691131803878518\n",
            "Accuracy: 0.6894380731144171\n",
            "Accuracy: 0.6866464723240245\n",
            "Accuracy: 0.6874282279711091\n",
            "Accuracy: 0.6881926112704807\n",
            "Accuracy: 0.6866056290301648\n",
            "Accuracy: 0.683966189622879\n",
            "Accuracy: 0.6835340505005211\n",
            "Accuracy: 0.6820472757867042\n",
            "Accuracy: 0.6795391697632639\n",
            "Accuracy: 0.6803379567960898\n",
            "Accuracy: 0.6789301142250139\n",
            "Accuracy: 0.6775510031349805\n",
            "Accuracy: 0.6772098541259766\n",
            "Accuracy: 0.6799999833106994\n",
            "Accuracy: 0.6807546639206385\n",
            "Accuracy: 0.6814945468715593\n",
            "Accuracy: 0.6822200631632388\n",
            "Accuracy: 0.6799273433593603\n",
            "Accuracy: 0.6816063472202846\n",
            "Accuracy: 0.6812494689563535\n",
            "Accuracy: 0.6819501535914768\n",
            "Accuracy: 0.6843744786801161\n",
            "Accuracy: 0.6868684997252368\n",
            "Accuracy: 0.6846585788510062\n",
            "Accuracy: 0.6853033008876147\n",
            "Accuracy: 0.6840396752314908\n",
            "Accuracy: 0.6836833705944297\n",
            "Accuracy: 0.6843196807200449\n",
            "Accuracy: 0.6849449245826058\n",
            "Accuracy: 0.6845900308469246\n",
            "Accuracy: 0.6860569771538433\n",
            "Accuracy: 0.6823088440854671\n",
            "Accuracy: 0.6794640406840989\n",
            "Accuracy: 0.6816666508714359\n",
            "Accuracy: 0.6813535153373214\n",
            "Accuracy: 0.6819671973830363\n",
            "Accuracy: 0.6808437016921315\n",
            "Accuracy: 0.6814515970406994\n",
            "Accuracy: 0.6787502021789551\n",
            "Accuracy: 0.6809523654362512\n",
            "Accuracy: 0.6815450454321433\n",
            "Accuracy: 0.6821284648030996\n",
            "Accuracy: 0.6787303935649783\n",
            "Accuracy: 0.6776922922867995\n",
            "Accuracy: 0.6743799639112167\n",
            "Accuracy: 0.6726329733024944\n",
            "Accuracy: 0.6732614676755174\n",
            "Accuracy: 0.6715488985403261\n",
            "Accuracy: 0.672176112069024\n",
            "Accuracy: 0.6742646904552684\n",
            "Accuracy: 0.6748629242834383\n",
            "Accuracy: 0.6754524880561276\n",
            "Accuracy: 0.6766634574896998\n",
            "Accuracy: 0.6749999842473439\n",
            "Accuracy: 0.6762852520807415\n",
            "Accuracy: 0.6768481920302754\n",
            "Accuracy: 0.6738197240796122\n",
            "Accuracy: 0.6743919671409659\n",
            "Accuracy: 0.6734911758324196\n",
            "Accuracy: 0.6746575187330377\n",
            "Accuracy: 0.6717263606129861\n",
            "Accuracy: 0.6701861636058705\n",
            "Accuracy: 0.6721058723910543\n",
            "Accuracy: 0.6726666514078776\n",
            "Accuracy: 0.6718130790634661\n",
            "Accuracy: 0.6723684056809074\n",
            "Accuracy: 0.672916473126879\n",
            "Accuracy: 0.6720779065961962\n",
            "Accuracy: 0.6739110992800805\n",
            "Accuracy: 0.674358959381397\n",
            "Accuracy: 0.6748803846395699\n",
            "Accuracy: 0.675949352074273\n",
            "Accuracy: 0.6770831470219594\n",
            "Accuracy: 0.6788277696818114\n",
            "Accuracy: 0.6793084836894681\n",
            "Accuracy: 0.676620019438826\n",
            "Accuracy: 0.6751915359058263\n",
            "Accuracy: 0.6756856285217332\n",
            "Accuracy: 0.6773858536373485\n",
            "Accuracy: 0.6765809823949653\n",
            "Accuracy: 0.6745881456101012\n",
            "Accuracy: 0.6738095091921943\n",
            "Accuracy: 0.6742971613562319\n",
            "Accuracy: 0.6747790764359867\n",
            "Accuracy: 0.6752553550820601\n",
            "Accuracy: 0.675726095604342\n",
            "Accuracy: 0.6761913940396612\n",
            "Accuracy: 0.675430393081972\n",
            "Accuracy: 0.6741066605704171\n",
            "Accuracy: 0.674573229117827\n",
            "Accuracy: 0.6750345257042492\n",
            "Accuracy: 0.6760524370697107\n",
            "Accuracy: 0.6765003157727545\n",
            "Accuracy: 0.6769432180457645\n",
            "Accuracy: 0.6762074943405488\n",
            "Accuracy: 0.6776776579055157\n",
            "Accuracy: 0.6781068659219586\n",
            "Accuracy: 0.6790072483860928\n",
            "Accuracy: 0.676654653935819\n",
            "Accuracy: 0.6770824392636617\n",
            "Accuracy: 0.675834817682358\n",
            "Accuracy: 0.6762624129335931\n",
            "Accuracy: 0.6755614331159642\n",
            "Accuracy: 0.6764467794644206\n",
            "Accuracy: 0.6768644545714892\n",
            "Accuracy: 0.6772777788961927\n",
            "Accuracy: 0.6776868200672723\n",
            "Accuracy: 0.6769965644349757\n",
            "Accuracy: 0.6763133883476258\n",
            "Accuracy: 0.6767210890443958\n",
            "Accuracy: 0.6771246506477976\n",
            "Accuracy: 0.6759461281877576\n",
            "Accuracy: 0.6768520408539317\n",
            "Accuracy: 0.6772488942742347\n",
            "Accuracy: 0.6760873435741633\n",
            "Accuracy: 0.6769174915729183\n",
            "Accuracy: 0.677308157747015\n",
            "Accuracy: 0.6776339867535759\n",
            "Accuracy: 0.6764932219575092\n",
            "Accuracy: 0.6753635325478119\n",
            "Accuracy: 0.6757541566655256\n",
            "Accuracy: 0.6771025634728945\n",
            "Accuracy: 0.6774811288386441\n",
            "Accuracy: 0.677856088819958\n",
            "Accuracy: 0.6772206439791133\n",
            "Accuracy: 0.6770628921265872\n",
            "Accuracy: 0.6783735018940599\n",
            "Accuracy: 0.6787372833100435\n",
            "Accuracy: 0.6781095621197722\n",
            "Accuracy: 0.6784135793094281\n",
            "Accuracy: 0.678772146800696\n",
            "Accuracy: 0.6791274246819522\n",
            "Accuracy: 0.6794794580163477\n",
            "Accuracy: 0.67749899354848\n",
            "Accuracy: 0.6768939176835626\n",
            "Accuracy: 0.6772512546530715\n",
            "Accuracy: 0.6753074251482839\n",
            "Accuracy: 0.6747202367654869\n",
            "Accuracy: 0.6754716012212966\n",
            "Accuracy: 0.6753313607346695\n",
            "Accuracy: 0.6747518271076521\n",
            "Accuracy: 0.6755477522026029\n",
            "Accuracy: 0.6767190601107335\n",
            "Accuracy: 0.6748367045236671\n",
            "Accuracy: 0.6760009493662681\n",
            "Accuracy: 0.6763467328815624\n",
            "Accuracy: 0.6757777650468851\n",
            "Accuracy: 0.673931609106879\n",
            "Accuracy: 0.6747073160841109\n",
            "Accuracy: 0.6728813413341167\n",
            "Accuracy: 0.671914657208487\n",
            "Accuracy: 0.6709560964788709\n",
            "Accuracy: 0.6713128606145851\n",
            "Accuracy: 0.6707814621428648\n",
            "Accuracy: 0.6711359902041582\n",
            "Accuracy: 0.6701964910857934\n",
            "Accuracy: 0.6705505085089569\n",
            "Accuracy: 0.6709016241499635\n",
            "Accuracy: 0.6712498735408394\n",
            "Accuracy: 0.6715952916358544\n",
            "Accuracy: 0.6726972429375899\n",
            "Accuracy: 0.6738404910410604\n",
            "Accuracy: 0.6741713433859339\n",
            "Accuracy: 0.6744995489120483\n",
            "Accuracy: 0.6739787428502543\n",
            "Accuracy: 0.673462070169903\n",
            "Accuracy: 0.673344738869799\n",
            "Accuracy: 0.6720472290290622\n",
            "Accuracy: 0.6723773290129269\n",
            "Accuracy: 0.6727048500906676\n",
            "Accuracy: 0.6730298223662469\n",
            "Accuracy: 0.6733522754768992\n",
            "Accuracy: 0.6728519857159913\n",
            "Accuracy: 0.6731726424052165\n",
            "Accuracy: 0.6734908419550607\n",
            "Accuracy: 0.6726140723428653\n",
            "Accuracy: 0.6733122041923465\n",
            "Accuracy: 0.673200329370571\n",
            "Accuracy: 0.6735136214292274\n",
            "Accuracy: 0.6738245579084956\n",
            "Accuracy: 0.6733374892549122\n",
            "Accuracy: 0.6739734584715829\n",
            "Accuracy: 0.6742792178263894\n",
            "Accuracy: 0.6752773587350492\n",
            "Accuracy: 0.6755760501231655\n",
            "Accuracy: 0.6750914956278661\n",
            "Accuracy: 0.6760756925785498\n",
            "Accuracy: 0.6763681999958344\n",
            "Accuracy: 0.6751587783206593\n",
            "Accuracy: 0.6735958018596622\n",
            "Accuracy: 0.6745711861965028\n",
            "Accuracy: 0.674864896767431\n",
            "Accuracy: 0.6758733479352835\n",
            "Accuracy: 0.6761603099959237\n",
            "Accuracy: 0.674977452093172\n",
            "Accuracy: 0.675265555897503\n",
            "Accuracy: 0.6755516236325456\n",
            "Accuracy: 0.6765399023260869\n",
            "Accuracy: 0.6757231911023458\n",
            "Accuracy: 0.6760046580037871\n",
            "Accuracy: 0.6762841634634065\n",
            "Accuracy: 0.6772129589484798\n",
            "Accuracy: 0.677486349142134\n",
            "Accuracy: 0.6784475092230172\n",
            "Accuracy: 0.6779847247494045\n",
            "Accuracy: 0.6788949731686343\n",
            "Accuracy: 0.6794577112783751\n",
            "Accuracy: 0.6797188167669335\n",
            "Accuracy: 0.6799359640832675\n",
            "Accuracy: 0.6791381304328507\n",
            "Accuracy: 0.6776722689670345\n",
            "Accuracy: 0.6779358611010865\n",
            "Accuracy: 0.6771527217782062\n",
            "Accuracy: 0.6774162884553273\n",
            "Accuracy: 0.6779689826442554\n",
            "Accuracy: 0.6782281009961437\n",
            "Accuracy: 0.6791045008712869\n",
            "Accuracy: 0.6793581792398503\n",
            "Accuracy: 0.6796101941437017\n",
            "Accuracy: 0.6788394980960422\n",
            "Accuracy: 0.6794172937007991\n",
            "Accuracy: 0.6796666619839606\n",
            "Accuracy: 0.6805616655781817\n",
            "Accuracy: 0.6808057335115248\n",
            "Accuracy: 0.6813297576459658\n",
            "Accuracy: 0.6815697992077241\n",
            "Accuracy: 0.6814490562429825\n",
            "Accuracy: 0.6823241317727763\n",
            "Accuracy: 0.6815668399371798\n",
            "Accuracy: 0.6818030927377411\n",
            "Accuracy: 0.6810522224248772\n",
            "Accuracy: 0.6812886076528322\n",
            "Accuracy: 0.6821114561027121\n",
            "Accuracy: 0.6829680539667606\n",
            "Accuracy: 0.6838193147724663\n",
            "Accuracy: 0.6833844014576503\n",
            "Accuracy: 0.682952181104536\n",
            "Accuracy: 0.6837571967899063\n",
            "Accuracy: 0.6839801676456745\n",
            "Accuracy: 0.683550097276828\n",
            "Accuracy: 0.6828168468373267\n",
            "Accuracy: 0.6836504014890369\n",
            "Accuracy: 0.6844788889392168\n",
            "Accuracy: 0.6846962945027785\n",
            "Accuracy: 0.6842705571399355\n",
            "Accuracy: 0.6844872805368469\n",
            "Accuracy: 0.6840647278247295\n",
            "Accuracy: 0.6836447053683732\n",
            "Accuracy: 0.6838613561729887\n",
            "Accuracy: 0.6846719556266353\n",
            "Accuracy: 0.6833636628060383\n",
            "Accuracy: 0.6829506851864989\n",
            "Accuracy: 0.6837200851215374\n",
            "Accuracy: 0.6839333281797522\n",
            "Accuracy: 0.6841453205455433\n",
            "Accuracy: 0.6843196829857184\n",
            "Accuracy: 0.6836183929582379\n",
            "Accuracy: 0.6832118779420853\n",
            "Accuracy: 0.6834235035854838\n",
            "Accuracy: 0.682441866983568\n",
            "Accuracy: 0.6826544919343778\n",
            "Accuracy: 0.6819680629447958\n",
            "Accuracy: 0.6827182325729326\n",
            "Accuracy: 0.6829282454081944\n",
            "Accuracy: 0.6836714050029418\n",
            "Accuracy: 0.6838775167072361\n",
            "Accuracy: 0.6843657468263555\n",
            "Accuracy: 0.6839686041161165\n",
            "Accuracy: 0.6841721368507601\n",
            "Accuracy: 0.684374526143074\n",
            "Accuracy: 0.6845757816018176\n",
            "Accuracy: 0.6847759127283896\n",
            "Accuracy: 0.6843831587302651\n",
            "Accuracy: 0.6845827130807771\n",
            "Accuracy: 0.6833616453854992\n",
            "Accuracy: 0.6835629190858556\n",
            "Accuracy: 0.6843140481588926\n",
            "Accuracy: 0.6836532326517525\n",
            "Accuracy: 0.6838520531784998\n",
            "Accuracy: 0.6845962354719964\n",
            "Accuracy: 0.6853024508063085\n",
            "Accuracy: 0.6854951689424722\n",
            "Accuracy: 0.6851111094803022\n",
            "Accuracy: 0.6855735733702376\n",
            "Accuracy: 0.6857640023501415\n",
            "Accuracy: 0.6859534075183253\n",
            "Accuracy: 0.6861417971091999\n",
            "Accuracy: 0.6863291792691073\n",
            "Accuracy: 0.6865155620574951\n",
            "Accuracy: 0.6869669109582901\n",
            "Accuracy: 0.6865870974740552\n",
            "Accuracy: 0.6867713187106703\n",
            "Accuracy: 0.6863940255937602\n",
            "Accuracy: 0.6865777853288149\n",
            "Accuracy: 0.6872528495751028\n",
            "Accuracy: 0.6863536987629236\n",
            "Accuracy: 0.686536124418672\n",
            "Accuracy: 0.6867175999407967\n",
            "Accuracy: 0.6874176133762706\n",
            "Accuracy: 0.6865273533398624\n",
            "Accuracy: 0.6869658427337035\n",
            "Accuracy: 0.6873699958176956\n",
            "Accuracy: 0.6875469951212866\n",
            "Accuracy: 0.6877230867361411\n",
            "Accuracy: 0.6878982776266229\n",
            "Accuracy: 0.6885827788893057\n",
            "Accuracy: 0.688754890711253\n",
            "Accuracy: 0.6894337432033519\n",
            "Accuracy: 0.6896028292330005\n",
            "Accuracy: 0.6897710612928024\n",
            "Accuracy: 0.6899384458359302\n",
            "Accuracy: 0.6901049892507006\n",
            "Accuracy: 0.6907719511137271\n",
            "Accuracy: 0.6909355780482292\n",
            "Accuracy: 0.6913477655063544\n",
            "Accuracy: 0.6909806746155468\n",
            "Accuracy: 0.6906154055157607\n",
            "Accuracy: 0.6900044198673551\n",
            "Accuracy: 0.6901679218551259\n",
            "Accuracy: 0.6898073536421865\n",
            "Accuracy: 0.6904619369811449\n",
            "Accuracy: 0.6908682134805941\n",
            "Average train loss: 0.49952580448480505, avarage valid loss: 0.5258101868185251\n",
            "Average accuracy rate: 0.6908682134805941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  model_gpt.test()\n",
        "  precision, recall, f1 = model_gpt.calculate_metrics()\n",
        "\n",
        "  print(f'Precision: {precision}')\n",
        "  print(f'Recall: {recall}')\n",
        "  print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs95iLNyrHkP",
        "outputId": "744131e9-b471-49bc-e953-7ee27f4e190c"
      },
      "id": "cs95iLNyrHkP",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average loss: 1.8697055965298558e-05\n",
            "Accuracy: 0.9562227725982666\n",
            "Average loss: 0.0020493235041546386\n",
            "Accuracy: 0.75\n",
            "Average loss: 0.006250575208673026\n",
            "Accuracy: 0.6145924131075541\n",
            "Average loss: 0.006830354204128791\n",
            "Accuracy: 0.6499999910593033\n",
            "Average loss: 0.012409601329893964\n",
            "Accuracy: 0.5687554478645325\n",
            "Average loss: 0.015427979959811015\n",
            "Accuracy: 0.547925740480423\n",
            "Average loss: 0.02100722708557619\n",
            "Accuracy: 0.5044759597097125\n",
            "Average loss: 0.02124533001447051\n",
            "Accuracy: 0.5468886196613312\n",
            "Average loss: 0.021276169017002713\n",
            "Accuracy: 0.5909873578283522\n",
            "Average loss: 0.02185594801245848\n",
            "Accuracy: 0.607510894536972\n",
            "Average loss: 0.022435727007914245\n",
            "Accuracy: 0.6210301518440247\n",
            "Average loss: 0.022466566010446446\n",
            "Accuracy: 0.6479257444540659\n",
            "Average loss: 0.023046345005902212\n",
            "Accuracy: 0.6562562814125648\n",
            "Average loss: 0.023626124001357978\n",
            "Accuracy: 0.6633967416627067\n",
            "Average loss: 0.02420590299681374\n",
            "Accuracy: 0.6695851405461629\n",
            "Average loss: 0.024785681992269507\n",
            "Accuracy: 0.6749999895691872\n",
            "Average loss: 0.024804379048234806\n",
            "Accuracy: 0.6915425062179565\n",
            "Average loss: 0.025384158043690572\n",
            "Accuracy: 0.6951358517011007\n",
            "Average loss: 0.02741478449187991\n",
            "Accuracy: 0.6871696083169234\n",
            "Average loss: 0.030433163121796964\n",
            "Accuracy: 0.674999988079071\n",
            "Average loss: 0.031012942117252727\n",
            "Accuracy: 0.6788677374521891\n",
            "Average loss: 0.0365921892430179\n",
            "Accuracy: 0.6590908982537009\n",
            "Average loss: 0.037171968238473664\n",
            "Accuracy: 0.6633140211519988\n",
            "Average loss: 0.040190346868390715\n",
            "Accuracy: 0.654166653752327\n",
            "Average loss: 0.040770125863846485\n",
            "Accuracy: 0.6582488965988159\n",
            "Average loss: 0.04134990485930225\n",
            "Accuracy: 0.6620171207648057\n",
            "Average loss: 0.04338053130749159\n",
            "Accuracy: 0.6576378654550623\n",
            "Average loss: 0.04341137031002379\n",
            "Accuracy: 0.6678571296589715\n",
            "Average loss: 0.04544199675821313\n",
            "Accuracy: 0.6635785123397564\n",
            "Average loss: 0.04564367381674124\n",
            "Accuracy: 0.6699999868869781\n",
            "Average loss: 0.046223452812197\n",
            "Accuracy: 0.6727813655330289\n",
            "Average loss: 0.04625429181472921\n",
            "Accuracy: 0.681249987334013\n",
            "Average loss: 0.046455968873257314\n",
            "Accuracy: 0.6865521922256007\n",
            "Average loss: 0.04703574786871308\n",
            "Accuracy: 0.6886013255399817\n",
            "Average loss: 0.047615526864168846\n",
            "Accuracy: 0.6905333655221122\n",
            "Average loss: 0.0506339054940859\n",
            "Accuracy: 0.6836790276898278\n",
            "Average loss: 0.051871902107192974\n",
            "Accuracy: 0.682600601299389\n",
            "Average loss: 0.051890599163158274\n",
            "Accuracy: 0.6898011847546226\n",
            "Average loss: 0.051909296219123574\n",
            "Accuracy: 0.6966325075198443\n",
            "Average loss: 0.05248907521457934\n",
            "Accuracy: 0.6981222629547119\n",
            "Average loss: 0.05269075227310745\n",
            "Accuracy: 0.7019783723645094\n",
            "Average loss: 0.05327053126856321\n",
            "Accuracy: 0.7032699045680818\n",
            "Average loss: 0.05328922832452851\n",
            "Accuracy: 0.709152529405993\n",
            "Average loss: 0.05386900731998428\n",
            "Accuracy: 0.7102223065766421\n",
            "Average loss: 0.05444878631544004\n",
            "Accuracy: 0.7112445380952623\n",
            "Average loss: 0.05468688924433436\n",
            "Accuracy: 0.7141256837741189\n",
            "Average loss: 0.05526666823979013\n",
            "Accuracy: 0.715021365500511\n",
            "Average loss: 0.05584644723524589\n",
            "Accuracy: 0.7158797271549702\n",
            "Average loss: 0.05642622623070166\n",
            "Accuracy: 0.7167030536398595\n",
            "Average loss: 0.057006005226157425\n",
            "Accuracy: 0.7174934470653533\n",
            "Average loss: 0.057036844228689626\n",
            "Accuracy: 0.7219303846359253\n",
            "Average loss: 0.0626160913544548\n",
            "Accuracy: 0.7127351325291854\n",
            "Average loss: 0.06319587034991056\n",
            "Accuracy: 0.71355565314023\n",
            "Average loss: 0.06339754740843867\n",
            "Accuracy: 0.7161976363923814\n",
            "Average loss: 0.06397732640389443\n",
            "Accuracy: 0.7169253652746027\n",
            "Average loss: 0.06817857810841282\n",
            "Accuracy: 0.7102620058826038\n",
            "Average loss: 0.06941657472151991\n",
            "Accuracy: 0.7090956066784105\n",
            "Average loss: 0.06943527177748521\n",
            "Accuracy: 0.7133564198839253\n",
            "Average loss: 0.07245365040740226\n",
            "Accuracy: 0.7087872806241957\n",
            "Average loss: 0.07247234746336756\n",
            "Accuracy: 0.7129112054904302\n",
            "Average loss: 0.07250318646589976\n",
            "Accuracy: 0.7166958949604972\n",
            "Average loss: 0.07308296546135552\n",
            "Accuracy: 0.7173334244758852\n",
            "Average loss: 0.07366274445681129\n",
            "Accuracy: 0.7179507149590386\n",
            "Average loss: 0.07390084738570561\n",
            "Accuracy: 0.7199167544022202\n",
            "Average loss: 0.07810209909022399\n",
            "Accuracy: 0.7141299926317655\n",
            "Average loss: 0.07868187808567977\n",
            "Accuracy: 0.7147677613027168\n",
            "Average loss: 0.07871271708821197\n",
            "Accuracy: 0.7181858135693109\n",
            "Average loss: 0.07929249608366773\n",
            "Accuracy: 0.7187451799126232\n",
            "Average loss: 0.07987227507912349\n",
            "Accuracy: 0.7192883327387382\n",
            "Average loss: 0.08045205407457925\n",
            "Accuracy: 0.7198159669126784\n",
            "Average loss: 0.08169005068768634\n",
            "Accuracy: 0.7187449990863531\n",
            "Average loss: 0.08170874774365164\n",
            "Accuracy: 0.7220433014962409\n",
            "Average loss: 0.08590999944817002\n",
            "Accuracy: 0.7168615746171507\n",
            "Average loss: 0.08892837807808708\n",
            "Accuracy: 0.7131712452785389\n",
            "Average loss: 0.0891664810069814\n",
            "Accuracy: 0.7149126585324606\n",
            "Average loss: 0.0893681580655095\n",
            "Accuracy: 0.7167720018248809\n",
            "Average loss: 0.09000864682673682\n",
            "Accuracy: 0.7171227196594337\n",
            "Average loss: 0.09302702545665387\n",
            "Accuracy: 0.7136182899658496\n",
            "Average loss: 0.09360680445210963\n",
            "Accuracy: 0.7141575866107699\n",
            "Average loss: 0.09380848151063774\n",
            "Accuracy: 0.7159334011375904\n",
            "Average loss: 0.09401015856916585\n",
            "Accuracy: 0.7176653683921437\n",
            "Average loss: 0.09458993756462161\n",
            "Accuracy: 0.7181355800570511\n",
            "Average loss: 0.09760831619453866\n",
            "Accuracy: 0.7148300574486515\n",
            "Average loss: 0.09762701325050396\n",
            "Accuracy: 0.7177037802480516\n",
            "Average loss: 0.10064539188042101\n",
            "Accuracy: 0.7144811146399555\n",
            "Average loss: 0.10122517087587678\n",
            "Accuracy: 0.7149664822012879\n",
            "Average loss: 0.10180494987133254\n",
            "Accuracy: 0.7154406918876473\n",
            "Average loss: 0.10482332850124959\n",
            "Accuracy: 0.7123536067930135\n",
            "Average loss: 0.10484202555721489\n",
            "Accuracy: 0.7150937097795894\n",
            "Average loss: 0.10542180455267065\n",
            "Accuracy: 0.7155506988366445\n",
            "Average loss: 0.10745243100086\n",
            "Accuracy: 0.7136630782714257\n",
            "Average loss: 0.11047080963077704\n",
            "Accuracy: 0.7107295361549958\n",
            "Average loss: 0.11050164863330926\n",
            "Accuracy: 0.7132354256927326\n",
            "Average loss: 0.11108142762876502\n",
            "Accuracy: 0.7136927373865818\n",
            "Average loss: 0.11110012468473032\n",
            "Accuracy: 0.7162456851256521\n",
            "Average loss: 0.11167990368018609\n",
            "Accuracy: 0.7166621126234531\n",
            "Average loss: 0.11588115538470448\n",
            "Accuracy: 0.7128179386718986\n",
            "Average loss: 0.11646093438016024\n",
            "Accuracy: 0.7132608446539664\n",
            "Average loss: 0.11947931301007729\n",
            "Accuracy: 0.7105387876732181\n",
            "Average loss: 0.12005909200553305\n",
            "Accuracy: 0.7109956270456315\n",
            "Average loss: 0.12008993100806525\n",
            "Accuracy: 0.7133003957200759\n",
            "Average loss: 0.12066971000352102\n",
            "Accuracy: 0.7137212028690413\n",
            "Average loss: 0.1219077066166281\n",
            "Accuracy: 0.713042135377532\n",
            "Average loss: 0.12214580954552241\n",
            "Accuracy: 0.7142992036846968\n",
            "Average loss: 0.12216450660148771\n",
            "Accuracy: 0.7166032376743499\n",
            "Average loss: 0.12274428559694349\n",
            "Accuracy: 0.7169770064218989\n",
            "Average loss: 0.12332406459239925\n",
            "Accuracy: 0.7173437888377181\n",
            "Average loss: 0.12390384358785501\n",
            "Accuracy: 0.7177037789865777\n",
            "Average loss: 0.12593447003604435\n",
            "Accuracy: 0.7161081225500194\n",
            "Average loss: 0.12613614709457246\n",
            "Accuracy: 0.7173818918791685\n",
            "Average loss: 0.13171539422033762\n",
            "Accuracy: 0.7131151836197656\n",
            "Average loss: 0.13174623322286985\n",
            "Accuracy: 0.7151746664728437\n",
            "Average loss: 0.1329842298359769\n",
            "Accuracy: 0.7145428309398415\n",
            "Average loss: 0.1330029268919422\n",
            "Accuracy: 0.7166628304280733\n",
            "Average loss: 0.1330216239479075\n",
            "Accuracy: 0.7187459603599881\n",
            "Average loss: 0.13505225039609686\n",
            "Accuracy: 0.7172376092137962\n",
            "Average loss: 0.1356320293915526\n",
            "Accuracy: 0.7175708153308966\n",
            "Average loss: 0.1362118083870084\n",
            "Accuracy: 0.7178983738866903\n",
            "Average loss: 0.13679158738246414\n",
            "Accuracy: 0.7182204272566723\n",
            "Average loss: 0.1373713663779199\n",
            "Accuracy: 0.718537113070488\n",
            "Average loss: 0.1379511453733757\n",
            "Accuracy: 0.7188485644080422\n",
            "Average loss: 0.13853092436883144\n",
            "Accuracy: 0.7191549099859644\n",
            "Average loss: 0.1405615508170208\n",
            "Accuracy: 0.7177290751682064\n",
            "Average loss: 0.140592389819553\n",
            "Accuracy: 0.7195520444262412\n",
            "Average loss: 0.14117216881500874\n",
            "Accuracy: 0.7198454098701477\n",
            "Average loss: 0.14175194781046452\n",
            "Accuracy: 0.7201341187197065\n",
            "Average loss: 0.1423317268059203\n",
            "Accuracy: 0.7204182809732091\n",
            "Average loss: 0.14653297851043867\n",
            "Accuracy: 0.7174757728353143\n",
            "Average loss: 0.14777097512354576\n",
            "Accuracy: 0.7169044664663862\n",
            "Average loss: 0.1483507541190015\n",
            "Accuracy: 0.7172069146082951\n",
            "Average loss: 0.14893053311445728\n",
            "Accuracy: 0.71750474522132\n",
            "Average loss: 0.15313178481897566\n",
            "Accuracy: 0.7146734762372393\n",
            "Average loss: 0.15871103194474084\n",
            "Accuracy: 0.7111329032962483\n",
            "Average loss: 0.16074165839293017\n",
            "Accuracy: 0.7098839803418117\n",
            "Average loss: 0.16132143738838595\n",
            "Accuracy: 0.7102272303016097\n",
            "Average loss: 0.16152311444691406\n",
            "Accuracy: 0.7113007267608362\n",
            "Average loss: 0.1645414930768311\n",
            "Accuracy: 0.7093480003141138\n",
            "Average loss: 0.16657211952502043\n",
            "Accuracy: 0.7081482121046039\n",
            "Average loss: 0.1671518985204762\n",
            "Accuracy: 0.7084940719089919\n",
            "Average loss: 0.16773167751593196\n",
            "Accuracy: 0.7088349908590317\n",
            "Average loss: 0.16831145651138774\n",
            "Accuracy: 0.7091710740792836\n",
            "Average loss: 0.1688912355068435\n",
            "Accuracy: 0.7095024237330531\n",
            "Average loss: 0.1730924872113619\n",
            "Accuracy: 0.7069449049609524\n",
            "Average loss: 0.17329416426989\n",
            "Accuracy: 0.7079815566539764\n",
            "Average loss: 0.17532479071807933\n",
            "Accuracy: 0.7068491130039609\n",
            "Average loss: 0.1759045697135351\n",
            "Accuracy: 0.7071872884280062\n",
            "Average loss: 0.1759232667695004\n",
            "Accuracy: 0.7088814073679398\n",
            "Average loss: 0.1771612633826075\n",
            "Accuracy: 0.7084415144211537\n",
            "Average loss: 0.1773629404411356\n",
            "Accuracy: 0.7094333347858198\n",
            "Average loss: 0.17794271943659135\n",
            "Accuracy: 0.7097452640533447\n",
            "Average loss: 0.1809610980665084\n",
            "Accuracy: 0.707983886169282\n",
            "Average loss: 0.1809797951224737\n",
            "Accuracy: 0.7096170367378938\n",
            "Average loss: 0.18655904224823888\n",
            "Accuracy: 0.7065723324133679\n",
            "Average loss: 0.18957742087815593\n",
            "Accuracy: 0.7048658705376959\n",
            "Average loss: 0.19015719987361168\n",
            "Accuracy: 0.7051972050820627\n",
            "Average loss: 0.19073697886906746\n",
            "Accuracy: 0.7055242917476556\n",
            "Average loss: 0.19076781787159966\n",
            "Accuracy: 0.7070418265974445\n",
            "Average loss: 0.19078651492756496\n",
            "Accuracy: 0.7086189211923865\n",
            "Average loss: 0.19136629392302074\n",
            "Accuracy: 0.7089183161843498\n",
            "Average loss: 0.19194607291847648\n",
            "Accuracy: 0.7092139687389135\n",
            "Average loss: 0.19252585191393226\n",
            "Accuracy: 0.7095059485909361\n",
            "Average loss: 0.193105630909388\n",
            "Accuracy: 0.7097943237534275\n",
            "Average loss: 0.1986848780351532\n",
            "Accuracy: 0.706935323454851\n",
            "Average loss: 0.19926465703060897\n",
            "Accuracy: 0.707235856390581\n",
            "Average loss: 0.19984443602606472\n",
            "Accuracy: 0.7075327465028474\n",
            "Average loss: 0.2004242150215205\n",
            "Accuracy: 0.7078260596258095\n",
            "Average loss: 0.20600346214728565\n",
            "Accuracy: 0.7050473243890408\n",
            "Average loss: 0.21158270927305084\n",
            "Accuracy: 0.7023016693336623\n",
            "Average loss: 0.2121624882685066\n",
            "Accuracy: 0.702620728834141\n",
            "Average loss: 0.21518086689842364\n",
            "Accuracy: 0.7010981198619394\n",
            "Average loss: 0.21576064589387942\n",
            "Accuracy: 0.7014204859733582\n",
            "Average loss: 0.21779127234206877\n",
            "Accuracy: 0.7005039554002673\n",
            "Average loss: 0.21837105133752452\n",
            "Accuracy: 0.7008260292124886\n",
            "Average loss: 0.22138942996744157\n",
            "Accuracy: 0.6993487371110368\n",
            "Average loss: 0.22440780859735862\n",
            "Accuracy: 0.6978883283478873\n",
            "Average loss: 0.2249875875928144\n",
            "Accuracy: 0.6982197737829252\n",
            "Average loss: 0.22556736658827015\n",
            "Accuracy: 0.6985474740718044\n",
            "Average loss: 0.22614714558372592\n",
            "Accuracy: 0.6988714923349659\n",
            "Average loss: 0.227385142196833\n",
            "Accuracy: 0.6985637032785895\n",
            "Average loss: 0.22796492119228876\n",
            "Accuracy: 0.698884031176567\n",
            "Average loss: 0.2309832998222058\n",
            "Accuracy: 0.6974746011897345\n",
            "Average loss: 0.23400167845212286\n",
            "Accuracy: 0.6960806594445155\n",
            "Average loss: 0.23402037550808816\n",
            "Accuracy: 0.6975022010464486\n",
            "Average loss: 0.2342584784369825\n",
            "Accuracy: 0.6982971740805585\n",
            "Average loss: 0.23483825743243825\n",
            "Accuracy: 0.6986102851661475\n",
            "Average loss: 0.23541803642789402\n",
            "Accuracy: 0.6989200294658702\n",
            "Average loss: 0.23843641505781107\n",
            "Accuracy: 0.6975556293273355\n",
            "Average loss: 0.23901619405326682\n",
            "Accuracy: 0.6978676883464164\n",
            "Average loss: 0.2402541906663739\n",
            "Accuracy: 0.6975814955575126\n",
            "Average loss: 0.24083396966182968\n",
            "Accuracy: 0.6978901336067601\n",
            "Average loss: 0.24641321678759484\n",
            "Accuracy: 0.6955125793736643\n",
            "Average loss: 0.24644405579012704\n",
            "Accuracy: 0.6968056246017417\n",
            "Average loss: 0.2494624344200441\n",
            "Accuracy: 0.6954945965134418\n",
            "Average loss: 0.25004221341549987\n",
            "Accuracy: 0.6958076281031382\n",
            "Average loss: 0.2530605920454169\n",
            "Accuracy: 0.6945151643875318\n",
            "Average loss: 0.25364037104087267\n",
            "Accuracy: 0.6948299988800165\n",
            "Average loss: 0.253878473969767\n",
            "Accuracy: 0.6955860762426695\n",
            "Average loss: 0.2540801510282951\n",
            "Accuracy: 0.6963973725684965\n",
            "Average loss: 0.25465993002375087\n",
            "Accuracy: 0.6966980024797833\n",
            "Average loss: 0.2576783086536679\n",
            "Accuracy: 0.6954333984851837\n",
            "Average loss: 0.2582580876491237\n",
            "Accuracy: 0.6957358329450313\n",
            "Average loss: 0.260288714097313\n",
            "Accuracy: 0.6949835626205595\n",
            "Average loss: 0.2623193405455024\n",
            "Accuracy: 0.6942387038263781\n",
            "Average loss: 0.2628991195409581\n",
            "Accuracy: 0.6945425470670065\n",
            "Average loss: 0.2629299585434903\n",
            "Accuracy: 0.6957583261699212\n",
            "Average loss: 0.26313163560201847\n",
            "Accuracy: 0.6965372796776226\n",
            "Average loss: 0.2637114145974742\n",
            "Accuracy: 0.6968256151618589\n",
            "Average loss: 0.26574204104566357\n",
            "Accuracy: 0.6960898056053199\n",
            "Average loss: 0.2663218200411193\n",
            "Accuracy: 0.6963775229225889\n",
            "Average loss: 0.26690159903657507\n",
            "Accuracy: 0.6966625000749316\n",
            "Average loss: 0.2689322254847644\n",
            "Accuracy: 0.6959379253229259\n",
            "Average loss: 0.26951200448022017\n",
            "Accuracy: 0.6962222875851505\n",
            "Average loss: 0.2725303831101372\n",
            "Accuracy: 0.6950370993972385\n",
            "Average loss: 0.273110162105593\n",
            "Accuracy: 0.6953230135351698\n",
            "Average loss: 0.2731410011081252\n",
            "Accuracy: 0.6964786146962365\n",
            "Average loss: 0.27337910403701954\n",
            "Accuracy: 0.6971605527732108\n",
            "Average loss: 0.27339780109298484\n",
            "Accuracy: 0.698354387887612\n",
            "Average loss: 0.27341649814895014\n",
            "Accuracy: 0.6995372703862847\n",
            "Average loss: 0.2754471245971395\n",
            "Accuracy: 0.6988260373132958\n",
            "Average loss: 0.2754658216531048\n",
            "Accuracy: 0.6999960224736821\n",
            "Average loss: 0.2754845187090701\n",
            "Accuracy: 0.7011554195330694\n",
            "Average loss: 0.2767225153221772\n",
            "Accuracy: 0.7008969593155492\n",
            "Average loss: 0.2769606182510715\n",
            "Accuracy: 0.7015376780599757\n",
            "Average loss: 0.27754039724652724\n",
            "Accuracy: 0.7017818077334336\n",
            "Average loss: 0.27755909430249254\n",
            "Accuracy: 0.7029126564661662\n",
            "Average loss: 0.27819958306371984\n",
            "Accuracy: 0.7030934733627117\n",
            "Average loss: 0.27821828011968514\n",
            "Accuracy: 0.7042085804078022\n",
            "Average loss: 0.2787980591151409\n",
            "Accuracy: 0.7044367126205511\n",
            "Average loss: 0.2793778381105967\n",
            "Accuracy: 0.704662852412228\n",
            "Average loss: 0.27995761710605244\n",
            "Accuracy: 0.7048870257709338\n",
            "Average loss: 0.2855368642318176\n",
            "Accuracy: 0.7028908796640702\n",
            "Average loss: 0.2867748608449247\n",
            "Accuracy: 0.7026360795415681\n",
            "Average loss: 0.28679355790089\n",
            "Accuracy: 0.7037244344473909\n",
            "Average loss: 0.28737333689634575\n",
            "Accuracy: 0.7039487861160539\n",
            "Average loss: 0.28740417589887796\n",
            "Accuracy: 0.7049693328268984\n",
            "Average loss: 0.28864217251198504\n",
            "Accuracy: 0.7047100443456132\n",
            "Average loss: 0.2888438495705131\n",
            "Accuracy: 0.7053493384570512\n",
            "Average loss: 0.2930451012750315\n",
            "Accuracy: 0.7038301279564866\n",
            "Average loss: 0.2950757277232209\n",
            "Accuracy: 0.7031604505483077\n",
            "Average loss: 0.29565550671867663\n",
            "Accuracy: 0.703381543358167\n",
            "Average loss: 0.2962352857141324\n",
            "Accuracy: 0.7036008013729238\n",
            "Average loss: 0.2968150647095882\n",
            "Accuracy: 0.7038182473379718\n",
            "Average loss: 0.3023943118353533\n",
            "Accuracy: 0.7019250746126529\n",
            "Average loss: 0.30541269046527036\n",
            "Accuracy: 0.7008670915345676\n",
            "Average loss: 0.30961394216978877\n",
            "Accuracy: 0.6994095819337027\n",
            "Average loss: 0.3116445686179781\n",
            "Accuracy: 0.6987769300860118\n",
            "Average loss: 0.3122243476134339\n",
            "Accuracy: 0.6990095041541435\n",
            "Average loss: 0.3128041266088896\n",
            "Accuracy: 0.6992402026249517\n",
            "Average loss: 0.3133839056043454\n",
            "Accuracy: 0.6994690480959942\n",
            "Average loss: 0.3139636845998012\n",
            "Accuracy: 0.6996960628032685\n",
            "Average loss: 0.3145434635952569\n",
            "Accuracy: 0.699921268628413\n",
            "Average loss: 0.3151232425907127\n",
            "Accuracy: 0.700144687105739\n",
            "Average loss: 0.3157030215861685\n",
            "Accuracy: 0.7003663394290939\n",
            "Average loss: 0.3159411245150628\n",
            "Accuracy: 0.7009309492711946\n",
            "Average loss: 0.31797175096325214\n",
            "Accuracy: 0.7003146601658241\n",
            "Average loss: 0.31799044801921744\n",
            "Accuracy: 0.7013143012300134\n",
            "Average loss: 0.32002107446740674\n",
            "Accuracy: 0.7007013165069461\n",
            "Average loss: 0.3220517009155961\n",
            "Accuracy: 0.7000930836034376\n",
            "Average loss: 0.3226314799110519\n",
            "Accuracy: 0.7003098003652565\n",
            "Average loss: 0.32821072703681703\n",
            "Accuracy: 0.6985539060372573\n",
            "Average loss: 0.3288512157980443\n",
            "Accuracy: 0.6987271756504687\n",
            "Average loss: 0.32943099479350013\n",
            "Accuracy: 0.698946624311782\n",
            "Average loss: 0.3300107737889559\n",
            "Accuracy: 0.6991644041619827\n",
            "Average loss: 0.33059055278441163\n",
            "Accuracy: 0.6993805341648333\n",
            "Average loss: 0.33117033177986743\n",
            "Accuracy: 0.6995950329978511\n",
            "Average loss: 0.3341887104097845\n",
            "Accuracy: 0.6986333118345505\n",
            "Average loss: 0.3342195494123167\n",
            "Accuracy: 0.699551453974363\n",
            "Average loss: 0.33479932840777243\n",
            "Accuracy: 0.6997629139405578\n",
            "Average loss: 0.3353791074032282\n",
            "Accuracy: 0.6999728017136954\n",
            "Average loss: 0.33740973385141754\n",
            "Accuracy: 0.6993942995866139\n",
            "Average loss: 0.33742843090738284\n",
            "Accuracy: 0.7003420061290924\n",
            "Average loss: 0.33806891966861013\n",
            "Accuracy: 0.7005016946179026\n",
            "Average loss: 0.3410872982985272\n",
            "Accuracy: 0.6995613118667743\n",
            "Average loss: 0.34166707729398293\n",
            "Accuracy: 0.6997681053450507\n",
            "Average loss: 0.34168577434994823\n",
            "Accuracy: 0.7007006677714261\n",
            "Average loss: 0.34226555334540404\n",
            "Accuracy: 0.7009018346451331\n",
            "Average loss: 0.3428453323408598\n",
            "Accuracy: 0.7011015490504379\n",
            "Average loss: 0.34487595878904914\n",
            "Accuracy: 0.7005356342243634\n",
            "Average loss: 0.3461139554021562\n",
            "Accuracy: 0.7003321991599162\n",
            "Average loss: 0.34635205833105054\n",
            "Accuracy: 0.7008445028747831\n",
            "Average loss: 0.34699254709227784\n",
            "Accuracy: 0.7009972885410133\n",
            "Average loss: 0.3490231735404672\n",
            "Accuracy: 0.7004397705937109\n",
            "Average loss: 0.34960295253592294\n",
            "Accuracy: 0.7006368835065053\n",
            "Average loss: 0.349804629594451\n",
            "Accuracy: 0.7011847210602021\n",
            "Average loss: 0.3503844085899068\n",
            "Accuracy: 0.7013778368631999\n",
            "Average loss: 0.3524150350380961\n",
            "Accuracy: 0.7008267857811668\n",
            "Average loss: 0.35261671209662426\n",
            "Accuracy: 0.7013682351295125\n",
            "Average loss: 0.35319649109208\n",
            "Accuracy: 0.7015587021079328\n",
            "Average loss: 0.35339816815060815\n",
            "Accuracy: 0.7020938718195192\n",
            "Average loss: 0.35359984520913623\n",
            "Accuracy: 0.7026253507055086\n",
            "Average loss: 0.354179624204592\n",
            "Accuracy: 0.7028095341220344\n",
            "Average loss: 0.3547594032000478\n",
            "Accuracy: 0.7029924560083102\n",
            "Average loss: 0.3547781002560131\n",
            "Accuracy: 0.703856723300426\n",
            "Average loss: 0.35897935196053143\n",
            "Accuracy: 0.7026319631508419\n",
            "Average loss: 0.35955913095598724\n",
            "Accuracy: 0.7028136267500409\n",
            "Average loss: 0.35957782801195254\n",
            "Accuracy: 0.7036697387292579\n",
            "Average loss: 0.3597795050704806\n",
            "Accuracy: 0.7041833852276658\n",
            "Average loss: 0.36035928406593637\n",
            "Accuracy: 0.7043580138843332\n",
            "Average loss: 0.3609390630613922\n",
            "Accuracy: 0.7045314744563406\n",
            "Average loss: 0.3615188420568479\n",
            "Accuracy: 0.7047037786245346\n",
            "Average loss: 0.36209862105230367\n",
            "Accuracy: 0.7048749379145347\n",
            "Average loss: 0.3626784000477595\n",
            "Accuracy: 0.7050449636993029\n",
            "Average loss: 0.3668796517522778\n",
            "Accuracy: 0.7038526609785879\n",
            "Average loss: 0.3689102782004672\n",
            "Accuracy: 0.7033260970523483\n",
            "Average loss: 0.3694900571959229\n",
            "Accuracy: 0.7034995286191096\n",
            "Average loss: 0.37006983619137873\n",
            "Accuracy: 0.7036718266462189\n",
            "Average loss: 0.37008853324734403\n",
            "Accuracy: 0.704494468163978\n",
            "Average loss: 0.3706683122427998\n",
            "Accuracy: 0.7046624170495318\n",
            "Average loss: 0.37269893869098913\n",
            "Accuracy: 0.7041417530053642\n",
            "Average loss: 0.3732787176864449\n",
            "Accuracy: 0.7043097561405551\n",
            "Average loss: 0.3732974147424102\n",
            "Accuracy: 0.7051197658397761\n",
            "Average loss: 0.37387719373786593\n",
            "Accuracy: 0.7052835573752722\n",
            "Average loss: 0.37445697273332174\n",
            "Accuracy: 0.705446302319487\n",
            "Average loss: 0.37648759918151103\n",
            "Accuracy: 0.7049314326541439\n",
            "Average loss: 0.3785182256297004\n",
            "Accuracy: 0.704419832002549\n",
            "Average loss: 0.3791587143909277\n",
            "Accuracy: 0.7045443808730645\n",
            "Average loss: 0.3791895533934599\n",
            "Accuracy: 0.7052990587351826\n",
            "Average loss: 0.3797693323889157\n",
            "Accuracy: 0.7054591960502121\n",
            "Average loss: 0.379788029444881\n",
            "Accuracy: 0.7062452887666637\n",
            "Average loss: 0.38036780844033674\n",
            "Accuracy: 0.7064014682546258\n",
            "Average loss: 0.38039864744286894\n",
            "Accuracy: 0.707140956712289\n",
            "Average loss: 0.38597789456863413\n",
            "Accuracy: 0.7057019390674851\n",
            "Average loss: 0.38621599749752844\n",
            "Accuracy: 0.7061294167772535\n",
            "Average loss: 0.3867957764929842\n",
            "Accuracy: 0.7062840257529859\n",
            "Average loss: 0.3909970281975026\n",
            "Accuracy: 0.7051686202562772\n",
            "Average loss: 0.39157680719295834\n",
            "Accuracy: 0.7053252279392781\n",
            "Average loss: 0.3936074336411477\n",
            "Accuracy: 0.7048311973565945\n",
            "Average loss: 0.3938091106996758\n",
            "Accuracy: 0.7052927569645208\n",
            "Average loss: 0.3943888896951316\n",
            "Accuracy: 0.7054475592987153\n",
            "Average loss: 0.39496866869058733\n",
            "Accuracy: 0.7056014234369451\n",
            "Average loss: 0.39498736574655263\n",
            "Accuracy: 0.7063585876338071\n",
            "Average loss: 0.3955671447420084\n",
            "Accuracy: 0.7065087808183876\n",
            "Average loss: 0.39614692373746413\n",
            "Accuracy: 0.7066580719417996\n",
            "Average loss: 0.39672670273291993\n",
            "Accuracy: 0.7068064691063887\n",
            "Average loss: 0.39674539978888523\n",
            "Accuracy: 0.707550995385469\n",
            "Average loss: 0.397325178784341\n",
            "Accuracy: 0.707695851723353\n",
            "Average loss: 0.39790495777979673\n",
            "Accuracy: 0.7078398483797068\n",
            "Average loss: 0.39792365483576203\n",
            "Accuracy: 0.7085747091022468\n",
            "Average loss: 0.39795449383829423\n",
            "Accuracy: 0.7092685219460884\n",
            "Average loss: 0.39815617089682237\n",
            "Accuracy: 0.7097007402602364\n",
            "Average loss: 0.3987359498922781\n",
            "Accuracy: 0.709837168367727\n",
            "Average loss: 0.3987546469482434\n",
            "Accuracy: 0.7105575941110912\n",
            "Average loss: 0.40177302557816047\n",
            "Accuracy: 0.7097798087158982\n",
            "Average loss: 0.40301102219126755\n",
            "Accuracy: 0.7095879408162694\n",
            "Average loss: 0.40302971924723285\n",
            "Accuracy: 0.7103028243866519\n",
            "Average loss: 0.4036094982426886\n",
            "Accuracy: 0.7104355408621661\n",
            "Average loss: 0.4038476011715829\n",
            "Accuracy: 0.710819810886548\n",
            "Average loss: 0.40587822761977227\n",
            "Accuracy: 0.7103398034627411\n",
            "Average loss: 0.4060799046783004\n",
            "Accuracy: 0.7107578061713189\n",
            "Average loss: 0.40909828330821746\n",
            "Accuracy: 0.7099950044495719\n",
            "Average loss: 0.4103362799213245\n",
            "Accuracy: 0.7098063498820335\n",
            "Average loss: 0.41057438285021886\n",
            "Accuracy: 0.7101869489997625\n",
            "Average loss: 0.41077605990874694\n",
            "Accuracy: 0.7106006481492148\n",
            "Average loss: 0.4113558389042027\n",
            "Accuracy: 0.7107295240722807\n",
            "Average loss: 0.41338646535239204\n",
            "Accuracy: 0.7102592359126454\n",
            "Average loss: 0.4139662443478478\n",
            "Accuracy: 0.7103883468368081\n",
            "Average loss: 0.41599687079603714\n",
            "Accuracy: 0.7099216490232644\n",
            "Average loss: 0.4180274972442265\n",
            "Accuracy: 0.7094575584600757\n",
            "Average loss: 0.4180461943001918\n",
            "Accuracy: 0.7101449267445833\n",
            "Average loss: 0.42007682074838115\n",
            "Accuracy: 0.7096827942464087\n",
            "Average loss: 0.42010765975091335\n",
            "Accuracy: 0.7103312553791459\n",
            "Average loss: 0.4206874387463691\n",
            "Accuracy: 0.710458027394437\n",
            "Average loss: 0.42126721774182485\n",
            "Accuracy: 0.7105841009413244\n",
            "Average loss: 0.42184699673728066\n",
            "Accuracy: 0.7107094817764157\n",
            "Average loss: 0.42387762318546995\n",
            "Accuracy: 0.7102521331342933\n",
            "Average loss: 0.42511561979857704\n",
            "Accuracy: 0.7100705077739361\n",
            "Average loss: 0.42931687150309544\n",
            "Accuracy: 0.709072433472979\n",
            "Average loss: 0.4298966504985512\n",
            "Accuracy: 0.7092005592649397\n",
            "Average loss: 0.4301347534274455\n",
            "Accuracy: 0.7095652657149607\n",
            "Average loss: 0.43315313205736256\n",
            "Accuracy: 0.7088469196010281\n",
            "Average loss: 0.43317182911332786\n",
            "Accuracy: 0.7095137008759532\n",
            "Average loss: 0.4361902077432449\n",
            "Accuracy: 0.7087993554530605\n",
            "Average loss: 0.4367699867387007\n",
            "Accuracy: 0.7089264958537616\n",
            "Average loss: 0.43734976573415646\n",
            "Accuracy: 0.7090529563592717\n",
            "Average loss: 0.43736846279012176\n",
            "Accuracy: 0.7097120758692423\n",
            "Average loss: 0.43757013984864984\n",
            "Accuracy: 0.710101731914155\n",
            "Average loss: 0.43814991884410565\n",
            "Accuracy: 0.710224068765615\n",
            "Average loss: 0.43838802177299996\n",
            "Accuracy: 0.710577384031639\n",
            "Average loss: 0.43841886077553216\n",
            "Accuracy: 0.7111926871428075\n",
            "Average loss: 0.4389986397709879\n",
            "Accuracy: 0.7113111872422068\n",
            "Average loss: 0.4390294787735201\n",
            "Accuracy: 0.7119213344231052\n",
            "Average loss: 0.43923115583204825\n",
            "Accuracy: 0.7122990868166479\n",
            "Average loss: 0.439810934827504\n",
            "Accuracy: 0.7124137699448408\n",
            "Average loss: 0.44282931345742105\n",
            "Accuracy: 0.7117141955532134\n",
            "Average loss: 0.4448599399056104\n",
            "Accuracy: 0.7112779956359367\n",
            "Average loss: 0.4450616169641385\n",
            "Accuracy: 0.7116535001776997\n",
            "Average loss: 0.44807999559405554\n",
            "Accuracy: 0.7109613133647337\n",
            "Average loss: 0.44865977458951134\n",
            "Accuracy: 0.7110779664872848\n",
            "Average loss: 0.4516781532194284\n",
            "Accuracy: 0.7103908179964689\n",
            "Average loss: 0.45225793221488414\n",
            "Accuracy: 0.7105083357065152\n",
            "Average loss: 0.4578371793406493\n",
            "Accuracy: 0.7093146501294793\n",
            "Average loss: 0.45807528226954364\n",
            "Accuracy: 0.7096576669386455\n",
            "Average loss: 0.4586550612649994\n",
            "Accuracy: 0.709776153091256\n",
            "Average loss: 0.4592955500262267\n",
            "Accuracy: 0.7098624503552006\n",
            "Average loss: 0.459314247082192\n",
            "Accuracy: 0.7104861473735374\n",
            "Average loss: 0.46055224369529907\n",
            "Accuracy: 0.7103176905651285\n",
            "Average loss: 0.4635706223252161\n",
            "Accuracy: 0.7096463039479868\n",
            "Average loss: 0.4691498694509813\n",
            "Accuracy: 0.7084757787498397\n",
            "Average loss: 0.4693879723798756\n",
            "Accuracy: 0.7088148801547841\n",
            "Average loss: 0.46962607530876993\n",
            "Accuracy: 0.7091522860527039\n",
            "Average loss: 0.46982775236729807\n",
            "Accuracy: 0.7095190453113166\n",
            "Average loss: 0.4728461309972151\n",
            "Accuracy: 0.7088579959537259\n",
            "Average loss: 0.47342590999267087\n",
            "Accuracy: 0.7089755262985419\n",
            "Average loss: 0.4740056889881266\n",
            "Accuracy: 0.7090924748099676\n",
            "Average loss: 0.47603631543631597\n",
            "Accuracy: 0.7086842890138979\n",
            "Average loss: 0.4762744183652103\n",
            "Accuracy: 0.7090170302708161\n",
            "Average loss: 0.4763052573677425\n",
            "Accuracy: 0.7095938367398423\n",
            "Average loss: 0.4768850363631983\n",
            "Accuracy: 0.7097081232304666\n",
            "Average loss: 0.4769158753657305\n",
            "Accuracy: 0.7102804194161245\n",
            "Average loss: 0.4769345724216958\n",
            "Accuracy: 0.7108802788141297\n",
            "Average loss rate: 0.7108802788141297\n",
            "Average accuracy rate: 0.7108802788141297\n",
            "Precision: 0\n",
            "Recall: 0.0\n",
            "F1 Score: 0\n",
            "Precision: 0\n",
            "Recall: 0.0\n",
            "F1 Score: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_score_tensor = torch.tensor(model_gpt.true_score)\n",
        "pred_score_tensor = torch.tensor(model_gpt.pred_score)\n",
        "\n",
        "print(\"True values\\n-----------\")\n",
        "print_tensor(true_score_tensor)\n",
        "print(\"Pred values\\n-----------\")\n",
        "print_tensor(pred_score_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG5_XtijrV27",
        "outputId": "f1ab20d0-bb48-449c-c417-6531fa5b248b"
      },
      "id": "PG5_XtijrV27",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True values\n",
            "-----------\n",
            "[-0.6, 0.4, 0.8, -1.0, 1.0, 0.6, 1.0, -0.2, -0.4, -1.0, -1.0, -0.4, -1.0, -1.0, -1.0, -1.0, -0.6, -1.0, 0.4, 0.6]\n",
            "[-1.0, 1.0, -1.0, 0.6, -1.0, -1.0, 0.4, -0.4, 0.4, -0.8, -1.0, -0.4, -0.8, -1.0, -1.0, 0.6, 0.2, -0.6, -0.6, -1.0]\n",
            "[-0.8, -1.0, -0.6, -1.0, -1.0, -0.2, -1.0, -1.0, -1.0, -1.0, -0.4, 1.0, -1.0, -0.8, -1.0, 0.8, 0.2, -0.6, 0.6, -0.6]\n",
            "[-0.4, -1.0, -1.0, -0.2, 0.8, -1.0, -0.4, -1.0, -1.0, -1.0, 0.2, -0.6, 0.8, 0.6, -0.2, -0.8, 0.0, 0.6, -1.0, -0.8]\n",
            "[-0.8, -1.0, 0.6, -0.6, 0.6, -1.0, -1.0, 0.6, -0.6, -1.0, 0.4, 0.6, -0.4, -1.0, -0.6, -1.0, 0.8, -1.0, 0.6, -1.0]\n",
            "[-0.4, -1.0, 0.2, -0.2, -0.6, -1.0, -1.0, -1.0, 0.4, -0.8, 1.0, -0.4, 0.2, -0.6, -0.6, 0.4, -1.0, -1.0, -1.0, -1.0]\n",
            "[-1.0, -1.0, 0.4, -0.4, -1.0, -1.0, -1.0, 0.8, 0.2, -1.0, -1.0, 0.8, 1.0, 0.4, -1.0, -0.8, 0.6, 0.4, -1.0, -1.0]\n",
            "[-1.0, -1.0, 0.8, -0.8, 0.4, -1.0, -0.6, 0.2, -0.8, -1.0, 0.6, -0.6, 1.0, 0.6, -1.0, -1.0, -0.4, -0.6, -1.0, -1.0]\n",
            "[-1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 0.6, -1.0, 0.4, -1.0, 0.6, 0.6, -1.0, -1.0, -1.0, 0.2, -1.0]\n",
            "[0.6, 0.6, -0.6, -0.2, -1.0, -1.0, 0.6, -1.0, 0.2, -1.0, 1.0, -0.4, 0.6, -1.0, 0.6, -1.0, -0.2, -0.8, -1.0, 0.6]\n",
            "[-1.0, 0.4, 0.4, -1.0, -0.4, -0.8, -1.0, 0.4, -1.0, -1.0, 0.4, -1.0, 0.6, -1.0, -0.4, -0.2, -0.6, -0.6, 0.4, -0.6]\n",
            "[-0.6, 0.2, -0.2, -1.0, -0.6, 0.0, -0.6, -1.0, -1.0, -1.0, 1.0, 0.2, -0.6, -1.0, -0.4, 0.2, -0.8, 0.8, 0.4, -1.0]\n",
            "[-1.0, -1.0, 1.0, 0.6, 0.8, 0.4, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.2, 0.4, -0.6, 0.4, 0.4, -1.0, 1.0]\n",
            "[0.0, -1.0, -1.0, -1.0, -1.0, 0.6, -0.4, -1.0, -1.0, 0.4, -0.6, 0.0, 0.6, -1.0, -0.6, -1.0, -1.0, 0.4, 0.2, -0.2]\n",
            "[0.0, 0.4, -1.0, -0.8, -1.0, 0.4, -0.8, -1.0, -0.8, -0.8, -1.0, -1.0, -0.6, 0.8, -1.0, -0.6, -0.8, -1.0, -1.0, -1.0]\n",
            "[-1.0, -1.0, 0.8, 0.4, -1.0, -1.0, -0.6, -1.0, 0.4, -1.0, -0.6, -1.0, -1.0, 0.4, 0.4, 0.0, -0.4, -1.0, -0.6, -1.0]\n",
            "[-0.4, 1.0, -0.2, -1.0, 0.8, -1.0, 0.4, -0.8, -1.0, -1.0, -0.6, -1.0, -1.0, -1.0, -0.6, -1.0, -1.0, -0.6, -0.4, -0.8]\n",
            "[-1.0, -0.6, 0.6, 0.2, -0.6, -1.0, -0.2, 0.4, -0.8, 0.6, 0.2, -0.2, -0.8, -1.0, 0.4, -1.0, 0.4, 0.4, -0.6, 0.4]\n",
            "[-0.4, -1.0, -1.0, -1.0, 0.4, 0.2, 0.8, -1.0, -0.2, 0.6, -0.6, 0.6, -1.0, -1.0, -0.6, -0.8, -1.0, -0.2, -0.4, -1.0]\n",
            "[-0.4, -0.8, -1.0, 0.6, 0.4, -0.8, 0.6, -1.0, 0.6, -1.0, 1.0, -0.2, -1.0, 0.0, -0.6, 0.2, 0.6, 1.0, -0.2, -0.2]\n",
            "[-0.8, 0.6, -1.0, -1.0, 0.4, -0.2, -0.4, -1.0, -0.4, -0.6]\n",
            "Pred values\n",
            "-----------\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n",
            "[-0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51, -0.51]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ea277d933a14a1592a2ce7c3657e2c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10b6e9f7c7634dbe9a58947cae66b0ce",
              "IPY_MODEL_d3a47c4cdc294a149e94007e5293bccc",
              "IPY_MODEL_fe4889f6624a42618f2316ffe9ced11b"
            ],
            "layout": "IPY_MODEL_1e0f19f26fcf420b969f5ac77706074e"
          }
        },
        "10b6e9f7c7634dbe9a58947cae66b0ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6902adad8dd74079997719d62f90c005",
            "placeholder": "​",
            "style": "IPY_MODEL_d634f2beb1614c29bf9611b0525bc684",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "d3a47c4cdc294a149e94007e5293bccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10600c7a9e844e76ac70820663beb53b",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f49a539fe8b14fd0ad4ad12f01d0f09f",
            "value": 48
          }
        },
        "fe4889f6624a42618f2316ffe9ced11b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12e761319b2c4daea2acb71e39fbe341",
            "placeholder": "​",
            "style": "IPY_MODEL_199350505f4146f98cc156f3c0f48b43",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.30kB/s]"
          }
        },
        "1e0f19f26fcf420b969f5ac77706074e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6902adad8dd74079997719d62f90c005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d634f2beb1614c29bf9611b0525bc684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10600c7a9e844e76ac70820663beb53b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f49a539fe8b14fd0ad4ad12f01d0f09f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12e761319b2c4daea2acb71e39fbe341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "199350505f4146f98cc156f3c0f48b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "400521c4ec7b47f7886e90af7f59cd70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_65d797477f924eb585ab063b2d88efe1",
              "IPY_MODEL_331c7372bf084130936e8e2224dcca65",
              "IPY_MODEL_db38bf661fd447e5bb0223090f72f9fa"
            ],
            "layout": "IPY_MODEL_89ce3d26ef3d4eddb2d3ab618d40e057"
          }
        },
        "65d797477f924eb585ab063b2d88efe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da224cf70187439c81ee45d949acf74d",
            "placeholder": "​",
            "style": "IPY_MODEL_d352842b063f4b75bb69f5b46f7e241d",
            "value": "config.json: 100%"
          }
        },
        "331c7372bf084130936e8e2224dcca65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8682358a4ef046bb91764482dab9aaec",
            "max": 629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f652ccfea70947e4a80a247e7cb98c25",
            "value": 629
          }
        },
        "db38bf661fd447e5bb0223090f72f9fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_010af8321b2c489ca12f8db514d75368",
            "placeholder": "​",
            "style": "IPY_MODEL_232a41abb1774a4aa31177b13de3fc1b",
            "value": " 629/629 [00:00&lt;00:00, 43.9kB/s]"
          }
        },
        "89ce3d26ef3d4eddb2d3ab618d40e057": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da224cf70187439c81ee45d949acf74d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d352842b063f4b75bb69f5b46f7e241d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8682358a4ef046bb91764482dab9aaec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f652ccfea70947e4a80a247e7cb98c25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "010af8321b2c489ca12f8db514d75368": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "232a41abb1774a4aa31177b13de3fc1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74b13e85ae2e4f1f9180bcb2145d5db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e935823e671b43a78ac19abac3e76630",
              "IPY_MODEL_263ea69aaab640268c61f2174201fb36",
              "IPY_MODEL_8f390cf64afe48dfadb61c5e0a8d20a0"
            ],
            "layout": "IPY_MODEL_7ba7aa9bc372483bb99f2340d5d727bd"
          }
        },
        "e935823e671b43a78ac19abac3e76630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30eb5431263c45cd8f8b9de33adb3729",
            "placeholder": "​",
            "style": "IPY_MODEL_71796eb063ee4349b90d251f451a3c37",
            "value": "vocab.txt: 100%"
          }
        },
        "263ea69aaab640268c61f2174201fb36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe18dac36da0458d852dc6f7d66daedd",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59d555d6ded949baa96cad8bd8f4a5f0",
            "value": 231508
          }
        },
        "8f390cf64afe48dfadb61c5e0a8d20a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64b3bea6328f496790315146a2b08487",
            "placeholder": "​",
            "style": "IPY_MODEL_607623b2f1c749b68d6c3c87cb753e13",
            "value": " 232k/232k [00:00&lt;00:00, 1.81MB/s]"
          }
        },
        "7ba7aa9bc372483bb99f2340d5d727bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30eb5431263c45cd8f8b9de33adb3729": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71796eb063ee4349b90d251f451a3c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe18dac36da0458d852dc6f7d66daedd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59d555d6ded949baa96cad8bd8f4a5f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64b3bea6328f496790315146a2b08487": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "607623b2f1c749b68d6c3c87cb753e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79e605f8ca0a4f2d8ab7926797017a11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20670b215a084c8ab26e722bb68e8ec3",
              "IPY_MODEL_5450498f4b0548e7b2e4e05ab7085916",
              "IPY_MODEL_da1645398e89405f8c3b8d438af2f405"
            ],
            "layout": "IPY_MODEL_d64344e14e41424abaa082e139ea714e"
          }
        },
        "20670b215a084c8ab26e722bb68e8ec3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f329795c7c4a47e0b3601bc74582bab6",
            "placeholder": "​",
            "style": "IPY_MODEL_3018e18e42394ba0a74f79decb9f7a29",
            "value": "model.safetensors: 100%"
          }
        },
        "5450498f4b0548e7b2e4e05ab7085916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e809271193f14b378b0b3a467dbb38ce",
            "max": 267832558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3381036a9c649e189dd4dce7edfb774",
            "value": 267832558
          }
        },
        "da1645398e89405f8c3b8d438af2f405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0da43e357054c6da146dcbb317ee57c",
            "placeholder": "​",
            "style": "IPY_MODEL_2d4a60f1fe4e48d8a34fe94e6df018dc",
            "value": " 268M/268M [00:01&lt;00:00, 177MB/s]"
          }
        },
        "d64344e14e41424abaa082e139ea714e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f329795c7c4a47e0b3601bc74582bab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3018e18e42394ba0a74f79decb9f7a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e809271193f14b378b0b3a467dbb38ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3381036a9c649e189dd4dce7edfb774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0da43e357054c6da146dcbb317ee57c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d4a60f1fe4e48d8a34fe94e6df018dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}